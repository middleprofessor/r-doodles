---
title: The statistical significance filter
author: Jeff Walker
date: '2019-04-17'
slug: the-statistical-significance-filter
categories:
  - stats 101
tags:
  - fake data
  - p-values
keywords:
  - tech
output:
  blogdown::html_page:
    toc: true
    number_sections: true
---


<div id="TOC">
<ul>
<li><a href="#why-reported-effect-sizes-are-inflated"><span class="toc-section-number">1</span> Why reported effect sizes are inflated</a></li>
<li><a href="#setup"><span class="toc-section-number">2</span> Setup</a></li>
<li><a href="#exploration-1"><span class="toc-section-number">3</span> Exploration 1</a></li>
<li><a href="#unconditional-means-power-and-sign-error"><span class="toc-section-number">4</span> Unconditional means, power, and sign error</a></li>
<li><a href="#conditional-means"><span class="toc-section-number">5</span> Conditional means</a><ul>
<li><a href="#filter-0.05"><span class="toc-section-number">5.1</span> filter = 0.05</a></li>
<li><a href="#filter-0.2"><span class="toc-section-number">5.2</span> filter = 0.2</a></li>
</ul></li>
</ul>
</div>

<div id="why-reported-effect-sizes-are-inflated" class="section level1">
<h1><span class="header-section-number">1</span> Why reported effect sizes are inflated</h1>
<p>This post is motivated by many discussions in Gelman’s blog <a href="https://statmodeling.stat.columbia.edu/2011/09/10/the-statistical-significance-filter/">but start here</a></p>
<p>When we estimate an effect<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, the estimate will be a little inflated or a little diminished relative to the true effect but the expectation of the effect is the true effect. If all effects were reported, there would be no bias toward inflated effects. Reported effects are inflated if we use p-values to decide which to report and which to archive in the file drawer.</p>
<p>The magnitude of an estimate of an effect is a function of its true effect size plus sampling error (this is with a perfectly designed and executed study. In any real study there will be biases of various sorts). The absolute magnitude of sampling error is bigger with smaller <span class="math inline">\(n\)</span>. The relative magnitude is bigger for smaller true effect size. Consequently, estimates in low powered studies (some combination of low <span class="math inline">\(n\)</span> and small true effect size) can be wildly off, especially relative to the true effect size. In low powered studies, it is these “wildly-off” estimates that are big enough to have <span class="math inline">\(p &lt; 0.05\)</span>. This phenomenon attenuates as power increases because estimates are less and less wildely-off.</p>
<p>Here is a simulation of this</p>
</div>
<div id="setup" class="section level1">
<h1><span class="header-section-number">2</span> Setup</h1>
<pre class="r"><code>library(ggplot2)
library(GGally)
library(data.table)

source(&quot;../R/fake_x.R&quot;) # bookdown</code></pre>
</div>
<div id="exploration-1" class="section level1">
<h1><span class="header-section-number">3</span> Exploration 1</h1>
<p>Modeling a typical set of experiments in ecology or physiology with <span class="math inline">\(p\)</span> independent responses each with the same standardized effect size. How big are the reported effect sizes for the subset with <span class="math inline">\(p.val &lt; 0.05\)</span> (with or without correction for multiple testing). Make this a function of power.</p>
<pre class="r"><code>n &lt;- 20 # per treatment level. power will be a function of effect size
np1 &lt;- n+1
N &lt;- 2*n
niter &lt;- 100 # number of iterations for each combination of fake data parameters
treatment_levels &lt;- c(&quot;Cn&quot;, &quot;Tr&quot;)
Treatment &lt;- rep(treatment_levels, each=n)
p &lt;- 50
b &lt;- pval &lt;- numeric(p)
combo &lt;- 0 # which treatment combo
res_table &lt;- data.table(NULL)
for(beta_1 in c(0.05, 0.15, 0.5, 1)){
  combo &lt;- combo + 1
  j1 &lt;- 0
  j &lt;- 0
  res &lt;- matrix(NA, nrow=niter*p, ncol=3)
  colnames(res) &lt;- c(&quot;ID&quot;, &quot;b&quot;, &quot;pval&quot;)
  for(iter in 1:niter){
    j1 &lt;- j1 + j # completed row -- start row will be this plus 1
    Y &lt;- matrix(rnorm(n*2*p), nrow=n*2, ncol=p)
    Y[np1:N,] &lt;- Y[np1:N,] + beta_1
    fit &lt;- lm(Y ~ Treatment)
    fit.coefs &lt;- coef(summary(fit))
    for(j in 1:p){# inefficient...how do I extract this without a 
      res[j1 + j, &quot;ID&quot;] &lt;- niter*(combo - 1) + iter
      res[j1 + j, &quot;b&quot;] &lt;- fit.coefs[[j]][&quot;TreatmentTr&quot;, &quot;Estimate&quot;]
      res[j1 + j, &quot;pval&quot;] &lt;- fit.coefs[[j]][&quot;TreatmentTr&quot;, &quot;Pr(&gt;|t|)&quot;]
    }
  } # iter
  res_table &lt;- rbind(res_table, data.table(beta=beta_1, res))
}</code></pre>
</div>
<div id="unconditional-means-power-and-sign-error" class="section level1">
<h1><span class="header-section-number">4</span> Unconditional means, power, and sign error</h1>
<p>beta is the true effect. The unconditional mean is the mean of the estimated effect. The absolute value of the estimated effect is the measure of “size” or magnitude and the mean of the absolute values of the effect size will be bigger then the mean effect size if the true effect size is near zero.</p>
<pre class="r"><code>table1 &lt;- res_table[, .(mean_unconditional=mean(b),
              mean_abs_unconditional=mean(abs(b)),
              power = sum(pval &lt; 0.05 &amp; b &gt; 0)/niter/p,
              sign.error=sum(pval &lt; 0.05 &amp; b &lt; 0)/niter/p), by=.(beta)]
knitr::kable(table1, digits=c(2, 2, 2, 2, 3))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">beta</th>
<th align="right">mean_unconditional</th>
<th align="right">mean_abs_unconditional</th>
<th align="right">power</th>
<th align="right">sign.error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.05</td>
<td align="right">0.05</td>
<td align="right">0.25</td>
<td align="right">0.04</td>
<td align="right">0.015</td>
</tr>
<tr class="even">
<td align="right">0.15</td>
<td align="right">0.15</td>
<td align="right">0.28</td>
<td align="right">0.06</td>
<td align="right">0.006</td>
</tr>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.51</td>
<td align="right">0.52</td>
<td align="right">0.35</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">0.87</td>
<td align="right">0.000</td>
</tr>
</tbody>
</table>
</div>
<div id="conditional-means" class="section level1">
<h1><span class="header-section-number">5</span> Conditional means</h1>
<p>The conditional mean is the mean effect size conditional on pval &lt; filter. Again, beta is the true effect. And again, the absolute estimate (<span class="math inline">\(|b|\)</span>) is the measure of effect “size”.</p>
<div id="filter-0.05" class="section level2">
<h2><span class="header-section-number">5.1</span> filter = 0.05</h2>
<pre class="r"><code>table2 &lt;- res_table[pval &lt; 0.05, .(mean_conditional=mean(b),
                         mean_abs.conditional=mean(abs(b)),
                         multiplier = mean(abs(b))/beta), by=.(beta)]
knitr::kable(table2, digits=c(2, 2, 2, 1))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">beta</th>
<th align="right">mean_conditional</th>
<th align="right">mean_abs.conditional</th>
<th align="right">multiplier</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.05</td>
<td align="right">0.32</td>
<td align="right">0.73</td>
<td align="right">14.6</td>
</tr>
<tr class="even">
<td align="right">0.15</td>
<td align="right">0.63</td>
<td align="right">0.74</td>
<td align="right">4.9</td>
</tr>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.83</td>
<td align="right">0.83</td>
<td align="right">1.7</td>
</tr>
<tr class="even">
<td align="right">1.00</td>
<td align="right">1.08</td>
<td align="right">1.08</td>
<td align="right">1.1</td>
</tr>
</tbody>
</table>
</div>
<div id="filter-0.2" class="section level2">
<h2><span class="header-section-number">5.2</span> filter = 0.2</h2>
<pre class="r"><code>table3 &lt;- res_table[pval &lt; 0.2, .(mean_conditional=mean(b),
                         mean_abs.conditional=mean(abs(b)),
                         multiplier = mean(abs(b))/beta), by=.(beta)]
knitr::kable(table3, digits=c(2, 2, 2, 1))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">beta</th>
<th align="right">mean_conditional</th>
<th align="right">mean_abs.conditional</th>
<th align="right">multiplier</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.05</td>
<td align="right">0.16</td>
<td align="right">0.55</td>
<td align="right">11.1</td>
</tr>
<tr class="even">
<td align="right">0.15</td>
<td align="right">0.42</td>
<td align="right">0.57</td>
<td align="right">3.8</td>
</tr>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.69</td>
<td align="right">0.70</td>
<td align="right">1.4</td>
</tr>
<tr class="even">
<td align="right">1.00</td>
<td align="right">1.02</td>
<td align="right">1.02</td>
<td align="right">1.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>for example, in an experiment, if we compare the mean response between a control group and a treated group, the difference in means is the effect. More generally, an effect is the coefficient of a linear model<a href="#fnref1">↩</a></p></li>
</ol>
</div>
