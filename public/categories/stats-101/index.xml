<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stats 101 on R Doodles</title>
    <link>/categories/stats-101/</link>
    <description>Recent content in Stats 101 on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/stats-101/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Textbook error 101 -- A low p-value for the full model does not mean that the model is a good predictor of the response</title>
      <link>/2018/09/a-low-p-value-for-the-full-model-does-not-mean-that-the-model-is-a-good-predictor-of-the-response/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/a-low-p-value-for-the-full-model-does-not-mean-that-the-model-is-a-good-predictor-of-the-response/</guid>
      <description>On page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point average.” Ah no.
library(data.table) library(mvtnorm) rho &amp;lt;- 0.5 n &amp;lt;- 10^5 Sigma &amp;lt;- diag(2) Sigma[1,2] &amp;lt;- Sigma[2,1] &amp;lt;- rho X &amp;lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma) colnames(X) &amp;lt;- c(&amp;quot;X1&amp;quot;, &amp;quot;X2&amp;quot;) beta &amp;lt;- c(0.01, -0.</description>
    </item>
    
    <item>
      <title>On alpha</title>
      <link>/2018/04/on-alpha/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/on-alpha/</guid>
      <description>This post is motivated by Terry McGlynn’s thought provoking How do we move beyond an arbitrary statistical threshold? I have been struggling with the ideas explored in Terry’s post ever since starting my PhD 30 years ago, and its only been in the last couple of years that my own thoughts have begun to gel. This long marination period is largely because of my very classical biostatistical training. My PhD is from the Department of Anatomical Sciences at Stony Brook but the content was geometric morphometrics and James Rohlf was my mentor for morphometrics specifically, and multivariate statistics more generally.</description>
    </item>
    
    <item>
      <title>What is the range of reasonable P-values given a two standard error difference in means?</title>
      <link>/2018/03/what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means/</link>
      <pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means/</guid>
      <description>Here is the motivating quote for this post, from Andrew Gelman’s blog post “Five ways to fix statistics”
 I agree with just about everything in Leek’s article except for this statement: “It’s also impractical to say that statistical metrics such as P values should not be used to make decisions. Sometimes a decision (editorial or funding, say) must be made, and clear guidelines are useful.” Yes, decisions need to be made, but to suggest that p-values be used to make editorial or funding decisions—that’s just horrible.</description>
    </item>
    
  </channel>
</rss>