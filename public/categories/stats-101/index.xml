<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stats 101 on R Doodles</title>
    <link>/categories/stats-101/</link>
    <description>Recent content in Stats 101 on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Jun 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/stats-101/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What does cell biology data look like?</title>
      <link>/2019/06/what-does-cell-biology-data-look-like/</link>
      <pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/what-does-cell-biology-data-look-like/</guid>
      <description>


&lt;p&gt;If I’m going to evaluate the widespread use of t-tests/ANOVAs on count data in bench biology then I’d like to know what these data look like, specifically the shape (“overdispersion”) parameter.&lt;/p&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set up&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(readxl)
library(ggpubr)
library(cowplot)
library(plyr) #mapvalues
library(data.table)

# glm packages
library(MASS)
library(pscl) #zeroinfl
library(DHARMa)
library(mvabund)

  data_path &amp;lt;- &amp;quot;../data&amp;quot; # notebook, console
  source(&amp;quot;../../../R/clean_labels.R&amp;quot;) # notebook, console&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-the-enteric-nervous-system-promotes-intestinal-health-by-constraining-microbiota-composition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data from The enteric nervous system promotes intestinal health by constraining microbiota composition&lt;/h1&gt;
&lt;div id=&#34;import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_enteric &amp;lt;- function(sheet_i, range_i, file_path, wide_2_long=TRUE){
  dt_wide &amp;lt;- data.table(read_excel(file_path, sheet=sheet_i, range=range_i))
  dt_long &amp;lt;- na.omit(melt(dt_wide, measure.vars=colnames(dt_wide), variable.name=&amp;quot;treatment&amp;quot;, value.name=&amp;quot;count&amp;quot;))
  return(dt_long)
}

folder &amp;lt;- &amp;quot;Data from The enteric nervous system promotes intestinal health by constraining microbiota composition&amp;quot;
fn &amp;lt;- &amp;quot;journal.pbio.2000689.s008.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, fn, sep=&amp;quot;/&amp;quot;)
fig1c &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;a2:b11&amp;quot;, file_path)
fig1e &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;d2:g31&amp;quot;, file_path)
fig1f &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;i2:l53&amp;quot;, file_path)
fig2a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 2&amp;quot;, range_i=&amp;quot;a2:d33&amp;quot;, file_path)
fig2d &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 2&amp;quot;, range_i=&amp;quot;F2:I24&amp;quot;, file_path)
fig3a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 3&amp;quot;, range_i=&amp;quot;a2:c24&amp;quot;, file_path)
fig3b &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 3&amp;quot;, range_i=&amp;quot;e2:g12&amp;quot;, file_path)
fig4a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 4&amp;quot;, range_i=&amp;quot;a2:b125&amp;quot;, file_path)
fig5c &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 5&amp;quot;, range_i=&amp;quot;i2:l205&amp;quot;, file_path)
fig6d &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 6&amp;quot;, range_i=&amp;quot;I2:L16&amp;quot;, file_path)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimates-of-the-shape-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimates of the shape parameter&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_enteric &amp;lt;- function(fig_i, fig_num=NULL){
  fit &amp;lt;- glm.nb(count ~ treatment, data=fig_i)

  #fig_num &amp;lt;- names(fig_i)
  if(is.null(fig_num)){
    fig_num &amp;lt;- deparse(substitute(fig_i)) # this works when df is sent but not a list element
  }
  theta &amp;lt;- fit$theta
  fit_title &amp;lt;- paste0(fig_num, &amp;quot; (theta = &amp;quot;, round(theta,1), &amp;quot;)&amp;quot;)
  gg &amp;lt;- ggdotplot(fig_i,
           x=&amp;quot;treatment&amp;quot;, 
           y=&amp;quot;count&amp;quot;,
           color=&amp;quot;treatment&amp;quot;,
           pallete=&amp;quot;jco&amp;quot;,
           add=&amp;quot;mean&amp;quot;) +
    #annotate(&amp;quot;text&amp;quot;, x=1, y= max(fig_i[, count]), label=paste(&amp;quot;theta =&amp;quot;, round(theta,1))) +
    ggtitle(fit_title) +
    rremove(&amp;quot;legend&amp;quot;) +
    NULL
  return(gg)
}

plot_enteric2 &amp;lt;- function(fig_i, fig_num, i){
  fit &amp;lt;- glm.nb(count ~ treatment, data=fig_i[[i]])
  #fig_no &amp;lt;- deparse(substitute(fig_i)) # this works when df is sent but not a list element
  #fig_no &amp;lt;- names(fig_i)
  theta &amp;lt;- fit$theta
  fit_title &amp;lt;- paste0(fig_num[[i]], &amp;quot; (theta = &amp;quot;, round(theta,1), &amp;quot;)&amp;quot;)
  gg &amp;lt;- ggdotplot(fig_i[[i]],
           x=&amp;quot;treatment&amp;quot;, 
           y=&amp;quot;count&amp;quot;,
           color=&amp;quot;treatment&amp;quot;,
           pallete=&amp;quot;jco&amp;quot;,
           add=&amp;quot;mean&amp;quot;) +
    #annotate(&amp;quot;text&amp;quot;, x=1, y= max(fig_i[, count]), label=paste(&amp;quot;theta =&amp;quot;, round(theta,1))) +
    ggtitle(fit_title) +
    rremove(&amp;quot;legend&amp;quot;) +
    NULL
  return(gg)
}

fig_list_names &amp;lt;- c(&amp;quot;fig1c&amp;quot;, &amp;quot;fig1e&amp;quot;, &amp;quot;fig1f&amp;quot;, &amp;quot;fig2a&amp;quot;, &amp;quot;fig2d&amp;quot;, &amp;quot;fig3a&amp;quot;, &amp;quot;fig3b&amp;quot;, &amp;quot;fig4a&amp;quot;, &amp;quot;fig5c&amp;quot;, &amp;quot;fig6d&amp;quot;)
fig_list &amp;lt;- list(fig1c, fig1e, fig1f, fig2a, fig2d, fig3a, fig3b, fig4a, fig5c, fig6d)
names(fig_list) &amp;lt;- fig_list_names # super kludgy
# this doesn&amp;#39;t work
# gg_list &amp;lt;- lapply(fig_list, plot_enteric, names(fig_list))

# this works but requires i in the function which is unsatifying
#gg_list &amp;lt;- lapply(seq_along(fig_list), plot_enteric2, fig_i=fig_list, fig_num=names(fig_list))
gg_list &amp;lt;- list(NULL)
for(i in 1:length(fig_list)){
  gg_list[[i]] &amp;lt;- plot_enteric(fig_list[[i]], names(fig_list)[[i]])
}

plot_grid(plotlist=gg_list, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-09-what-does-cell-biology-data-look-like_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-organic-cation-transporter-3-oct3-is-a-distinct-catecholamines-clearance-route-in-adipocytes-mediating-the-beiging-of-white-adipose-tissue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data from Organic cation transporter 3 (Oct3) is a distinct catecholamines clearance route in adipocytes mediating the beiging of white adipose tissue&lt;/h1&gt;
&lt;div id=&#34;import-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;folder &amp;lt;- &amp;quot;Data from Organic cation transporter 3 (Oct3) is a distinct catecholamines clearance route in adipocytes mediating the beiging of white adipose tissue&amp;quot;
fn &amp;lt;- &amp;quot;journal.pbio.2006571.s012.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, fn, sep=&amp;quot;/&amp;quot;)
fig5b &amp;lt;- read_enteric(sheet_i=&amp;quot;Fig 5B&amp;quot;, range_i=&amp;quot;b2:c12&amp;quot;, file_path)
plot_enteric(fig5b)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimates-of-the-shape-parameter-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimates of the shape parameter&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-of-simulated-samples-that-differ-in-mu-and-theta&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plots of simulated samples that differ in &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST -- Update</title>
      <link>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../../../2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/&#34;&gt;Update to the earlier post&lt;/a&gt;, which was written in response to my own thinking about how to teach stastics to experimental biologists working in fields that are dominated by hypothesis testing instead of estimation. That is, should these researchers learn GLMs or is a t-test on raw or log-transformed data on something like count data good enough – or even superior? My post was written without the benefit of either [Ives](Ives, Anthony R. “For testing the significance of regression coefficients, go ahead and log‐transform count data.” Methods in Ecology and Evolution 6, no. 7 (2015): 828-835) or &lt;a href=&#34;Warton,%20D.I.,%20Lyons,%20M.,%20Stoklosa,%20J.%20and%20Ives,%20A.R.,%202016.%20Three%20points%20to%20consider%20when%20choosing%20a%20LM%20or%20GLM%20test%20for%20count%20data.%20Methods%20in%20Ecology%20and%20Evolution,%207(8),%20pp.882-890&#34;&gt;Warton et al.&lt;/a&gt;. With hindsight, I do vaguely recall Ives, and my previous results support his conclusions, but I was unaware of Warton.&lt;/p&gt;
&lt;p&gt;Warton et al is a fabulous paper. A must read. A question that I have is, &lt;em&gt;under the null&lt;/em&gt; isn’t the response itself exchangeable, so that residuals are unnecessary? Regardless, the implementation in the mvabund package is way faster than my own R-scripted permutation. So here is my earlier simulation in light of Warton et al.&lt;/p&gt;
&lt;p&gt;TL;DR – If we live and die by NHST, then we want to choose a test with good Type I error control but has high power. The quasi-poisson both estimates an interpretable effect (unlike a t-test of log(y +1)) and has good Type I control with high power.&lt;/p&gt;
&lt;p&gt;A bit longer: The quasi-poisson LRT and the permutation NB have good Type I control and high power. The NB Wald and LRT have too liberal Type I control. The t-test of log response has good Type I control and high power at low &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; but is slightly inferior to the glm with increased &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The t-test, Welch, and Wilcoxan have conservative Type I control. Of these, the Wilcoxan has higher power than the t-test and Welch but not as high as the GLMs or log-transformed response.&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(MASS)
library(mvabund)
library(lmtest)
library(nlme)
library(data.table)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Single factor with two levels and a count (negative binomial) response.&lt;/li&gt;
&lt;li&gt;Relative effect sizes of 0%, 100%, and 200%&lt;/li&gt;
&lt;li&gt;Ref count of 4&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of 5 and 10&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;p&lt;/em&gt;-values computed from&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;t-test on raw response&lt;/li&gt;
&lt;li&gt;Welch t-test on raw response&lt;/li&gt;
&lt;li&gt;t-test on log transformed response&lt;/li&gt;
&lt;li&gt;Wilcoxan test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link using Wald test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link using LRT&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and permutation test (using PIT residuals)&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;glm with quasi-poisson family and log-link using LRT&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_sim &amp;lt;- function(sim_space=NULL, niter=1000, nperm=1000, algebra=FALSE){
  # the function was run with n=1000 and the data saved. on subsequent runs
  # the data are loaded from a file
  # the function creates three different objects to return, the object
  # return is specified by &amp;quot;return_object&amp;quot; = NULL, plot_data1, plot_data2
  
  set.seed(1)
  
  methods &amp;lt;- c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;, &amp;quot;nb.x2&amp;quot;, &amp;quot;nb.perm&amp;quot;, &amp;quot;qp&amp;quot;)
  p_table &amp;lt;- data.table(NULL)
  
  if(is.null(sim_space)){
    mu_0_list &amp;lt;- c(4) # control count
    theta_list &amp;lt;- c(0.5) # dispersion
    effect_list &amp;lt;- c(1) # effect size will be 1X, 1.5X, 2X, 3X
    n_list &amp;lt;- c(10) # sample size
    sim_space &amp;lt;- data.table(expand.grid(theta=theta_list, mu_0=mu_0_list, effect=effect_list, n=n_list))
  }
  
  res_table &amp;lt;- data.table(NULL)
  i &amp;lt;- 1 # this is just for debugging
  for(i in 1:nrow(sim_space)){
    # construct clean results table 
    p_table_part &amp;lt;- matrix(NA, nrow=niter, ncol=length(methods))
    colnames(p_table_part) &amp;lt;- methods
    
    # parameters of simulation
    theta_i &amp;lt;- sim_space[i, theta]
    mu_0_i &amp;lt;- sim_space[i, mu_0]
    effect_i &amp;lt;- sim_space[i, effect]
    n_i &amp;lt;- sim_space[i, n]
    treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Trt&amp;quot;), each=n_i)
    fd &amp;lt;- data.table(treatment=treatment)
    
    # mu (using algebra)
    if(algebra==TRUE){
      X &amp;lt;- model.matrix(~treatment)
      beta_0 &amp;lt;- log(mu_0_i)
      beta_1 &amp;lt;- log(effect_i*mu_0_i) - beta_0
      beta &amp;lt;- c(beta_0, beta_1)
      mu_i &amp;lt;- exp((X%*%beta)[,1])
    }else{ #  using R
      mu_vec &amp;lt;- c(mu_0_i, mu_0_i*effect_i)
      mu_i &amp;lt;- rep(mu_vec, each=n_i)
    }
    nb.error &amp;lt;- numeric(niter)
    
    for(iter in 1:niter){
      set.seed(niter*(i-1) + iter)
      fd[, y:=rnegbin(n=n_i*2, mu=mu_i, theta=theta_i)]
      fd[, log_yp1:=log10(y+1)]
      
      p.t &amp;lt;- t.test(y~treatment, data=fd, var.equal=TRUE)$p.value
      p.welch &amp;lt;- t.test(y~treatment, data=fd, var.equal=FALSE)$p.value
      p.log &amp;lt;- t.test(log_yp1~treatment, data=fd, var.equal=TRUE)$p.value
      p.wilcox &amp;lt;- wilcox.test(y~treatment, data=fd, exact=FALSE)$p.value
      
      # weighted lm, this will be ~same as welch for k=2 groups
      # fit &amp;lt;- gls(y~treatment, data=fd, weights = varIdent(form=~1|treatment), method=&amp;quot;ML&amp;quot;)
      # p.wls &amp;lt;- coef(summary(fit))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;p-value&amp;quot;]
      
      # negative binomial
      # default test using summary is Wald.
      # anova(fit) uses chisq of sequential fit, but using same estimate of theta
      # anova(fit2, fit1), uses chisq but with different estimate of theta
      # lrtest(fit) same as anova(fit2, fit1)
      
      # m1 &amp;lt;- glm.nb(y~treatment, data=fd)
      # m0 &amp;lt;- glm.nb(y~1, data=fd)
      # p.nb.x2 &amp;lt;- anova(m0, m1)[2, &amp;quot;Pr(Chi)&amp;quot;]
      # lr &amp;lt;- 2*(logLik(m1) - logLik(m0))
      # df.x2 = m0$df.residual-m1$df.residual
      # p.nb.x2 &amp;lt;- pchisq(lr, df=df.x2, lower.tail = F)
                
      m1 &amp;lt;- manyglm(y~treatment, data=fd) # default theta estimation &amp;quot;PHI&amp;quot;
      m0 &amp;lt;- manyglm(y~1, data=fd)
      lr &amp;lt;- 2*(logLik(m1) - logLik(m0))
      df.x2 = m0$df.residual-m1$df.residual
      p.nb &amp;lt;- coef(summary(m1))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;Pr(&amp;gt;wald)&amp;quot;] # Wald
      p.nb.x2 &amp;lt;- as.numeric(pchisq(lr, df=df.x2, lower.tail = F))
      p.nb.perm &amp;lt;- (anova(m0, m1, nBoot=nperm, show.time=&amp;#39;none&amp;#39;, p.uni=&amp;quot;unadjusted&amp;quot;)$uni.p)[2,1]

      # p.nb.x2 &amp;lt;- lrtest(fit)[2, &amp;quot;Pr(&amp;gt;Chisq)&amp;quot;] # doesn&amp;#39;t work with a data.table
      
      # quasipoisson
      fit &amp;lt;- glm(y~treatment, data=fd, family=quasipoisson)
      p.qp &amp;lt;- coeftest(fit)[2, &amp;quot;Pr(&amp;gt;|z|)&amp;quot;]
      
      p_table_part[iter,] &amp;lt;- c(p.t, p.welch, p.log, p.wilcox, p.nb, p.nb.x2, p.nb.perm, p.qp)
      
    } # niter
    p_table &amp;lt;- rbind(p_table, data.table(combo=i,
                                         mu_0=mu_0_i,
                                         effect=effect_i,
                                         n=n_i,
                                         theta=theta_i,
                                         nb.error=nb.error,
                                         p_table_part))
    
  } # combos
  
  return(p_table)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Algebra is slower (duh!)
# start_time &amp;lt;- Sys.time()
# do_sim(niter=niter, algebra=FALSE)
# end_time &amp;lt;- Sys.time()
# end_time - start_time
# 
# start_time &amp;lt;- Sys.time()
# do_sim(niter=niter, algebra=TRUE)
# end_time &amp;lt;- Sys.time()
# end_time - start_time

n_iter &amp;lt;- 2000
n_perm &amp;lt;- 2000
mu_0_list &amp;lt;- c(4) # control count
theta_list &amp;lt;- c(0.5) # dispersion
effect_list &amp;lt;- c(1, 2, 4) # effect size will be 1X, 1.5X, 2X, 3X
n_list &amp;lt;- c(5, 10) # sample size
sim_space &amp;lt;- data.table(expand.grid(theta=theta_list, mu_0=mu_0_list, effect=effect_list, n=n_list))

do_it &amp;lt;- FALSE # if FALSE the results are available as a file
if(do_it==TRUE){
  p_table &amp;lt;- do_sim(sim_space, niter=n_iter, nperm=n_perm)
  write.table(p_table, &amp;quot;../output/glm-v-lm.0004.txt&amp;quot;, row.names = FALSE, quote=FALSE)
}else{
  p_table &amp;lt;- fread(&amp;quot;../output/glm-v-lm.0001.txt&amp;quot;)
  p_table[, combo:=paste(effect, n, sep=&amp;quot;-&amp;quot;)]
  ycols &amp;lt;- setdiff(colnames(p_table), c(&amp;quot;combo&amp;quot;, &amp;quot;mu_0&amp;quot;, &amp;quot;effect&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;))
  res_table &amp;lt;- data.table(NULL)
  for(i in p_table[, unique(combo)]){
    p_table_part &amp;lt;- p_table[combo==i, ]
    mu_0_i &amp;lt;- p_table_part[1, mu_0]
    effect_i &amp;lt;- p_table_part[1, effect]
    n_i &amp;lt;- p_table_part[1, n]
    theta_i &amp;lt;- p_table_part[1, theta]
    n_iter_i &amp;lt;- nrow(p_table_part)
    p_sum &amp;lt;- apply(p_table_part[, .SD, .SDcols=ycols], 2, function(x) length(which(x &amp;lt;= 0.05))/n_iter_i)
    res_table &amp;lt;- rbind(res_table, data.table(mu_0 = mu_0_i,
                                             effect = effect_i,
                                             n = n_i,
                                             theta = theta_i,
                                             t(p_sum)))    
  }
  res_table[, n:=factor(n)]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;p&gt;Key: 1. “nb” uses the Wald test of negative binomial model. 2. “nb.x2” uses the LRT of negative binomial model. 3. “nb.perm” uses a permutation test on PIT residuals of negative binomial model 4. qp uses a LRT of quasi-poisson model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[effect==1,],
             caption = &amp;quot;Type 1 error as a function of n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:type-1-table&#34;&gt;Table 1: &lt;/span&gt;Type 1 error as a function of n&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;mu_0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;effect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;theta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Welch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Wilcoxan&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.perm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.032&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1280&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.054&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.036&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0505&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0675&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[effect!=1,],
             caption = &amp;quot;Power as a function of n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 2: &lt;/span&gt;Power as a function of n&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;mu_0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;effect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;theta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Welch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Wilcoxan&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.perm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0465&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0845&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0825&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1950&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1860&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1025&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1730&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1850&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1285&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5345&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4405&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;plot&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- melt(res_table, 
            id.vars=c(&amp;quot;mu_0&amp;quot;, &amp;quot;effect&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;, &amp;quot;nb.error&amp;quot;),
            measure.vars=c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;, &amp;quot;nb.x2&amp;quot;, &amp;quot;nb.perm&amp;quot;, &amp;quot;qp&amp;quot;),
            variable.name=&amp;quot;model&amp;quot;,
            value.name=&amp;quot;frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[effect==1,], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(. ~ effect, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-30-glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update_files/figure-html/type-1-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[effect!=1,], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(. ~ effect, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-30-glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update_files/figure-html/plower-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Warton, D.I., Thibaut, L., Wang, Y.A., 2017. The PIT-trap—A “model-free” bootstrap procedure for inference about regression models with discrete, multivariate responses. PLOS ONE 12, e0181790. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0181790&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0181790&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Should we be skeptical of a &#34;large&#34; effect size if p &gt; 0.05?</title>
      <link>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</guid>
      <description>


&lt;p&gt;Motivator: A twitter comment “Isn’t the implication that the large effect size is a direct byproduct of the lack of power? i.e. that if the the study had more power, the effect size would have been found to be smaller.”&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A thought: our belief in the magnitude of an observed effect should be based on our priors, which, hopefully, are formed from good mechanistic models and not sample size“.&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we observe a large effect but the sample size is small, then should we believe that the effect is strongly inflated?&lt;/li&gt;
&lt;li&gt;If we had measured a larger sample, would the effect be smaller?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But…maybe sample size should influence our prior, because the expected estimated effect &lt;strong&gt;magnitude&lt;/strong&gt; is bigger than than the true effect &lt;em&gt;if the true effect is near zero&lt;/em&gt; &lt;a href=&#34;../../../2019/04/the-statistical-significance-filter/&#34;&gt;explored a bit here&lt;/a&gt;. This is because, if an effect is near zero, estimates will vary on both sides of zero, and the absolute value of most of these estimates will be bigger than the absolute value of the true effect. But what effect size should we worry about this?&lt;/p&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: magrittr&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;p&gt;Simulate an experiment with two treatment levels (“control” and “treated”), with standardized (&lt;span class=&#34;math inline&#34;&gt;\(\frac{\delta}{\sigma}\)&lt;/span&gt;) effect sizes of 0.05, .1, .2, .3, .5, .8, 1, 2 and sample sizes of 100, 20, and 10. Cohen considered .8 a “large” standardized effect but I’ll leave what is large up to you. Regardless, its worth comparing the results here to observed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100 # per treatment level. power will be a function of effect size
b_array &amp;lt;- c(0.05, 0.1, 0.2, 0.3, 0.5, 0.8, 1, 2)
niter &amp;lt;- 10^4
res_table &amp;lt;- data.table(NULL)
power_table &amp;lt;- data.table(NULL)
for(b1 in b_array){
  y1 &amp;lt;- matrix(rnorm(n*niter), nrow=n)
  y2 &amp;lt;- matrix(rnorm(n*niter), nrow=n) + b1
  d100 &amp;lt;- apply(y2, 2, mean) - apply(y1,2,mean)
  d20 &amp;lt;- apply(y2[1:20,], 2, mean) - apply(y1[1:20,],2,mean)
  d10 &amp;lt;- apply(y2[1:10,], 2, mean) - apply(y1[1:10,],2,mean)
  res_table &amp;lt;- rbind(res_table, data.table(b=b1, d100=d100, d20=d20, d10=d10))
  power_table &amp;lt;- rbind(power_table, data.table(
    b=b1,
    &amp;quot;power (n=100)&amp;quot;=power.t.test(n=100, delta=b1, sd=1)$power,
    &amp;quot;power (n=20)&amp;quot;=power.t.test(n=20, delta=b1, sd=1)$power,
    &amp;quot;power (n=10)&amp;quot;=power.t.test(n=10, delta=b1, sd=1)$power
  ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;power-for-each-simulation-combination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power for each simulation combination&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(power_table, digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;absolute-median-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Absolute median effects&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[, .(&amp;quot;median(|b|) (n=100)&amp;quot;=median(abs(d100)),
              &amp;quot;median(|b|) (n=20)&amp;quot;=median(abs(d20)),
              &amp;quot;median(|b|) (n=10)&amp;quot;=median(abs(d10))
              ), by=b], digits=c(2, 3, 3, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.305&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.121&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.227&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.320&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.254&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.329&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.297&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.319&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.378&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.503&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.510&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.799&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.799&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.798&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.003&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.005&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;inflation-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inflation factors&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[, .(
              &amp;quot;IF (n=100)&amp;quot; = median(abs(d100))/b,
              &amp;quot;IF (n=20)&amp;quot; = median(abs(d20))/b,
              &amp;quot;IF (n=10)&amp;quot; = median(abs(d10))/b
              ), by=b], digits=c(2, 1, 1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;directly-answering-question-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Directly answering question #2&lt;/h1&gt;
&lt;p&gt;Notice that if power is obove about .2, the absolute median effect is not inflated. That is, a study would have to be wicked underpowered for there to be an expected inflated effect size. This is an indirect answer to question no. 2. A more direct answer is explored by computing the log10 ratio of absolute effects between sample size levels for each run of the simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_table[, d100.d20:=log10(abs(d20)/abs(d100))]
res_table[, d100.d10:=log10(abs(d10)/abs(d100))]
res_table[, d20.d10:=log10(abs(d10)/abs(d20))]
res_melt &amp;lt;- melt(res_table[, .SD, .SDcols=c(&amp;quot;b&amp;quot;, &amp;quot;d100.d20&amp;quot;, &amp;quot;d100.d10&amp;quot;, &amp;quot;d20.d10&amp;quot;)], id.vars=&amp;quot;b&amp;quot;, variable.name=&amp;quot;comparison&amp;quot;, value.name=&amp;quot;contrast&amp;quot;)
res_melt[, b:=factor(b)]
pd &amp;lt;- position_dodge(0.8)
ggplot(data=res_melt, aes(x=b, y=contrast, fill=comparison)) +
  geom_boxplot(position=pd, outlier.shape=NA) +
  coord_cartesian(ylim=c(-1.25, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-28-should-we-be-skeptical-of-a-large-effect-size-if-p-0-05_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the true effect is really small (0.05) then a smaller sample will often estimate a larger effect (just less than 75% of the time when decreasing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; from 100 to 20). When the true effect is about 0.5 or higher, decreasing sample size is no more likely to estimate a bigger effect than increasing sample size.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The original question, and the motivating tweet, raise the question of what a “large” effect is. There is large in the absolute since, which would require subject level expertise to identify, and large relative to noise.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that the original post was not about the &lt;a href=&#34;https://rdoodles.rbind.io/2019/04/the-statistical-significance-filter/&#34;&gt;statistical significance filter&lt;/a&gt; but about the ethics of a RCT in which the observed effect was “large” but there was not enough power to get a statistically significant p-value.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;at least if we are using an experiment to estimate an effect. If we are trying to estimate multiple effects, the bigger observed effects have tend to be inflated and the smaller observed effects tend to be dd&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Blocking vs. covariate adjustment</title>
      <link>/2019/04/blocking-vs-covariate-adjustment/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/blocking-vs-covariate-adjustment/</guid>
      <description>




HUGOMORE42

&lt;p&gt;“A more efficient design would be to first group the rats into homogeneous subsets based on baseline food consumption. This could be done by ranking the rats from heaviest to lightest eaters and then grouping them into pairs by taking the first two rats (the two that ate the most during baseline), then the next two in the list, and so on. The difference from a completely randomised design is that one rat within each pair is randomised to one of the treatment groups, and the other rat is then assigned to the remaining treatment group. Each rat in a pair is expected to eat a similar amount of food during the experiment because they have been matched on their baseline food consumption. By removing this source of variation, the comparison between rats in a pair will be mostly unaffected by the amount of food they eat, allowing treatment effects to be more easily detected.” – Lazic, Stanley E.. Experimental Design for Laboratory Biologists: Maximising Information and Improving Reproducibility . Cambridge University Press. Kindle Edition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)
library(doBy)
library(lmerTest)
library(nlme)

odd &amp;lt;- function(x) x%%2 != 0
even  &amp;lt;- function(x) x%%2 == 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate data in which the response is a function of baseline_food consumption:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10^1
niter &amp;lt;- 1000
# create the response y as a function of baseline_food
out_cols &amp;lt;- c(&amp;quot;adj&amp;quot;, &amp;quot;block.rand&amp;quot;, &amp;quot;block.fix&amp;quot;, &amp;quot;block.adj&amp;quot;)
b_mat &amp;lt;- p_mat &amp;lt;- matrix(NA, nrow=niter, ncol=length(out_cols))
colnames(b_mat) &amp;lt;-colnames(p_mat) &amp;lt;- out_cols
for(iter in 1:niter){
  baseline_food &amp;lt;- rnorm(n*2)
  beta_baseline_food &amp;lt;- 0.6
  y &amp;lt;- beta_baseline_food*baseline_food + sqrt(1-beta_baseline_food^2)*rnorm(n*2)
  
  # covariate adjustment
  # add treatment effect to half
  treatment &amp;lt;- as.factor(rep(c(&amp;quot;tr&amp;quot;, &amp;quot;cn&amp;quot;), each=n))
  beta_1 &amp;lt;- 1
  y[1:n] &amp;lt;- y[1:n] + beta_1
  fit1 &amp;lt;- lm(y ~ baseline_food + treatment)
  b_mat[iter, &amp;quot;adj&amp;quot;] &amp;lt;- coef(summary(fit1))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  p_mat[iter, &amp;quot;adj&amp;quot;] &amp;lt;- coef(summary(fit1))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  
  # block
  treatment &amp;lt;- NULL
  for(i in 1:n){
    treatment &amp;lt;- c(treatment, sample(c(&amp;quot;tr&amp;quot;, &amp;quot;cn&amp;quot;), 2))
  }
  fake_data &amp;lt;- data.table(y=y, baseline_food=baseline_food)
  setorder(fake_data, baseline_food)
  fake_data[, treatment:=factor(treatment)]
  fake_data[, block:=factor(rep(1:n, each=2))]
  fake_data[, y_exp:=ifelse(treatment==&amp;quot;tr&amp;quot;, y+1, y)]
  # fit2 &amp;lt;- lmer(y_exp ~ treatment + (1|block), data=fake_data)
  # b_mat[iter, &amp;quot;block&amp;quot;] &amp;lt;- coef(summary(fit2))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  fit2 &amp;lt;- lme(y_exp ~ treatment, random= ~1|block, data=fake_data)
  b_mat[iter, &amp;quot;block.rand&amp;quot;] &amp;lt;- coef(summary(fit2))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Value&amp;quot;]
  p_mat[iter, &amp;quot;block.rand&amp;quot;] &amp;lt;- coef(summary(fit2))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;p-value&amp;quot;]

  fit2b &amp;lt;- lm(y_exp ~ block + treatment, data=fake_data)
  b_mat[iter, &amp;quot;block.fix&amp;quot;] &amp;lt;- coef(summary(fit2b))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  p_mat[iter, &amp;quot;block.fix&amp;quot;] &amp;lt;- coef(summary(fit2b))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]

  fit3 &amp;lt;- lm(y_exp ~ baseline_food + treatment, data=fake_data)
  b_mat[iter, &amp;quot;block.adj&amp;quot;] &amp;lt;- coef(summary(fit3))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  p_mat[iter, &amp;quot;block.adj&amp;quot;] &amp;lt;- coef(summary(fit3))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Estimates&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(b_mat, 2, quantile, probs=c(0.025, 0.5, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             adj block.rand block.fix block.adj
## 2.5%  0.2797401  0.1476697 0.1476697 0.1528887
## 50%   1.0076974  1.0161665 1.0161665 1.0165686
## 97.5% 1.7149816  1.8121259 1.8121259 1.7995946&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Power&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(p_mat, 2, function(x) sum(x &amp;lt; 0.05)/niter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        adj block.rand  block.fix  block.adj 
##      0.732      0.576      0.558      0.619&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusion: over this model space, simply adjusting for baseline food consumption is more powerful than creating blocks using baseline food consumption.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The statistical significance filter</title>
      <link>/2019/04/the-statistical-significance-filter/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/the-statistical-significance-filter/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-reported-effect-sizes-are-inflated&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Why reported effect sizes are inflated&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setup&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploration-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Exploration 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unconditional-means-power-and-sign-error&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Unconditional means, power, and sign error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-means&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Conditional means&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#filter-0.05&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; filter = 0.05&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#filter-0.2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; filter = 0.2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;why-reported-effect-sizes-are-inflated&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Why reported effect sizes are inflated&lt;/h1&gt;
&lt;p&gt;This post is motivated by many discussions in Gelman’s blog &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2011/09/10/the-statistical-significance-filter/&#34;&gt;but start here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we estimate an effect&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, the estimate will be a little inflated or a little diminished relative to the true effect but the expectation of the effect is the true effect. If all effects were reported, there would be no bias toward inflated effects. Reported effects are inflated if we use p-values to decide which to report and which to archive in the file drawer.&lt;/p&gt;
&lt;p&gt;The magnitude of an estimate of an effect is a function of its true effect size plus sampling error (this is with a perfectly designed and executed study. In any real study there will be biases of various sorts). The absolute magnitude of sampling error is bigger with smaller &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The relative magnitude is bigger for smaller true effect size. Consequently, estimates in low powered studies (some combination of low &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and small true effect size) can be wildly off, especially relative to the true effect size. In low powered studies, it is these “wildly-off” estimates that are big enough to have &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; 0.05\)&lt;/span&gt;. This phenomenon attenuates as power increases because estimates are less and less wildely-off.&lt;/p&gt;
&lt;p&gt;Here is a simulation of this&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(data.table)

source(&amp;quot;../R/fake_x.R&amp;quot;) # bookdown&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Exploration 1&lt;/h1&gt;
&lt;p&gt;Modeling a typical set of experiments in ecology or physiology with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; independent responses each with the same standardized effect size. How big are the reported effect sizes for the subset with &lt;span class=&#34;math inline&#34;&gt;\(p.val &amp;lt; 0.05\)&lt;/span&gt; (with or without correction for multiple testing). Make this a function of power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 20 # per treatment level. power will be a function of effect size
np1 &amp;lt;- n+1
N &amp;lt;- 2*n
niter &amp;lt;- 100 # number of iterations for each combination of fake data parameters
treatment_levels &amp;lt;- c(&amp;quot;Cn&amp;quot;, &amp;quot;Tr&amp;quot;)
Treatment &amp;lt;- rep(treatment_levels, each=n)
p &amp;lt;- 50
b &amp;lt;- pval &amp;lt;- numeric(p)
combo &amp;lt;- 0 # which treatment combo
res_table &amp;lt;- data.table(NULL)
for(beta_1 in c(0.05, 0.15, 0.5, 1)){
  combo &amp;lt;- combo + 1
  j1 &amp;lt;- 0
  j &amp;lt;- 0
  res &amp;lt;- matrix(NA, nrow=niter*p, ncol=3)
  colnames(res) &amp;lt;- c(&amp;quot;ID&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;pval&amp;quot;)
  for(iter in 1:niter){
    j1 &amp;lt;- j1 + j # completed row -- start row will be this plus 1
    Y &amp;lt;- matrix(rnorm(n*2*p), nrow=n*2, ncol=p)
    Y[np1:N,] &amp;lt;- Y[np1:N,] + beta_1
    fit &amp;lt;- lm(Y ~ Treatment)
    fit.coefs &amp;lt;- coef(summary(fit))
    for(j in 1:p){# inefficient...how do I extract this without a 
      res[j1 + j, &amp;quot;ID&amp;quot;] &amp;lt;- niter*(combo - 1) + iter
      res[j1 + j, &amp;quot;b&amp;quot;] &amp;lt;- fit.coefs[[j]][&amp;quot;TreatmentTr&amp;quot;, &amp;quot;Estimate&amp;quot;]
      res[j1 + j, &amp;quot;pval&amp;quot;] &amp;lt;- fit.coefs[[j]][&amp;quot;TreatmentTr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
    }
  } # iter
  res_table &amp;lt;- rbind(res_table, data.table(beta=beta_1, res))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unconditional-means-power-and-sign-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Unconditional means, power, and sign error&lt;/h1&gt;
&lt;p&gt;beta is the true effect. The unconditional mean is the mean of the estimated effect. The absolute value of the estimated effect is the measure of “size” or magnitude and the mean of the absolute values of the effect size will be bigger then the mean effect size if the true effect size is near zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table1 &amp;lt;- res_table[, .(mean_unconditional=mean(b),
              mean_abs_unconditional=mean(abs(b)),
              power = sum(pval &amp;lt; 0.05 &amp;amp; b &amp;gt; 0)/niter/p,
              sign.error=sum(pval &amp;lt; 0.05 &amp;amp; b &amp;lt; 0)/niter/p), by=.(beta)]
knitr::kable(table1, digits=c(2, 2, 2, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_unconditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_abs_unconditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sign.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.88&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-means&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Conditional means&lt;/h1&gt;
&lt;p&gt;The conditional mean is the mean effect size conditional on pval &amp;lt; filter. Again, beta is the true effect. And again, the absolute estimate (&lt;span class=&#34;math inline&#34;&gt;\(|b|\)&lt;/span&gt;) is the measure of effect “size”.&lt;/p&gt;
&lt;div id=&#34;filter-0.05&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; filter = 0.05&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table2 &amp;lt;- res_table[pval &amp;lt; 0.05, .(mean_conditional=mean(b),
                         mean_abs.conditional=mean(abs(b)),
                         multiplier = mean(abs(b))/beta), by=.(beta)]
knitr::kable(table2, digits=c(2, 2, 2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_abs.conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;multiplier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;filter-0.2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; filter = 0.2&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table3 &amp;lt;- res_table[pval &amp;lt; 0.2, .(mean_conditional=mean(b),
                         mean_abs.conditional=mean(abs(b)),
                         multiplier = mean(abs(b))/beta), by=.(beta)]
knitr::kable(table3, digits=c(2, 2, 2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_abs.conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;multiplier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.55&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;for example, in an experiment, if we compare the mean response between a control group and a treated group, the difference in means is the effect. More generally, an effect is the coefficient of a linear model&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Covariate adjustment in randomized experiments</title>
      <link>/2019/04/covariate-adjustment-in-randomized-experiments/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/covariate-adjustment-in-randomized-experiments/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://twitter.com/statsepi/status/1115902270888128514&#34;&gt;The post motivated by a tweetorial from Darren Dahly&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In an experiment, do we adjust for covariates that differ between treatment levels measured pre-experiment (“imbalance” in random assignment), where a difference is inferred from a t-test with p &amp;lt; 0.05? Or do we adjust for all covariates, regardless of differences pre-test? Or do we adjust only for covariates that have sustantial correlation with the outcome? Or do we not adjust at all?&lt;/p&gt;
&lt;p&gt;The original tweet focussed on Randomized Clinical Trials, which typically have large sample size. Here I simulate experimental biology, which typically has much smaller n.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(data.table)

source(&amp;quot;../R/fake_x.R&amp;quot;) # bookdown&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fake-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fake data&lt;/h1&gt;
&lt;p&gt;Generate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; correlated variables and assign the first to the response (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt;) and the rest to the covariates (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;). Construct a treatment variable and effect and add this to the response.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100 # per treatment level - this is modified below
p &amp;lt;- 3 # number of covariates (columns of the data)
pp1 &amp;lt;- p+1
beta_0 &amp;lt;- 0 # intercept

niter &amp;lt;- 2000 # modified below
measure_cols &amp;lt;- c(&amp;quot;no_adjust&amp;quot;, &amp;quot;imbalance&amp;quot;, &amp;quot;all_covariates&amp;quot;, &amp;quot;weak_covariates&amp;quot;, &amp;quot;strong_covariates&amp;quot;)

xcols &amp;lt;- paste0(&amp;quot;X&amp;quot;, 1:p)
build_ycols &amp;lt;- c(&amp;quot;Y_o&amp;quot;, xcols)
cor_ycols &amp;lt;- c(&amp;quot;Y&amp;quot;, xcols)

b_mat &amp;lt;- data.table(NULL)
se_mat &amp;lt;- data.table(NULL)
p_mat &amp;lt;- data.table(NULL)
ci_mat &amp;lt;- data.table(NULL)

for(beta_1 in c(0, 0.2, 0.8)){ # treatment effect on standardized scale
  beta &amp;lt;- c(beta_0, beta_1)
  for(n in c(6, 10, 50)){
    # larger iterations with smaller n
    niter &amp;lt;- round((3*10^4)/sqrt(n), 0)
    niter &amp;lt;- 2000
    
    # repopulate with NA each n
    b &amp;lt;- se &amp;lt;- pval &amp;lt;- ci &amp;lt;- matrix(NA, nrow=niter, ncol=length(measure_cols))
    colnames(b) &amp;lt;- colnames(se) &amp;lt;- colnames(pval) &amp;lt;- colnames(ci) &amp;lt;- measure_cols
    
    Treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Tr&amp;quot;), each=n)
    X &amp;lt;- model.matrix(formula(&amp;quot;~ Treatment&amp;quot;))
    
    for(iter in 1:niter){
      # generate p random, correlated variables. The first is assigned to Y
      fake_data &amp;lt;- fake.X(n*2, pp1, fake.eigenvectors(pp1), fake.eigenvalues(pp1))
      colnames(fake_data) &amp;lt;- build_ycols
      
      # resacale so that var(Y) = 1, where Y is the first column
      fake_data &amp;lt;- fake_data/sd(fake_data[,1])
      
      fake_data &amp;lt;- data.table(fake_data)
      
      # view the scatterplots
      #gg &amp;lt;- ggpairs(X,progress = ggmatrix_progress(clear = FALSE))
      show_it &amp;lt;- FALSE
      if(show_it ==TRUE){
        gg &amp;lt;- ggpairs(fake_data)
        print(gg, progress = F)
      }
      
      # add the treatment effect
      fake_data[, Y:=Y_o + X%*%beta]
      fake_data[, Treatment:=Treatment]
      
      # model 1 - just the treatment
      fit1 &amp;lt;- lm(Y ~ Treatment, data=fake_data)
      res &amp;lt;- coef(summary(fit1))[&amp;quot;TreatmentTr&amp;quot;, ]
      b[iter, 1] &amp;lt;- res[&amp;quot;Estimate&amp;quot;]
      se[iter, 1] &amp;lt;- res[&amp;quot;Std. Error&amp;quot;]
      pval[iter, 1] &amp;lt;-res[&amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
      ci_i &amp;lt;- confint(fit1)[&amp;quot;TreatmentTr&amp;quot;,]
      ci[iter, 1] &amp;lt;- ifelse(beta_1 &amp;gt;= ci_i[1] &amp;amp; beta_1 &amp;lt;= ci_i[2], 1, 0)
      res1 &amp;lt;- copy(res)
      
      # model 2 - adjust for imablance
      inc_xcols &amp;lt;- NULL
      for(i in 1:p){
        formula &amp;lt;- paste0(xcols[i], &amp;quot; ~ Treatment&amp;quot;)
        fit2a &amp;lt;- lm(formula, data=fake_data)
        if(coef(summary(fit2a))[&amp;quot;TreatmentTr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; 0.05){
          inc_xcols &amp;lt;- c(inc_xcols, xcols[i])
        }
      }
      if(length(inc_xcols) &amp;gt; 0){ # if any signifianct effects refit, otherwise use old fit
        formula &amp;lt;- paste0(&amp;quot;Y ~ Treatment + &amp;quot;, paste(inc_xcols, collapse=&amp;quot; + &amp;quot;))
        fit2b &amp;lt;- lm(formula, data=fake_data)
        res &amp;lt;- coef(summary(fit2b))[&amp;quot;TreatmentTr&amp;quot;, ]
        ci_i &amp;lt;- confint(fit2b)[&amp;quot;TreatmentTr&amp;quot;,]
      }else{
        res &amp;lt;- res1
      }
      b[iter, 2] &amp;lt;- res[&amp;quot;Estimate&amp;quot;]
      se[iter, 2] &amp;lt;- res[&amp;quot;Std. Error&amp;quot;]
      pval[iter, 2] &amp;lt;-res[&amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
      ci[iter, 2] &amp;lt;- ifelse(beta_1 &amp;gt;= ci_i[1] &amp;amp; beta_1 &amp;lt;= ci_i[2], 1, 0)
      
      
      # model 3- adjust for covariates
      (ycor &amp;lt;- abs(cor(fake_data[, .SD, .SDcols=cor_ycols])[2:pp1, 1]))
      mean(ycor)
      
      j &amp;lt;- 2
      for(target_cor in c(0, .2, .4)){
        j &amp;lt;- j+1
        if(target_cor == 0.2){
          inc &amp;lt;- which(ycor &amp;lt; target_cor) # include only weak covariates
        }else{
          inc &amp;lt;- which(ycor &amp;gt; target_cor) # include all OR strong covariates
        }
        if(length(inc) &amp;gt; 0){  # if matches refit, otherwise use old fit
          inc_xcols &amp;lt;- xcols[inc]
          formula &amp;lt;- paste0(&amp;quot;Y ~ Treatment + &amp;quot;, paste(inc_xcols, collapse=&amp;quot; + &amp;quot;))
          fit3 &amp;lt;- lm(formula, data=fake_data)
          res &amp;lt;- coef(summary(fit3))[&amp;quot;TreatmentTr&amp;quot;, ]
          ci_i &amp;lt;- confint(fit3)[&amp;quot;TreatmentTr&amp;quot;,]
        }else{
          res &amp;lt;- res1
        }
        b[iter, j] &amp;lt;- res[&amp;quot;Estimate&amp;quot;]
        se[iter, j] &amp;lt;- res[&amp;quot;Std. Error&amp;quot;]
        pval[iter, j] &amp;lt;-res[&amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
        ci[iter, j] &amp;lt;- ifelse(beta_1 &amp;gt;= ci_i[1] &amp;amp; beta_1 &amp;lt;= ci_i[2], 1, 0)
      }
    }  
    b_mat &amp;lt;- rbind(b_mat, data.table(n=n, beta_1=beta_1, b))
    se_mat &amp;lt;- rbind(se_mat, data.table(n=n, beta_1=beta_1, se))
    p_mat &amp;lt;- rbind(p_mat, data.table(n=n, beta_1=beta_1, pval))
    ci_mat &amp;lt;- rbind(ci_mat, data.table(n=n, beta_1=beta_1, ci))
  }
}

p_long &amp;lt;- melt(p_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;p&amp;quot;)
ci_long &amp;lt;- melt(ci_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;covers&amp;quot;)
b_long &amp;lt;- melt(b_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;b&amp;quot;)
se_long &amp;lt;- melt(se_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;se&amp;quot;)


#ci_long[, .(coverage=sum(covers)/niter), by=.(method, n, beta_1)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-estimates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distribution of estimates&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pd &amp;lt;- position_dodge(0.8)
gg &amp;lt;- ggplot(data=b_long, aes(x=factor(n), y=b, fill=method)) +
  geom_boxplot(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-se-of-estimate&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distribution of SE of estimate&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pd &amp;lt;- position_dodge(0.8)
gg &amp;lt;- ggplot(data=se_long, aes(x=factor(n), y=se, fill=method)) +
  geom_boxplot(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# type I
p_sum &amp;lt;- p_long[, .(error=sum(p &amp;lt; 0.05)/niter), by=.(method, n, beta_1)]
pd &amp;lt;- position_dodge(0.8)
gg &amp;lt;- ggplot(data=p_sum[beta_1==0], aes(x=factor(n), y=error, color=method, group=method)) +
  geom_point(position=pd) +
  geom_line(position=pd) + 
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  ylab(&amp;quot;Type I error&amp;quot;) +
  # facet_grid(.~beta_1) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# power
# need p_sum from above
gg &amp;lt;- ggplot(data=p_sum[beta_1!=0], aes(x=factor(n), y=error, color=method)) +
  geom_point(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  ylab(&amp;quot;Power&amp;quot;) +
  facet_grid(.~beta_1) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sign-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sign error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sign error
p_long2 &amp;lt;- cbind(p_long, b=b_long[, b])
sign_error &amp;lt;- p_long2[beta_1 &amp;gt; 0, .(error=sum(p &amp;lt; 0.1 &amp;amp; b &amp;lt; 0)/niter), by=.(method, n, beta_1)]
gg &amp;lt;- ggplot(data=sign_error, aes(x=factor(n), y=error, color=method)) +
  geom_point(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  ylab(&amp;quot;Sign error&amp;quot;) +
  facet_grid(.~beta_1) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What to write, and not write, in a results section — an ever-growing list</title>
      <link>/2019/01/what-to-write-and-not-write-in-a-results-section-an-ever-growing-list/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/what-to-write-and-not-write-in-a-results-section-an-ever-growing-list/</guid>
      <description>


&lt;p&gt;“GPP (n=4 per site) increased from the No Wildlife site to the Hippo site but was lowest at the Hippo + WB site (Fig. 6); however, these differences were not significant due to low sample sizes and high variability.” – Subalusky, A.L., Dutton, C.L., Njoroge, L., Rosi, E.J., and Post, D.M. (2018). Organic matter and nutrient inputs from large wildlife influence ecosystem function in the Mara River, Africa. Ecology 99, 2558–2574.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST</title>
      <link>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../../../2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/&#34;&gt;This post has been updated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A skeleton simulation of different strategies for NHST for count data if all we care about is a p-value, as in bench biology where p-values are used to simply give one confidence that something didn’t go terribly wrong (similar to doing experiments in triplicate – it’s not the effect size that matters only “we have experimental evidence of a replicable effect”).&lt;/p&gt;
&lt;p&gt;tl;dr - At least for Type I error at small &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, log(response) and Wilcoxan have the best performance over the simulation space. T-test is a bit conservative. Welch is even more conservative. glm-nb is too liberal.&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(MASS)
library(data.table)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Single factor with two levels and a count (negative binomial) response.&lt;/li&gt;
&lt;li&gt;Relative effect sizes of 0%, 50%, 100%, and 200%&lt;/li&gt;
&lt;li&gt;Ref count of 4, 10, 100&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of 5, 10, 20, 40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;p&lt;/em&gt;-values computed from&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;t-test on raw response&lt;/li&gt;
&lt;li&gt;Welch t-test on raw response&lt;/li&gt;
&lt;li&gt;t-test on log transformed response&lt;/li&gt;
&lt;li&gt;Wilcoxan test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_sim &amp;lt;- function(niter=1, return_object=NULL){
  # the function was run with n=1000 and the data saved. on subsequent runs
  # the data are loaded from a file
  # the function creates three different objects to return, the object
  # return is specified by &amp;quot;return_object&amp;quot; = NULL, plot_data1, plot_data2
  methods &amp;lt;- c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;)
  p_table_part &amp;lt;- matrix(NA, nrow=niter, ncol=length(methods))
  colnames(p_table_part) &amp;lt;- methods
  p_table &amp;lt;- data.table(NULL)
  
  res_table &amp;lt;- data.table(NULL)
  beta_0_list &amp;lt;- c(4, 10, 100) # control count
  theta_list &amp;lt;- c(0.5, 1, 100) # dispersion
  effect_list &amp;lt;- c(1:3, 5) # relative effect size will be 0%, 50%, 100%, 200%
  n_list &amp;lt;- c(5, 10, 20, 40) # sample size
  n_rows &amp;lt;- length(beta_0_list)*length(theta_list)*length(effect_list)*length(n_list)*niter
  sim_space &amp;lt;- expand.grid(theta_list, beta_0_list, effect_list, n_list)
  plot_data1 &amp;lt;- data.table(NULL)
  plot_data2 &amp;lt;- data.table(NULL)
  debug_table &amp;lt;- data.table(matrix(NA, nrow=niter, ncol=2))
  setnames(debug_table, old=colnames(debug_table), new=c(&amp;quot;seed&amp;quot;,&amp;quot;model&amp;quot;))
  debug_table[, seed:=as.integer(seed)]
  debug_table[, model:=as.character(model)]
  i &amp;lt;- 0
  for(theta_i in theta_list){
    for(beta_0 in beta_0_list){
      # first get plots of distributions given parameters
      y &amp;lt;- rnegbin(n=10^4, mu=beta_0, theta=theta_i)
      x_i &amp;lt;- seq(min(y), max(y), by=1)
      prob_x_i &amp;lt;- dnbinom(x_i, size=theta_i, mu=beta_0)
      plot_data1 &amp;lt;- rbind(plot_data1, data.table(
        theta=theta_i,
        mu=beta_0,
        x=x_i,
        prob_x=prob_x_i
      ))
      # the simulation
      for(effect in effect_list){
        for(n in n_list){
          beta_1 &amp;lt;- (effect-1)*beta_0/2 # 0% 50% 100%

          do_manual &amp;lt;- FALSE
          if(do_manual==TRUE){
            theta_i &amp;lt;- res_table[row, theta]
            beta_0 &amp;lt;- res_table[row, beta_0]
            beta_1 &amp;lt;- res_table[row, beta_1]
            n &amp;lt;- res_table[row, n]
          }
          
          beta &amp;lt;- c(beta_0, beta_1)
          treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Trt&amp;quot;), each=n)
          X &amp;lt;- model.matrix(~treatment)
          mu &amp;lt;- (X%*%beta)[,1]
          fd &amp;lt;- data.table(treatment=treatment, y=NA)
          for(iter in 1:niter){
            i &amp;lt;- i+1
            set.seed(i)
            fd[, y:=rnegbin(n=n*2, mu=mu, theta=theta_i)]
            fd[, log_yp1:=log10(y+1)]
            p.t &amp;lt;- t.test(y~treatment, data=fd, var.equal=TRUE)$p.value
            p.welch &amp;lt;- t.test(y~treatment, data=fd, var.equal=FALSE)$p.value
            p.log &amp;lt;- t.test(log_yp1~treatment, data=fd, var.equal=TRUE)$p.value
            p.wilcox &amp;lt;- wilcox.test(y~treatment, data=fd, exact=FALSE)$p.value
            fit &amp;lt;- glm.nb(y~treatment, data=fd)
            debug_table[iter, seed:=i]
            debug_table[iter, model:=&amp;quot;glm.nb&amp;quot;]
            #if(fit$th.warn == &amp;quot;iteration limit reached&amp;quot;){
            if(!is.null(fit$th.warn)){
              fit &amp;lt;- glm(y~treatment, data=fd, family=poisson)
              debug_table[iter, model:=&amp;quot;poisson&amp;quot;]
            }
            p.nb &amp;lt;- coef(summary(fit))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;Pr(&amp;gt;|z|)&amp;quot;]
            p_table_part[iter,] &amp;lt;- c(p.t, p.welch, p.log, p.wilcox, p.nb)
          }
          p_table &amp;lt;- rbind(p_table, data.table(p_table_part, debug_table))
          p_sum &amp;lt;- apply(p_table_part, 2, function(x) length(which(x &amp;lt;= 0.05))/niter)
          res_table &amp;lt;- rbind(res_table, data.table(beta_0=beta_0,
                                                   beta_1=beta_1,
                                                   n=n,
                                                   theta=theta_i,
                                                   t(p_sum)))
        } # n
      } # effect
      plot_data2 &amp;lt;- rbind(plot_data2, data.table(
        theta=theta_i,
        mu=beta_0,
        n_i=n,
        beta1=beta_1,
        x=treatment,
        y=fd[, y]
      ))
    }
  }
  if(is.null(return_object)){return(res_table)}else{
    if(return_object==&amp;quot;plot_data1&amp;quot;){return(plot_data1)}
    if(return_object==&amp;quot;plot_data2&amp;quot;){return(plot_data2)}
    
  }
  
}

do_it &amp;lt;- FALSE # if FALSE the results are available as a file
if(do_it==TRUE){
  res_table &amp;lt;- do_sim(niter=1000)
  write.table(res_table, &amp;quot;../output/glm-t-wilcoxon.txt&amp;quot;, row.names = FALSE, quote=FALSE)
}else{
  plot_data &amp;lt;- do_sim(niter=1, return_object=&amp;quot;plot_data2&amp;quot;)
  res_table &amp;lt;- fread(&amp;quot;../output/glm-t-wilcoxon.txt&amp;quot;)
  res_table[, n:=factor(n)]
}
#res_table&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Distribution of the response for the 3 x 3 simulation space&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extreme inelegance
mu_levels &amp;lt;- unique(plot_data[, mu])
theta_levels &amp;lt;- unique(plot_data[, theta])
show_function &amp;lt;- FALSE
show_violin &amp;lt;- TRUE

if(show_function==TRUE){
  gg1 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg2 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg3 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg4 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg5 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg6 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg7 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
  gg8 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
  gg9 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
}

if(show_violin==TRUE){
  gg1 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg2 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg3 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg4 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg5 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg6 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg7 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
  gg8 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
  gg9 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
}

gg_example &amp;lt;- plot_grid(gg1, gg2, gg3, gg4, gg5, gg6, gg7, gg8, gg9, 
          nrow=3,
          labels=c(paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[3]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[3]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[3])),
          label_size = 10, label_x=0.1)
gg_example&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- melt(res_table, 
            id.vars=c(&amp;quot;beta_0&amp;quot;, &amp;quot;beta_1&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;),
            measure.vars=c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;),
            variable.name=&amp;quot;model&amp;quot;,
            value.name=&amp;quot;frequency&amp;quot;)
# res[, beta_0:=factor(beta_0)]
# res[, beta_1:=factor(beta_1)]
# res[, theta:=factor(theta)]
# res[, n:=factor(n)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[beta_1==0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_0 ~ theta, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/type%20I-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ouch. glm-nb with hih error rates especially when n is small and the scale parameter is small&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0_levels &amp;lt;- unique(res$beta_0)
# small count
gg1 &amp;lt;- ggplot(data=res[beta_0==b0_levels[1] &amp;amp; beta_1 &amp;gt; 0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_1 ~ theta, labeller=label_both) +
  NULL

# large count
gg2 &amp;lt;- ggplot(data=res[beta_0==b0_levels[3] &amp;amp; beta_1 &amp;gt; 0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_1 ~ theta, labeller=label_both) +
  NULL

gg1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/power-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/power-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;glm-nb has higher power, especially at small n, but at a type I cost.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reporting effects as relative differences...with a confidence interval</title>
      <link>/2018/11/reporting-effects-as-relative-differences-with-a-confidence-interval/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/reporting-effects-as-relative-differences-with-a-confidence-interval/</guid>
      <description>


&lt;p&gt;Researchers frequently report results as relative effects, for example,&lt;/p&gt;
&lt;p&gt;“Male flies from selected lines had 50% larger upwind flight ability than male flies from control lines (Control mean: 117.5 cm/s; Selected mean 176.5 cm/s).”&lt;/p&gt;
&lt;p&gt;where a relative effect is&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
100 \frac{\bar{y}_B - \bar{y}_A}{\bar{y}_A}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;If we are to follow best practices, we should present this effect with a measure of uncertainty, such as a confidence interval. The absolute effect is 59.0 cm/s and the 95% CI of this effect is (48.7, 69.3 cm/s). But if we present the result as a relative effect, or percent difference from some reference value, how do we compute a “relative CI”?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/notebooks/relative_standard_errors.nb.html&#34;&gt;Read the whole post here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Textbook error 101 -- A low p-value for the full model does not mean that the model is a good predictor of the response</title>
      <link>/2018/09/a-low-p-value-for-the-full-model-does-not-mean-that-the-model-is-a-good-predictor-of-the-response/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/a-low-p-value-for-the-full-model-does-not-mean-that-the-model-is-a-good-predictor-of-the-response/</guid>
      <description>


&lt;p&gt;On page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point average.” Ah no.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(mvtnorm)
rho &amp;lt;- 0.5
n &amp;lt;- 10^5
Sigma &amp;lt;- diag(2)
Sigma[1,2] &amp;lt;- Sigma[2,1] &amp;lt;- rho
X &amp;lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma)
colnames(X) &amp;lt;- c(&amp;quot;X1&amp;quot;, &amp;quot;X2&amp;quot;)
beta &amp;lt;- c(0.01, -0.02)
y &amp;lt;- X%*%beta + rnorm(n)
fit &amp;lt;- lm(y ~ X)
summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2513 -0.6704 -0.0026  0.6701  4.1725 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.003597   0.003158  -1.139    0.255    
## XX1          0.005449   0.003629   1.502    0.133    
## XX2         -0.016562   0.003641  -4.548 5.41e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9987 on 99997 degrees of freedom
## Multiple R-squared:  0.0002146,  Adjusted R-squared:  0.0001946 
## F-statistic: 10.73 on 2 and 99997 DF,  p-value: 2.184e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A p-value is not a measure of the predictive capacity of a model because the p-value is a function of 1) signal, 2) noise (unmodeled error), and 3) sample size while predictive capacity is a function of the signal:noise ratio. If the signal:noise ratio is tiny, the predictive capacity is small but the p-value can be tiny if the sample size is large.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On alpha</title>
      <link>/2018/04/on-alpha/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/on-alpha/</guid>
      <description>


&lt;p&gt;This post is motivated by Terry McGlynn’s thought provoking &lt;a href=&#34;https://smallpondscience.com/2018/04/23/how-do-we-move-beyond-an-arbitrary-statistical-threshold/&#34;&gt;How do we move beyond an arbitrary statistical threshold?&lt;/a&gt; I have been struggling with the ideas explored in Terry’s post ever since starting my PhD 30 years ago, and its only been in the last couple of years that my own thoughts have begun to gel. This long marination period is largely because of my very classical biostatistical training. My PhD is from the Department of Anatomical Sciences at Stony Brook but the content was geometric morphometrics and James Rohlf was my mentor for morphometrics specifically, and multivariate statistics more generally. The last year of my PhD, I was Robert Sokal’s RA (I was the programmer!) and got two co-authored papers with him. I invested a tremendous amount of time generating little statistical doodles (first in Excel, then in Pascal, and then in R) to better understand ANOVA, and permutation tests, and the bootstrap, and similar frequentist tools.&lt;/p&gt;
&lt;p&gt;My answer is partly answered by my two posts to the &lt;a href=&#34;https://rapidecology.com&#34;&gt;Rapid Ecology&lt;/a&gt; blog. The first – &lt;a href=&#34;https://rapidecology.com/2018/04/09/when-do-we-introduce-best-statistical-practices-to-undergraduate-biology-majors/&#34;&gt;When do we introduce best statistical practices to undergraduate biology majors?&lt;/a&gt; was posted April 9. The second – “Abandon ANOVA-type experiments” is a more radical answer, and is scheduled to appear in a couple of weeks.&lt;/p&gt;
&lt;p&gt;Here, I expand on the second post but make it more general. Terry finishes his post with the statement “To be clear, I’m not arguing (here) that we should be ditching the hypothesis falsification approach to answering questions”. Maybe he’s arguing this elsewhere. Regardless, I &lt;em&gt;am&lt;/em&gt; arguing that here. I am &lt;em&gt;not&lt;/em&gt; arguing against the use of p-values (here!) – simply against the concept of comparing a p-value to a type I error rate (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The practice of comparing a p-value to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and classifying a result as “significant” or “non-significant” has led to the cargo-cult science practice of “discovery by p-value.” Many scientists literally believe they have discovered something about the world because they found p &amp;lt; 0.05. Fat poop microbes cause obesity? Exists (p &amp;lt; 0.05). Many scientists literally believe that something doesn’t exist because p &amp;gt; 0.05. An interaction between CO2 and Temperature on larval growth? Doesn’t exist (p = 0.079). Or, if we want the interaction to exist, then we report “the interaction trends toward significance (p = 0.079)”. How come results never trend away from significance?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparing a p-value to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is a reasonable decision theoretic strategy relevant to manufacturing (let’s test a sample from this lot and throw out the whole batch if p &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;). By contrast, in most papers in ecology or physiology that I read, a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is not used to make a decision. Classifying a p-value as signficant or non-significant adds zero-value to the analysis. Instead, it creates the illusion of discovery. Sometimes &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values are used to make decisions, for example, statistical significance is routinely used to find the subset of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; that are thought to be causally related to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This might be a classic multiple regression with tens of environmental variables or a more modern genomic analysis with thousands of genes or hundreds of thousands of SNPs. There are a great many papers devoted to methods for “correcting for multiple testing” as if we can discover by statistical significance. Scientific discovery and knowlege requires replication and rigorous probing, not statistical significance. I frankly don’t see the point of model simplification or adjusting p-values for multiple testing. Instead, we should use the results to rank the effect sizes and then do the hard work of experimentally isolating and rigorously probing these effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My answer is not to lower &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; or advocate for a more flexible &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. And, banning asterisks from tables and plots or the word “signficant” from the text isn’t really enough. I think we should simply teach our students to stop hypothesis testing. We should teach our students that estimating effect sizes is critical for model development and testing (the focus of the not-yet-published post at Rapid Ecology), and of course, for decision making. We should teach our students that uncertaintly is a part of science and the different ways to measure uncertainty. We should teach our students that rigorous probing of a hypothesis is vital for discovery. We should teach our students that replication is vital for discovery. And we should lobby editors to stop publishing cargo-cult science practices.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is the range of reasonable P-values given a two standard error difference in means?</title>
      <link>/2018/03/what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means/</link>
      <pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means/</guid>
      <description>


&lt;p&gt;Here is the motivating quote for this post, from Andrew Gelman’s blog post &lt;a href=&#34;http://andrewgelman.com/2017/11/28/five-ways-fix-statistics/&#34;&gt;“Five ways to fix statistics”&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I agree with just about everything in Leek’s article except for this statement: “It’s also impractical to say that statistical metrics such as P values should not be used to make decisions. Sometimes a decision (editorial or funding, say) must be made, and clear guidelines are useful.” Yes, decisions need to be made, but to suggest that p-values be used to make editorial or funding decisions—that’s just horrible. That’s what’s landed us in the current mess. As my colleagues and I have discussed, we strongly feel that editorial and funding decisions should be based on theory, statistical evidence, and cost-benefit analyses—not on a noisy measure such as a p-value. &lt;em&gt;Remember that if you’re in a setting where the true effect is two standard errors away from zero, that the p-value could easily be anywhere from 0.00006 and 1. That is, in such a setting, the 95% predictive interval for the z-score is (0, 4), which corresponds to a 95% predictive interval for the p-value of (1.0, 0.00006)&lt;/em&gt;. That’s how noisy the p-value is. So, no, don’t use it to make editorial and funding decisions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m not sure how Gelman computed these numbers, but the statement seems worthy of exploring with an R-doodle. Here is the way I’d frame the question for exploration: given a true, two SED (standard error of the difference in means) effect, what is the interval containing 95% of future &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values? Here is the R-doodle, which also explores the interval given 1, 3, and 4 SED effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# doodle to see 95% CI of p-value (range of p-values consistent with data) given
# a 2SE effect size (i.e. just at 0.05 for large n)
# motivating quote:
# &amp;quot;Remember that if you’re in a setting where the true effect is two standard errors away from zero, that the p-value could easily be anywhere from 0.00006 and 1. That is, in such a setting, the 95% predictive interval for the z-score is (0, 4), which corresponds to a 95% predictive interval for the p-value of (1.0, 0.00006). That’s how noisy the p-value is.&amp;quot;
# source: http://andrewgelman.com/2017/11/28/five-ways-fix-statistics/
# Jeffrey Walker
# November 29, 2017

library(ggplot2)
library(data.table)
set.seed(1)
n &amp;lt;- 30
niter &amp;lt;- 5*10^3
sigma &amp;lt;- 1
x &amp;lt;- rep(c(0,1),each=n)
p &amp;lt;- numeric(niter)
d &amp;lt;- numeric(niter)
res &amp;lt;- data.table(NULL)
# initialize in SED units
for(sed_effect in 1:4){
  # the effect in SEM units
  se_effect &amp;lt;- sqrt(2*sed_effect^2) # 
  # the effect in SD units
  sd_effect &amp;lt;- se_effect/sqrt(n)
  power &amp;lt;- power.t.test(n, sd_effect, sigma)$power
  y1 &amp;lt;- matrix(rnorm(n*niter,mean=0.0, sd=sigma), nrow=n)
  y2 &amp;lt;- matrix(rnorm(n*niter, mean=sd_effect, sd=sigma),nrow=n)
  for(i in 1:niter){
    p[i] &amp;lt;- t.test(y1[,i],y2[,i])$p.value
  }
  ci &amp;lt;- quantile(p, c(0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975))
  res &amp;lt;- rbind(res, data.table(n=n,
                          d.sed=sed_effect,
                          d.sem=se_effect, d.sd=round(sd_effect, 2),
                          power=round(power,2),
                          data.table(t(ci))))
}
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     n d.sed    d.sem d.sd power         2.5%           5%          25%
## 1: 30     1 1.414214 0.26  0.16 3.347471e-03 9.251045e-03 9.726431e-02
## 2: 30     2 2.828427 0.52  0.50 1.307935e-04 4.364955e-04 9.216022e-03
## 3: 30     3 4.242641 0.77  0.84 4.042272e-06 1.242138e-05 4.220133e-04
## 4: 30     4 5.656854 1.03  0.98 4.570080e-08 2.297983e-07 1.471134e-05
##             50%         75%        95%      97.5%
## 1: 0.2936458989 0.611365558 0.91934854 0.95630882
## 2: 0.0499891796 0.189903833 0.69030016 0.84298129
## 3: 0.0036464345 0.023343898 0.17671907 0.29660133
## 4: 0.0001906092 0.001769547 0.02642579 0.05377549&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;old_names &amp;lt;- c(&amp;#39;2.5%&amp;#39;, &amp;#39;5%&amp;#39;, &amp;#39;25%&amp;#39;, &amp;#39;50%&amp;#39;, &amp;#39;75%&amp;#39;, &amp;#39;95%&amp;#39;, &amp;#39;97.5%&amp;#39;)
new_names &amp;lt;- c(&amp;#39;lo3&amp;#39;, &amp;#39;lo2&amp;#39;, &amp;#39;lo1&amp;#39;, &amp;#39;med&amp;#39;, &amp;#39;up1&amp;#39;, &amp;#39;up2&amp;#39;, &amp;#39;up3&amp;#39;)
setnames(res, old=old_names, new=new_names)
gg &amp;lt;- ggplot(data=res, aes(x=d.sed, y=med)) +
  geom_linerange(aes(ymin=lo3, ymax=up3)) +
  geom_linerange(aes(ymin=lo1, ymax=up1), size=4, color=&amp;#39;darkgray&amp;#39;) +
  geom_point() +
  geom_hline(yintercept=0.05, linetype=&amp;#39;dashed&amp;#39;) +
#  geom_hline(yintercept=0.05, aes(linetype=&amp;#39;dashed&amp;#39;, color=&amp;#39;darkgray&amp;#39;))
#  geom_hline(yintercept=0.05, mapping=aes(linetype=&amp;#39;dashed&amp;#39;, color=&amp;#39;red&amp;#39;))
  labs(x=&amp;#39;Difference in means (SED)&amp;#39;, y=&amp;#39;p-value&amp;#39;) +
  theme_minimal()
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-18-what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means_files/figure-html/simulation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A 2 SED effect has an expected &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value near 0.05 given a reasonable sample size. My 95% interval for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values for a 2 SED effect is (0.0001, .83), which is narrower than Gelman’s. I’m not sure we’re computing the same thing. I’ve explored the question, what is the &lt;em&gt;confidence interval&lt;/em&gt; of the SED and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value if the true effect, in SED units, is 2?&lt;/p&gt;
&lt;p&gt;Regardless, the larger point remains intact. The larger point, of course, is that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values are noisy. If an effect is just statistically significant, future &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values from the experiment would reasonably range from very small to very large (and this assumes that the only difference in future experiments is sampling variation).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>