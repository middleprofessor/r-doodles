<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fake Data on R Doodles</title>
    <link>/tags/fake-data/</link>
    <description>Recent content in Fake Data on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/fake-data/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using Wright&#39;s rules and a DAG to compute the bias of an effect when we measure proxies for X and Y</title>
      <link>/2020/02/using-wright-s-rules-and-a-dag-to-compute-the-bias-of-an-effect-when-we-measure-proxies-for-x-and-y/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/using-wright-s-rules-and-a-dag-to-compute-the-bias-of-an-effect-when-we-measure-proxies-for-x-and-y/</guid>
      <description>This is a skeletal post to work up an answer to a twitter question using Wright’s rules of path models. Using this figure
from Panel A of a figure from Hernan and Cole. The scribbled red path coefficients are added
 the question is I want to know about A-&amp;gt;Y but I measure A* and Y*. So in figure A, is the bias the backdoor path from A* to Y* through A and Y?</description>
    </item>
    
    <item>
      <title>Normalization results in regression to the mean and inflated Type I error conditional on the reference values</title>
      <link>/2019/10/normalization-results-in-regression-to-the-mean/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/normalization-results-in-regression-to-the-mean/</guid>
      <description>Fig 1C of the Replication Study: Melanoma exosomes educate bone marrow progenitor cells toward a pro-metastatic phenotype through MET uses an odd (to me) three stage normalization procedure for the quantified western blots. The authors compared blot values between a treatment (shMet cells) and a control (shScr cells) using GAPDH to normalize the values. The three stages of the normalization are
first, the value for the Antibody levels were normalized by the value of a reference (GAPDH) for each Set.</description>
    </item>
    
    <item>
      <title>A comment on the novel transformation of the response in &#34; Senolytics decrease senescent cells in humans: Preliminary report from a clinical trial of Dasatinib plus Quercetin in individuals with diabetic kidney disease&#34;</title>
      <link>/2019/10/a-comment-on-the-novel-transformation-of-the-response-in-senolytics-decrease-senescent-cells-in-humans-preliminary-report-from-a-clinical-trial-of-dasatinib-plus-quercetin-in-individuals-with-diabetic-kidney-disease/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/a-comment-on-the-novel-transformation-of-the-response-in-senolytics-decrease-senescent-cells-in-humans-preliminary-report-from-a-clinical-trial-of-dasatinib-plus-quercetin-in-individuals-with-diabetic-kidney-disease/</guid>
      <description>Motivation: https://pubpeer.com/publications/8DF6E66FEFAA2C3C7D5BD9C3FC45A2#2 and https://twitter.com/CGATist/status/1175015246282539009
tl;dr: Given the transformation done by the authors, for any response in day_0 that is unusually small, there is automatically a response in day_14 that is unusually big and vice-versa. Consequently, if the mean for day_0 is unusually small, the mean for day_14 is automatically unusually big, hence the elevated type I error with an unpaired t-test. The transformation is necessary and sufficient to produce the result (meaning even in conditions where a paired t-test isn’t needed, the transformation still produces elevated Type I error).</description>
    </item>
    
    <item>
      <title>What is the consequence of a Shapiro-Wilk test-of-normality filter on Type I error and Power?</title>
      <link>/2019/08/what-is-the-consequence-of-a-shapiro-wilk-test-of-normality-filter-on-type-i-error-and-power/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/what-is-the-consequence-of-a-shapiro-wilk-test-of-normality-filter-on-type-i-error-and-power/</guid>
      <description>Set up Normal distribution Type I error Power  Right skewed continuous – lognormal What the parameterizations look like Type I error Power    This 1990-wants-you-back doodle explores the effects of a Normality Filter – using a Shapiro-Wilk (SW) test as a decision rule for using either a t-test or some alternative such as a 1) non-parametric Mann-Whitney-Wilcoxon (MWW) test, or 2) a t-test on the log-transformed response.</description>
    </item>
    
    <item>
      <title>What is the bias in the estimation of an effect given an omitted interaction term?</title>
      <link>/2019/07/what-is-bias-in-the-estimation-of-an-effect-giving-an-omitted-interaction-term/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/what-is-bias-in-the-estimation-of-an-effect-giving-an-omitted-interaction-term/</guid>
      <description>Some background (due to Sewall Wright’s method of path analysis) Given a generating model:
\[\begin{equation} y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 \end{equation}\] where \(x_3 = x_1 x_2\); that is, it is an interaction variable.
The total effect of \(x_1\) on \(y\) is \(\beta_1 + \frac{\mathrm{COV}(x_1, x_2)}{\mathrm{VAR}(x_1)} \beta_2 + \frac{\mathrm{COV}(x_1, x_3)}{\mathrm{VAR}(x_1)} \beta_3\).
If \(x_3\) (the interaction) is missing, its component on the total efffect is added to the coefficient of \(x_1\).</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST -- Update</title>
      <link>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</guid>
      <description>Update to the earlier post, which was written in response to my own thinking about how to teach stastics to experimental biologists working in fields that are dominated by hypothesis testing instead of estimation. That is, should these researchers learn GLMs or is a t-test on raw or log-transformed data on something like count data good enough – or even superior? My post was written without the benefit of either [Ives](Ives, Anthony R.</description>
    </item>
    
    <item>
      <title>The statistical significance filter</title>
      <link>/2019/04/the-statistical-significance-filter/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/the-statistical-significance-filter/</guid>
      <description>1 Why reported effect sizes are inflated 2 Setup 3 Exploration 1 4 Unconditional means, power, and sign error 5 Conditional means 5.1 filter = 0.05 5.2 filter = 0.2    1 Why reported effect sizes are inflated This post is motivated by many discussions in Gelman’s blog but start here
When we estimate an effect1, the estimate will be a little inflated or a little diminished relative to the true effect but the expectation of the effect is the true effect.</description>
    </item>
    
    <item>
      <title>Paired line plots</title>
      <link>/2019/01/paired-line-plots/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/paired-line-plots/</guid>
      <description>load libraries make some fake data make a plot with ggplot   ggplot scripts to draw figures like those in the Dynamic Ecology post Paired line plots (a.k.a. “reaction norms”) to visualize Likert data
load libraries library(ggplot2) library(ggpubr) library(data.table)  make some fake data set.seed(3) n &amp;lt;- 40 self &amp;lt;- rbinom(n, 5, 0.25) + 1 others &amp;lt;- self + rbinom(n, 3, 0.5) fd &amp;lt;- data.table(id=factor(rep(1:n, 2)), who=factor(rep(c(&amp;quot;self&amp;quot;, &amp;quot;others&amp;quot;), each=n)), stigma &amp;lt;- c(self, others))  make a plot with ggplot The students are identified by the column “id”.</description>
    </item>
    
    <item>
      <title>A simple ggplot of some measure against depth</title>
      <link>/2018/09/a-simple-ggplot-of-some-measure-against-depth/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/a-simple-ggplot-of-some-measure-against-depth/</guid>
      <description>set up The goal is to plot the measure of something, say O2 levels, against depth (soil or lake), with the measures taken on multiple days
library(ggplot2) library(data.table)  First – create fake data depths &amp;lt;- c(0, seq(10,100, by=10)) dates &amp;lt;- c(&amp;quot;Jan-18&amp;quot;, &amp;quot;Mar-18&amp;quot;, &amp;quot;May-18&amp;quot;, &amp;quot;Jul-18&amp;quot;) x &amp;lt;- expand.grid(date=dates, depth=depths) n &amp;lt;- nrow(x) head(x) ## date depth ## 1 Jan-18 0 ## 2 Mar-18 0 ## 3 May-18 0 ## 4 Jul-18 0 ## 5 Jan-18 10 ## 6 Mar-18 10 X &amp;lt;- model.</description>
    </item>
    
  </channel>
</rss>