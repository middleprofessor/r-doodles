<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nhst on R Doodles</title>
    <link>/tags/nhst/</link>
    <description>Recent content in Nhst on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/nhst/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST -- Update</title>
      <link>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</guid>
      <description>Update to the earlier post, which was written in response to my own thinking about how to teach stastics to experimental biologists working in fields that are dominated by hypothesis testing instead of estimation. That is, should these researchers learn GLMs or is a t-test on raw or log-transformed data on something like count data good enough – or even superior? My post was written without the benefit of either [Ives](Ives, Anthony R.</description>
    </item>
    
    <item>
      <title>Should we be skeptical of a &#34;large&#34; effect size if p &gt; 0.05?</title>
      <link>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</guid>
      <description>Motivator: A twitter comment “Isn’t the implication that the large effect size is a direct byproduct of the lack of power? i.e. that if the the study had more power, the effect size would have been found to be smaller.”1 2
A thought: our belief in the magnitude of an observed effect should be based on our priors, which, hopefully, are formed from good mechanistic models and not sample size“.</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST</title>
      <link>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</guid>
      <description>A skeleton simulation of different strategies for NHST for count data if all we care about is a p-value, as in bench biology where p-values are used to simply give one confidence that something didn’t go terribly wrong (similar to doing experiments in triplicate – it’s not the effect size that matters only “we have experimental evidence of a replicable effect”).
tl;dr - At least for Type I error at small \(n\), log(response) and Wilcoxan have the best performance over the simulation space.</description>
    </item>
    
    <item>
      <title>On alpha</title>
      <link>/2018/04/on-alpha/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/on-alpha/</guid>
      <description>This post is motivated by Terry McGlynn’s thought provoking How do we move beyond an arbitrary statistical threshold? I have been struggling with the ideas explored in Terry’s post ever since starting my PhD 30 years ago, and its only been in the last couple of years that my own thoughts have begun to gel. This long marination period is largely because of my very classical biostatistical training. My PhD is from the Department of Anatomical Sciences at Stony Brook but the content was geometric morphometrics and James Rohlf was my mentor for morphometrics specifically, and multivariate statistics more generally.</description>
    </item>
    
  </channel>
</rss>