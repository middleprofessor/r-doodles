<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nhst on R Doodles</title>
    <link>/tags/nhst/</link>
    <description>Recent content in Nhst on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Jun 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/nhst/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What does cell biology data look like?</title>
      <link>/2019/06/what-does-cell-biology-data-look-like/</link>
      <pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/what-does-cell-biology-data-look-like/</guid>
      <description>


&lt;p&gt;If I’m going to evaluate the widespread use of t-tests/ANOVAs on count data in bench biology then I’d like to know what these data look like, specifically the shape (“overdispersion”) parameter.&lt;/p&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set up&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(readxl)
library(ggpubr)
library(cowplot)
library(plyr) #mapvalues
library(data.table)

# glm packages
library(MASS)
library(pscl) #zeroinfl
library(DHARMa)
library(mvabund)

  data_path &amp;lt;- &amp;quot;../data&amp;quot; # notebook, console
  source(&amp;quot;../../../R/clean_labels.R&amp;quot;) # notebook, console&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-the-enteric-nervous-system-promotes-intestinal-health-by-constraining-microbiota-composition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data from The enteric nervous system promotes intestinal health by constraining microbiota composition&lt;/h1&gt;
&lt;div id=&#34;import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_enteric &amp;lt;- function(sheet_i, range_i, file_path, wide_2_long=TRUE){
  dt_wide &amp;lt;- data.table(read_excel(file_path, sheet=sheet_i, range=range_i))
  dt_long &amp;lt;- na.omit(melt(dt_wide, measure.vars=colnames(dt_wide), variable.name=&amp;quot;treatment&amp;quot;, value.name=&amp;quot;count&amp;quot;))
  return(dt_long)
}

folder &amp;lt;- &amp;quot;Data from The enteric nervous system promotes intestinal health by constraining microbiota composition&amp;quot;
fn &amp;lt;- &amp;quot;journal.pbio.2000689.s008.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, fn, sep=&amp;quot;/&amp;quot;)
fig1c &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;a2:b11&amp;quot;, file_path)
fig1e &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;d2:g31&amp;quot;, file_path)
fig1f &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;i2:l53&amp;quot;, file_path)
fig2a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 2&amp;quot;, range_i=&amp;quot;a2:d33&amp;quot;, file_path)
fig2d &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 2&amp;quot;, range_i=&amp;quot;F2:I24&amp;quot;, file_path)
fig3a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 3&amp;quot;, range_i=&amp;quot;a2:c24&amp;quot;, file_path)
fig3b &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 3&amp;quot;, range_i=&amp;quot;e2:g12&amp;quot;, file_path)
fig4a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 4&amp;quot;, range_i=&amp;quot;a2:b125&amp;quot;, file_path)
fig5c &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 5&amp;quot;, range_i=&amp;quot;i2:l205&amp;quot;, file_path)
fig6d &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 6&amp;quot;, range_i=&amp;quot;I2:L16&amp;quot;, file_path)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimates-of-the-shape-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimates of the shape parameter&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_enteric &amp;lt;- function(fig_i, fig_num=NULL){
  fit &amp;lt;- glm.nb(count ~ treatment, data=fig_i)

  #fig_num &amp;lt;- names(fig_i)
  if(is.null(fig_num)){
    fig_num &amp;lt;- deparse(substitute(fig_i)) # this works when df is sent but not a list element
  }
  theta &amp;lt;- fit$theta
  fit_title &amp;lt;- paste0(fig_num, &amp;quot; (theta = &amp;quot;, round(theta,1), &amp;quot;)&amp;quot;)
  gg &amp;lt;- ggdotplot(fig_i,
           x=&amp;quot;treatment&amp;quot;, 
           y=&amp;quot;count&amp;quot;,
           color=&amp;quot;treatment&amp;quot;,
           pallete=&amp;quot;jco&amp;quot;,
           add=&amp;quot;mean&amp;quot;) +
    #annotate(&amp;quot;text&amp;quot;, x=1, y= max(fig_i[, count]), label=paste(&amp;quot;theta =&amp;quot;, round(theta,1))) +
    ggtitle(fit_title) +
    rremove(&amp;quot;legend&amp;quot;) +
    NULL
  return(gg)
}

plot_enteric2 &amp;lt;- function(fig_i, fig_num, i){
  fit &amp;lt;- glm.nb(count ~ treatment, data=fig_i[[i]])
  #fig_no &amp;lt;- deparse(substitute(fig_i)) # this works when df is sent but not a list element
  #fig_no &amp;lt;- names(fig_i)
  theta &amp;lt;- fit$theta
  fit_title &amp;lt;- paste0(fig_num[[i]], &amp;quot; (theta = &amp;quot;, round(theta,1), &amp;quot;)&amp;quot;)
  gg &amp;lt;- ggdotplot(fig_i[[i]],
           x=&amp;quot;treatment&amp;quot;, 
           y=&amp;quot;count&amp;quot;,
           color=&amp;quot;treatment&amp;quot;,
           pallete=&amp;quot;jco&amp;quot;,
           add=&amp;quot;mean&amp;quot;) +
    #annotate(&amp;quot;text&amp;quot;, x=1, y= max(fig_i[, count]), label=paste(&amp;quot;theta =&amp;quot;, round(theta,1))) +
    ggtitle(fit_title) +
    rremove(&amp;quot;legend&amp;quot;) +
    NULL
  return(gg)
}

fig_list_names &amp;lt;- c(&amp;quot;fig1c&amp;quot;, &amp;quot;fig1e&amp;quot;, &amp;quot;fig1f&amp;quot;, &amp;quot;fig2a&amp;quot;, &amp;quot;fig2d&amp;quot;, &amp;quot;fig3a&amp;quot;, &amp;quot;fig3b&amp;quot;, &amp;quot;fig4a&amp;quot;, &amp;quot;fig5c&amp;quot;, &amp;quot;fig6d&amp;quot;)
fig_list &amp;lt;- list(fig1c, fig1e, fig1f, fig2a, fig2d, fig3a, fig3b, fig4a, fig5c, fig6d)
names(fig_list) &amp;lt;- fig_list_names # super kludgy
# this doesn&amp;#39;t work
# gg_list &amp;lt;- lapply(fig_list, plot_enteric, names(fig_list))

# this works but requires i in the function which is unsatifying
#gg_list &amp;lt;- lapply(seq_along(fig_list), plot_enteric2, fig_i=fig_list, fig_num=names(fig_list))
gg_list &amp;lt;- list(NULL)
for(i in 1:length(fig_list)){
  gg_list[[i]] &amp;lt;- plot_enteric(fig_list[[i]], names(fig_list)[[i]])
}

plot_grid(plotlist=gg_list, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-09-what-does-cell-biology-data-look-like_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-organic-cation-transporter-3-oct3-is-a-distinct-catecholamines-clearance-route-in-adipocytes-mediating-the-beiging-of-white-adipose-tissue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data from Organic cation transporter 3 (Oct3) is a distinct catecholamines clearance route in adipocytes mediating the beiging of white adipose tissue&lt;/h1&gt;
&lt;div id=&#34;import-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;folder &amp;lt;- &amp;quot;Data from Organic cation transporter 3 (Oct3) is a distinct catecholamines clearance route in adipocytes mediating the beiging of white adipose tissue&amp;quot;
fn &amp;lt;- &amp;quot;journal.pbio.2006571.s012.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, fn, sep=&amp;quot;/&amp;quot;)
fig5b &amp;lt;- read_enteric(sheet_i=&amp;quot;Fig 5B&amp;quot;, range_i=&amp;quot;b2:c12&amp;quot;, file_path)
plot_enteric(fig5b)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimates-of-the-shape-parameter-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimates of the shape parameter&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-of-simulated-samples-that-differ-in-mu-and-theta&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plots of simulated samples that differ in &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST -- Update</title>
      <link>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../../../2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/&#34;&gt;Update to the earlier post&lt;/a&gt;, which was written in response to my own thinking about how to teach stastics to experimental biologists working in fields that are dominated by hypothesis testing instead of estimation. That is, should these researchers learn GLMs or is a t-test on raw or log-transformed data on something like count data good enough – or even superior? My post was written without the benefit of either [Ives](Ives, Anthony R. “For testing the significance of regression coefficients, go ahead and log‐transform count data.” Methods in Ecology and Evolution 6, no. 7 (2015): 828-835) or &lt;a href=&#34;Warton,%20D.I.,%20Lyons,%20M.,%20Stoklosa,%20J.%20and%20Ives,%20A.R.,%202016.%20Three%20points%20to%20consider%20when%20choosing%20a%20LM%20or%20GLM%20test%20for%20count%20data.%20Methods%20in%20Ecology%20and%20Evolution,%207(8),%20pp.882-890&#34;&gt;Warton et al.&lt;/a&gt;. With hindsight, I do vaguely recall Ives, and my previous results support his conclusions, but I was unaware of Warton.&lt;/p&gt;
&lt;p&gt;Warton et al is a fabulous paper. A must read. A question that I have is, &lt;em&gt;under the null&lt;/em&gt; isn’t the response itself exchangeable, so that residuals are unnecessary? Regardless, the implementation in the mvabund package is way faster than my own R-scripted permutation. So here is my earlier simulation in light of Warton et al.&lt;/p&gt;
&lt;p&gt;TL;DR – If we live and die by NHST, then we want to choose a test with good Type I error control but has high power. The quasi-poisson both estimates an interpretable effect (unlike a t-test of log(y +1)) and has good Type I control with high power.&lt;/p&gt;
&lt;p&gt;A bit longer: The quasi-poisson LRT and the permutation NB have good Type I control and high power. The NB Wald and LRT have too liberal Type I control. The t-test of log response has good Type I control and high power at low &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; but is slightly inferior to the glm with increased &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The t-test, Welch, and Wilcoxan have conservative Type I control. Of these, the Wilcoxan has higher power than the t-test and Welch but not as high as the GLMs or log-transformed response.&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(MASS)
library(mvabund)
library(lmtest)
library(nlme)
library(data.table)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Single factor with two levels and a count (negative binomial) response.&lt;/li&gt;
&lt;li&gt;Relative effect sizes of 0%, 100%, and 200%&lt;/li&gt;
&lt;li&gt;Ref count of 4&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of 5 and 10&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;p&lt;/em&gt;-values computed from&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;t-test on raw response&lt;/li&gt;
&lt;li&gt;Welch t-test on raw response&lt;/li&gt;
&lt;li&gt;t-test on log transformed response&lt;/li&gt;
&lt;li&gt;Wilcoxan test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link using Wald test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link using LRT&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and permutation test (using PIT residuals)&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;glm with quasi-poisson family and log-link using LRT&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_sim &amp;lt;- function(sim_space=NULL, niter=1000, nperm=1000, algebra=FALSE){
  # the function was run with n=1000 and the data saved. on subsequent runs
  # the data are loaded from a file
  # the function creates three different objects to return, the object
  # return is specified by &amp;quot;return_object&amp;quot; = NULL, plot_data1, plot_data2
  
  set.seed(1)
  
  methods &amp;lt;- c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;, &amp;quot;nb.x2&amp;quot;, &amp;quot;nb.perm&amp;quot;, &amp;quot;qp&amp;quot;)
  p_table &amp;lt;- data.table(NULL)
  
  if(is.null(sim_space)){
    mu_0_list &amp;lt;- c(4) # control count
    theta_list &amp;lt;- c(0.5) # dispersion
    effect_list &amp;lt;- c(1) # effect size will be 1X, 1.5X, 2X, 3X
    n_list &amp;lt;- c(10) # sample size
    sim_space &amp;lt;- data.table(expand.grid(theta=theta_list, mu_0=mu_0_list, effect=effect_list, n=n_list))
  }
  
  res_table &amp;lt;- data.table(NULL)
  i &amp;lt;- 1 # this is just for debugging
  for(i in 1:nrow(sim_space)){
    # construct clean results table 
    p_table_part &amp;lt;- matrix(NA, nrow=niter, ncol=length(methods))
    colnames(p_table_part) &amp;lt;- methods
    
    # parameters of simulation
    theta_i &amp;lt;- sim_space[i, theta]
    mu_0_i &amp;lt;- sim_space[i, mu_0]
    effect_i &amp;lt;- sim_space[i, effect]
    n_i &amp;lt;- sim_space[i, n]
    treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Trt&amp;quot;), each=n_i)
    fd &amp;lt;- data.table(treatment=treatment)
    
    # mu (using algebra)
    if(algebra==TRUE){
      X &amp;lt;- model.matrix(~treatment)
      beta_0 &amp;lt;- log(mu_0_i)
      beta_1 &amp;lt;- log(effect_i*mu_0_i) - beta_0
      beta &amp;lt;- c(beta_0, beta_1)
      mu_i &amp;lt;- exp((X%*%beta)[,1])
    }else{ #  using R
      mu_vec &amp;lt;- c(mu_0_i, mu_0_i*effect_i)
      mu_i &amp;lt;- rep(mu_vec, each=n_i)
    }
    nb.error &amp;lt;- numeric(niter)
    
    for(iter in 1:niter){
      set.seed(niter*(i-1) + iter)
      fd[, y:=rnegbin(n=n_i*2, mu=mu_i, theta=theta_i)]
      fd[, log_yp1:=log10(y+1)]
      
      p.t &amp;lt;- t.test(y~treatment, data=fd, var.equal=TRUE)$p.value
      p.welch &amp;lt;- t.test(y~treatment, data=fd, var.equal=FALSE)$p.value
      p.log &amp;lt;- t.test(log_yp1~treatment, data=fd, var.equal=TRUE)$p.value
      p.wilcox &amp;lt;- wilcox.test(y~treatment, data=fd, exact=FALSE)$p.value
      
      # weighted lm, this will be ~same as welch for k=2 groups
      # fit &amp;lt;- gls(y~treatment, data=fd, weights = varIdent(form=~1|treatment), method=&amp;quot;ML&amp;quot;)
      # p.wls &amp;lt;- coef(summary(fit))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;p-value&amp;quot;]
      
      # negative binomial
      # default test using summary is Wald.
      # anova(fit) uses chisq of sequential fit, but using same estimate of theta
      # anova(fit2, fit1), uses chisq but with different estimate of theta
      # lrtest(fit) same as anova(fit2, fit1)
      
      # m1 &amp;lt;- glm.nb(y~treatment, data=fd)
      # m0 &amp;lt;- glm.nb(y~1, data=fd)
      # p.nb.x2 &amp;lt;- anova(m0, m1)[2, &amp;quot;Pr(Chi)&amp;quot;]
      # lr &amp;lt;- 2*(logLik(m1) - logLik(m0))
      # df.x2 = m0$df.residual-m1$df.residual
      # p.nb.x2 &amp;lt;- pchisq(lr, df=df.x2, lower.tail = F)
                
      m1 &amp;lt;- manyglm(y~treatment, data=fd) # default theta estimation &amp;quot;PHI&amp;quot;
      m0 &amp;lt;- manyglm(y~1, data=fd)
      lr &amp;lt;- 2*(logLik(m1) - logLik(m0))
      df.x2 = m0$df.residual-m1$df.residual
      p.nb &amp;lt;- coef(summary(m1))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;Pr(&amp;gt;wald)&amp;quot;] # Wald
      p.nb.x2 &amp;lt;- as.numeric(pchisq(lr, df=df.x2, lower.tail = F))
      p.nb.perm &amp;lt;- (anova(m0, m1, nBoot=nperm, show.time=&amp;#39;none&amp;#39;, p.uni=&amp;quot;unadjusted&amp;quot;)$uni.p)[2,1]

      # p.nb.x2 &amp;lt;- lrtest(fit)[2, &amp;quot;Pr(&amp;gt;Chisq)&amp;quot;] # doesn&amp;#39;t work with a data.table
      
      # quasipoisson
      fit &amp;lt;- glm(y~treatment, data=fd, family=quasipoisson)
      p.qp &amp;lt;- coeftest(fit)[2, &amp;quot;Pr(&amp;gt;|z|)&amp;quot;]
      
      p_table_part[iter,] &amp;lt;- c(p.t, p.welch, p.log, p.wilcox, p.nb, p.nb.x2, p.nb.perm, p.qp)
      
    } # niter
    p_table &amp;lt;- rbind(p_table, data.table(combo=i,
                                         mu_0=mu_0_i,
                                         effect=effect_i,
                                         n=n_i,
                                         theta=theta_i,
                                         nb.error=nb.error,
                                         p_table_part))
    
  } # combos
  
  return(p_table)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Algebra is slower (duh!)
# start_time &amp;lt;- Sys.time()
# do_sim(niter=niter, algebra=FALSE)
# end_time &amp;lt;- Sys.time()
# end_time - start_time
# 
# start_time &amp;lt;- Sys.time()
# do_sim(niter=niter, algebra=TRUE)
# end_time &amp;lt;- Sys.time()
# end_time - start_time

n_iter &amp;lt;- 2000
n_perm &amp;lt;- 2000
mu_0_list &amp;lt;- c(4) # control count
theta_list &amp;lt;- c(0.5) # dispersion
effect_list &amp;lt;- c(1, 2, 4) # effect size will be 1X, 1.5X, 2X, 3X
n_list &amp;lt;- c(5, 10) # sample size
sim_space &amp;lt;- data.table(expand.grid(theta=theta_list, mu_0=mu_0_list, effect=effect_list, n=n_list))

do_it &amp;lt;- FALSE # if FALSE the results are available as a file
if(do_it==TRUE){
  p_table &amp;lt;- do_sim(sim_space, niter=n_iter, nperm=n_perm)
  write.table(p_table, &amp;quot;../output/glm-v-lm.0004.txt&amp;quot;, row.names = FALSE, quote=FALSE)
}else{
  p_table &amp;lt;- fread(&amp;quot;../output/glm-v-lm.0001.txt&amp;quot;)
  p_table[, combo:=paste(effect, n, sep=&amp;quot;-&amp;quot;)]
  ycols &amp;lt;- setdiff(colnames(p_table), c(&amp;quot;combo&amp;quot;, &amp;quot;mu_0&amp;quot;, &amp;quot;effect&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;))
  res_table &amp;lt;- data.table(NULL)
  for(i in p_table[, unique(combo)]){
    p_table_part &amp;lt;- p_table[combo==i, ]
    mu_0_i &amp;lt;- p_table_part[1, mu_0]
    effect_i &amp;lt;- p_table_part[1, effect]
    n_i &amp;lt;- p_table_part[1, n]
    theta_i &amp;lt;- p_table_part[1, theta]
    n_iter_i &amp;lt;- nrow(p_table_part)
    p_sum &amp;lt;- apply(p_table_part[, .SD, .SDcols=ycols], 2, function(x) length(which(x &amp;lt;= 0.05))/n_iter_i)
    res_table &amp;lt;- rbind(res_table, data.table(mu_0 = mu_0_i,
                                             effect = effect_i,
                                             n = n_i,
                                             theta = theta_i,
                                             t(p_sum)))    
  }
  res_table[, n:=factor(n)]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;p&gt;Key: 1. “nb” uses the Wald test of negative binomial model. 2. “nb.x2” uses the LRT of negative binomial model. 3. “nb.perm” uses a permutation test on PIT residuals of negative binomial model 4. qp uses a LRT of quasi-poisson model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[effect==1,],
             caption = &amp;quot;Type 1 error as a function of n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:type-1-table&#34;&gt;Table 1: &lt;/span&gt;Type 1 error as a function of n&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;mu_0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;effect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;theta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Welch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Wilcoxan&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.perm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.032&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1280&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.054&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.036&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0505&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0675&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[effect!=1,],
             caption = &amp;quot;Power as a function of n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 2: &lt;/span&gt;Power as a function of n&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;mu_0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;effect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;theta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Welch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Wilcoxan&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.perm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0465&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0845&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0825&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1950&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1860&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1025&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1730&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1850&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1285&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5345&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4405&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;plot&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- melt(res_table, 
            id.vars=c(&amp;quot;mu_0&amp;quot;, &amp;quot;effect&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;, &amp;quot;nb.error&amp;quot;),
            measure.vars=c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;, &amp;quot;nb.x2&amp;quot;, &amp;quot;nb.perm&amp;quot;, &amp;quot;qp&amp;quot;),
            variable.name=&amp;quot;model&amp;quot;,
            value.name=&amp;quot;frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[effect==1,], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(. ~ effect, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-30-glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update_files/figure-html/type-1-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[effect!=1,], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(. ~ effect, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-30-glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update_files/figure-html/plower-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Warton, D.I., Thibaut, L., Wang, Y.A., 2017. The PIT-trap—A “model-free” bootstrap procedure for inference about regression models with discrete, multivariate responses. PLOS ONE 12, e0181790. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0181790&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0181790&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Should we be skeptical of a &#34;large&#34; effect size if p &gt; 0.05?</title>
      <link>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</guid>
      <description>


&lt;p&gt;Motivator: A twitter comment “Isn’t the implication that the large effect size is a direct byproduct of the lack of power? i.e. that if the the study had more power, the effect size would have been found to be smaller.”&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A thought: our belief in the magnitude of an observed effect should be based on our priors, which, hopefully, are formed from good mechanistic models and not sample size“.&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we observe a large effect but the sample size is small, then should we believe that the effect is strongly inflated?&lt;/li&gt;
&lt;li&gt;If we had measured a larger sample, would the effect be smaller?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But…maybe sample size should influence our prior, because the expected estimated effect &lt;strong&gt;magnitude&lt;/strong&gt; is bigger than than the true effect &lt;em&gt;if the true effect is near zero&lt;/em&gt; &lt;a href=&#34;../../../2019/04/the-statistical-significance-filter/&#34;&gt;explored a bit here&lt;/a&gt;. This is because, if an effect is near zero, estimates will vary on both sides of zero, and the absolute value of most of these estimates will be bigger than the absolute value of the true effect. But what effect size should we worry about this?&lt;/p&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: magrittr&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;p&gt;Simulate an experiment with two treatment levels (“control” and “treated”), with standardized (&lt;span class=&#34;math inline&#34;&gt;\(\frac{\delta}{\sigma}\)&lt;/span&gt;) effect sizes of 0.05, .1, .2, .3, .5, .8, 1, 2 and sample sizes of 100, 20, and 10. Cohen considered .8 a “large” standardized effect but I’ll leave what is large up to you. Regardless, its worth comparing the results here to observed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100 # per treatment level. power will be a function of effect size
b_array &amp;lt;- c(0.05, 0.1, 0.2, 0.3, 0.5, 0.8, 1, 2)
niter &amp;lt;- 10^4
res_table &amp;lt;- data.table(NULL)
power_table &amp;lt;- data.table(NULL)
for(b1 in b_array){
  y1 &amp;lt;- matrix(rnorm(n*niter), nrow=n)
  y2 &amp;lt;- matrix(rnorm(n*niter), nrow=n) + b1
  d100 &amp;lt;- apply(y2, 2, mean) - apply(y1,2,mean)
  d20 &amp;lt;- apply(y2[1:20,], 2, mean) - apply(y1[1:20,],2,mean)
  d10 &amp;lt;- apply(y2[1:10,], 2, mean) - apply(y1[1:10,],2,mean)
  res_table &amp;lt;- rbind(res_table, data.table(b=b1, d100=d100, d20=d20, d10=d10))
  power_table &amp;lt;- rbind(power_table, data.table(
    b=b1,
    &amp;quot;power (n=100)&amp;quot;=power.t.test(n=100, delta=b1, sd=1)$power,
    &amp;quot;power (n=20)&amp;quot;=power.t.test(n=20, delta=b1, sd=1)$power,
    &amp;quot;power (n=10)&amp;quot;=power.t.test(n=10, delta=b1, sd=1)$power
  ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;power-for-each-simulation-combination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power for each simulation combination&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(power_table, digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;absolute-median-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Absolute median effects&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[, .(&amp;quot;median(|b|) (n=100)&amp;quot;=median(abs(d100)),
              &amp;quot;median(|b|) (n=20)&amp;quot;=median(abs(d20)),
              &amp;quot;median(|b|) (n=10)&amp;quot;=median(abs(d10))
              ), by=b], digits=c(2, 3, 3, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.305&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.121&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.227&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.320&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.254&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.329&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.297&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.319&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.378&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.503&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.510&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.799&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.799&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.798&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.003&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.005&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;inflation-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inflation factors&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[, .(
              &amp;quot;IF (n=100)&amp;quot; = median(abs(d100))/b,
              &amp;quot;IF (n=20)&amp;quot; = median(abs(d20))/b,
              &amp;quot;IF (n=10)&amp;quot; = median(abs(d10))/b
              ), by=b], digits=c(2, 1, 1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;directly-answering-question-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Directly answering question #2&lt;/h1&gt;
&lt;p&gt;Notice that if power is obove about .2, the absolute median effect is not inflated. That is, a study would have to be wicked underpowered for there to be an expected inflated effect size. This is an indirect answer to question no. 2. A more direct answer is explored by computing the log10 ratio of absolute effects between sample size levels for each run of the simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_table[, d100.d20:=log10(abs(d20)/abs(d100))]
res_table[, d100.d10:=log10(abs(d10)/abs(d100))]
res_table[, d20.d10:=log10(abs(d10)/abs(d20))]
res_melt &amp;lt;- melt(res_table[, .SD, .SDcols=c(&amp;quot;b&amp;quot;, &amp;quot;d100.d20&amp;quot;, &amp;quot;d100.d10&amp;quot;, &amp;quot;d20.d10&amp;quot;)], id.vars=&amp;quot;b&amp;quot;, variable.name=&amp;quot;comparison&amp;quot;, value.name=&amp;quot;contrast&amp;quot;)
res_melt[, b:=factor(b)]
pd &amp;lt;- position_dodge(0.8)
ggplot(data=res_melt, aes(x=b, y=contrast, fill=comparison)) +
  geom_boxplot(position=pd, outlier.shape=NA) +
  coord_cartesian(ylim=c(-1.25, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-28-should-we-be-skeptical-of-a-large-effect-size-if-p-0-05_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the true effect is really small (0.05) then a smaller sample will often estimate a larger effect (just less than 75% of the time when decreasing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; from 100 to 20). When the true effect is about 0.5 or higher, decreasing sample size is no more likely to estimate a bigger effect than increasing sample size.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The original question, and the motivating tweet, raise the question of what a “large” effect is. There is large in the absolute since, which would require subject level expertise to identify, and large relative to noise.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that the original post was not about the &lt;a href=&#34;https://rdoodles.rbind.io/2019/04/the-statistical-significance-filter/&#34;&gt;statistical significance filter&lt;/a&gt; but about the ethics of a RCT in which the observed effect was “large” but there was not enough power to get a statistically significant p-value.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;at least if we are using an experiment to estimate an effect. If we are trying to estimate multiple effects, the bigger observed effects have tend to be inflated and the smaller observed effects tend to be dd&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST</title>
      <link>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../../../2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/&#34;&gt;This post has been updated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A skeleton simulation of different strategies for NHST for count data if all we care about is a p-value, as in bench biology where p-values are used to simply give one confidence that something didn’t go terribly wrong (similar to doing experiments in triplicate – it’s not the effect size that matters only “we have experimental evidence of a replicable effect”).&lt;/p&gt;
&lt;p&gt;tl;dr - At least for Type I error at small &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, log(response) and Wilcoxan have the best performance over the simulation space. T-test is a bit conservative. Welch is even more conservative. glm-nb is too liberal.&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(MASS)
library(data.table)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Single factor with two levels and a count (negative binomial) response.&lt;/li&gt;
&lt;li&gt;Relative effect sizes of 0%, 50%, 100%, and 200%&lt;/li&gt;
&lt;li&gt;Ref count of 4, 10, 100&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of 5, 10, 20, 40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;p&lt;/em&gt;-values computed from&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;t-test on raw response&lt;/li&gt;
&lt;li&gt;Welch t-test on raw response&lt;/li&gt;
&lt;li&gt;t-test on log transformed response&lt;/li&gt;
&lt;li&gt;Wilcoxan test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_sim &amp;lt;- function(niter=1, return_object=NULL){
  # the function was run with n=1000 and the data saved. on subsequent runs
  # the data are loaded from a file
  # the function creates three different objects to return, the object
  # return is specified by &amp;quot;return_object&amp;quot; = NULL, plot_data1, plot_data2
  methods &amp;lt;- c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;)
  p_table_part &amp;lt;- matrix(NA, nrow=niter, ncol=length(methods))
  colnames(p_table_part) &amp;lt;- methods
  p_table &amp;lt;- data.table(NULL)
  
  res_table &amp;lt;- data.table(NULL)
  beta_0_list &amp;lt;- c(4, 10, 100) # control count
  theta_list &amp;lt;- c(0.5, 1, 100) # dispersion
  effect_list &amp;lt;- c(1:3, 5) # relative effect size will be 0%, 50%, 100%, 200%
  n_list &amp;lt;- c(5, 10, 20, 40) # sample size
  n_rows &amp;lt;- length(beta_0_list)*length(theta_list)*length(effect_list)*length(n_list)*niter
  sim_space &amp;lt;- expand.grid(theta_list, beta_0_list, effect_list, n_list)
  plot_data1 &amp;lt;- data.table(NULL)
  plot_data2 &amp;lt;- data.table(NULL)
  debug_table &amp;lt;- data.table(matrix(NA, nrow=niter, ncol=2))
  setnames(debug_table, old=colnames(debug_table), new=c(&amp;quot;seed&amp;quot;,&amp;quot;model&amp;quot;))
  debug_table[, seed:=as.integer(seed)]
  debug_table[, model:=as.character(model)]
  i &amp;lt;- 0
  for(theta_i in theta_list){
    for(beta_0 in beta_0_list){
      # first get plots of distributions given parameters
      y &amp;lt;- rnegbin(n=10^4, mu=beta_0, theta=theta_i)
      x_i &amp;lt;- seq(min(y), max(y), by=1)
      prob_x_i &amp;lt;- dnbinom(x_i, size=theta_i, mu=beta_0)
      plot_data1 &amp;lt;- rbind(plot_data1, data.table(
        theta=theta_i,
        mu=beta_0,
        x=x_i,
        prob_x=prob_x_i
      ))
      # the simulation
      for(effect in effect_list){
        for(n in n_list){
          beta_1 &amp;lt;- (effect-1)*beta_0/2 # 0% 50% 100%

          do_manual &amp;lt;- FALSE
          if(do_manual==TRUE){
            theta_i &amp;lt;- res_table[row, theta]
            beta_0 &amp;lt;- res_table[row, beta_0]
            beta_1 &amp;lt;- res_table[row, beta_1]
            n &amp;lt;- res_table[row, n]
          }
          
          beta &amp;lt;- c(beta_0, beta_1)
          treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Trt&amp;quot;), each=n)
          X &amp;lt;- model.matrix(~treatment)
          mu &amp;lt;- (X%*%beta)[,1]
          fd &amp;lt;- data.table(treatment=treatment, y=NA)
          for(iter in 1:niter){
            i &amp;lt;- i+1
            set.seed(i)
            fd[, y:=rnegbin(n=n*2, mu=mu, theta=theta_i)]
            fd[, log_yp1:=log10(y+1)]
            p.t &amp;lt;- t.test(y~treatment, data=fd, var.equal=TRUE)$p.value
            p.welch &amp;lt;- t.test(y~treatment, data=fd, var.equal=FALSE)$p.value
            p.log &amp;lt;- t.test(log_yp1~treatment, data=fd, var.equal=TRUE)$p.value
            p.wilcox &amp;lt;- wilcox.test(y~treatment, data=fd, exact=FALSE)$p.value
            fit &amp;lt;- glm.nb(y~treatment, data=fd)
            debug_table[iter, seed:=i]
            debug_table[iter, model:=&amp;quot;glm.nb&amp;quot;]
            #if(fit$th.warn == &amp;quot;iteration limit reached&amp;quot;){
            if(!is.null(fit$th.warn)){
              fit &amp;lt;- glm(y~treatment, data=fd, family=poisson)
              debug_table[iter, model:=&amp;quot;poisson&amp;quot;]
            }
            p.nb &amp;lt;- coef(summary(fit))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;Pr(&amp;gt;|z|)&amp;quot;]
            p_table_part[iter,] &amp;lt;- c(p.t, p.welch, p.log, p.wilcox, p.nb)
          }
          p_table &amp;lt;- rbind(p_table, data.table(p_table_part, debug_table))
          p_sum &amp;lt;- apply(p_table_part, 2, function(x) length(which(x &amp;lt;= 0.05))/niter)
          res_table &amp;lt;- rbind(res_table, data.table(beta_0=beta_0,
                                                   beta_1=beta_1,
                                                   n=n,
                                                   theta=theta_i,
                                                   t(p_sum)))
        } # n
      } # effect
      plot_data2 &amp;lt;- rbind(plot_data2, data.table(
        theta=theta_i,
        mu=beta_0,
        n_i=n,
        beta1=beta_1,
        x=treatment,
        y=fd[, y]
      ))
    }
  }
  if(is.null(return_object)){return(res_table)}else{
    if(return_object==&amp;quot;plot_data1&amp;quot;){return(plot_data1)}
    if(return_object==&amp;quot;plot_data2&amp;quot;){return(plot_data2)}
    
  }
  
}

do_it &amp;lt;- FALSE # if FALSE the results are available as a file
if(do_it==TRUE){
  res_table &amp;lt;- do_sim(niter=1000)
  write.table(res_table, &amp;quot;../output/glm-t-wilcoxon.txt&amp;quot;, row.names = FALSE, quote=FALSE)
}else{
  plot_data &amp;lt;- do_sim(niter=1, return_object=&amp;quot;plot_data2&amp;quot;)
  res_table &amp;lt;- fread(&amp;quot;../output/glm-t-wilcoxon.txt&amp;quot;)
  res_table[, n:=factor(n)]
}
#res_table&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Distribution of the response for the 3 x 3 simulation space&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extreme inelegance
mu_levels &amp;lt;- unique(plot_data[, mu])
theta_levels &amp;lt;- unique(plot_data[, theta])
show_function &amp;lt;- FALSE
show_violin &amp;lt;- TRUE

if(show_function==TRUE){
  gg1 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg2 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg3 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg4 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg5 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg6 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg7 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
  gg8 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
  gg9 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
}

if(show_violin==TRUE){
  gg1 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg2 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg3 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg4 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg5 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg6 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg7 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
  gg8 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
  gg9 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
}

gg_example &amp;lt;- plot_grid(gg1, gg2, gg3, gg4, gg5, gg6, gg7, gg8, gg9, 
          nrow=3,
          labels=c(paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[3]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[3]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[3])),
          label_size = 10, label_x=0.1)
gg_example&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- melt(res_table, 
            id.vars=c(&amp;quot;beta_0&amp;quot;, &amp;quot;beta_1&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;),
            measure.vars=c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;),
            variable.name=&amp;quot;model&amp;quot;,
            value.name=&amp;quot;frequency&amp;quot;)
# res[, beta_0:=factor(beta_0)]
# res[, beta_1:=factor(beta_1)]
# res[, theta:=factor(theta)]
# res[, n:=factor(n)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[beta_1==0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_0 ~ theta, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/type%20I-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ouch. glm-nb with hih error rates especially when n is small and the scale parameter is small&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0_levels &amp;lt;- unique(res$beta_0)
# small count
gg1 &amp;lt;- ggplot(data=res[beta_0==b0_levels[1] &amp;amp; beta_1 &amp;gt; 0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_1 ~ theta, labeller=label_both) +
  NULL

# large count
gg2 &amp;lt;- ggplot(data=res[beta_0==b0_levels[3] &amp;amp; beta_1 &amp;gt; 0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_1 ~ theta, labeller=label_both) +
  NULL

gg1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/power-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/power-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;glm-nb has higher power, especially at small n, but at a type I cost.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On alpha</title>
      <link>/2018/04/on-alpha/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/on-alpha/</guid>
      <description>


&lt;p&gt;This post is motivated by Terry McGlynn’s thought provoking &lt;a href=&#34;https://smallpondscience.com/2018/04/23/how-do-we-move-beyond-an-arbitrary-statistical-threshold/&#34;&gt;How do we move beyond an arbitrary statistical threshold?&lt;/a&gt; I have been struggling with the ideas explored in Terry’s post ever since starting my PhD 30 years ago, and its only been in the last couple of years that my own thoughts have begun to gel. This long marination period is largely because of my very classical biostatistical training. My PhD is from the Department of Anatomical Sciences at Stony Brook but the content was geometric morphometrics and James Rohlf was my mentor for morphometrics specifically, and multivariate statistics more generally. The last year of my PhD, I was Robert Sokal’s RA (I was the programmer!) and got two co-authored papers with him. I invested a tremendous amount of time generating little statistical doodles (first in Excel, then in Pascal, and then in R) to better understand ANOVA, and permutation tests, and the bootstrap, and similar frequentist tools.&lt;/p&gt;
&lt;p&gt;My answer is partly answered by my two posts to the &lt;a href=&#34;https://rapidecology.com&#34;&gt;Rapid Ecology&lt;/a&gt; blog. The first – &lt;a href=&#34;https://rapidecology.com/2018/04/09/when-do-we-introduce-best-statistical-practices-to-undergraduate-biology-majors/&#34;&gt;When do we introduce best statistical practices to undergraduate biology majors?&lt;/a&gt; was posted April 9. The second – “Abandon ANOVA-type experiments” is a more radical answer, and is scheduled to appear in a couple of weeks.&lt;/p&gt;
&lt;p&gt;Here, I expand on the second post but make it more general. Terry finishes his post with the statement “To be clear, I’m not arguing (here) that we should be ditching the hypothesis falsification approach to answering questions”. Maybe he’s arguing this elsewhere. Regardless, I &lt;em&gt;am&lt;/em&gt; arguing that here. I am &lt;em&gt;not&lt;/em&gt; arguing against the use of p-values (here!) – simply against the concept of comparing a p-value to a type I error rate (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The practice of comparing a p-value to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and classifying a result as “significant” or “non-significant” has led to the cargo-cult science practice of “discovery by p-value.” Many scientists literally believe they have discovered something about the world because they found p &amp;lt; 0.05. Fat poop microbes cause obesity? Exists (p &amp;lt; 0.05). Many scientists literally believe that something doesn’t exist because p &amp;gt; 0.05. An interaction between CO2 and Temperature on larval growth? Doesn’t exist (p = 0.079). Or, if we want the interaction to exist, then we report “the interaction trends toward significance (p = 0.079)”. How come results never trend away from significance?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparing a p-value to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is a reasonable decision theoretic strategy relevant to manufacturing (let’s test a sample from this lot and throw out the whole batch if p &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;). By contrast, in most papers in ecology or physiology that I read, a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is not used to make a decision. Classifying a p-value as signficant or non-significant adds zero-value to the analysis. Instead, it creates the illusion of discovery. Sometimes &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values are used to make decisions, for example, statistical significance is routinely used to find the subset of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; that are thought to be causally related to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This might be a classic multiple regression with tens of environmental variables or a more modern genomic analysis with thousands of genes or hundreds of thousands of SNPs. There are a great many papers devoted to methods for “correcting for multiple testing” as if we can discover by statistical significance. Scientific discovery and knowlege requires replication and rigorous probing, not statistical significance. I frankly don’t see the point of model simplification or adjusting p-values for multiple testing. Instead, we should use the results to rank the effect sizes and then do the hard work of experimentally isolating and rigorously probing these effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My answer is not to lower &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; or advocate for a more flexible &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. And, banning asterisks from tables and plots or the word “signficant” from the text isn’t really enough. I think we should simply teach our students to stop hypothesis testing. We should teach our students that estimating effect sizes is critical for model development and testing (the focus of the not-yet-published post at Rapid Ecology), and of course, for decision making. We should teach our students that uncertaintly is a part of science and the different ways to measure uncertainty. We should teach our students that rigorous probing of a hypothesis is vital for discovery. We should teach our students that replication is vital for discovery. And we should lobby editors to stop publishing cargo-cult science practices.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>