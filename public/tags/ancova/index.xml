<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ancova on R Doodles</title>
    <link>/tags/ancova/</link>
    <description>Recent content in Ancova on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/ancova/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear models with a covariate (&#34;ANCOVA&#34;)</title>
      <link>/2020/10/linear-models-with-a-covariate-ancova/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/linear-models-with-a-covariate-ancova/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>ANCOVA when the covariate is a mediator affected by treatment</title>
      <link>/2020/07/ancova-when-the-covariate-is-a-mediator-affected-by-treatment/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/ancova-when-the-covariate-is-a-mediator-affected-by-treatment/</guid>
      <description>This is fake data that simulates an experiment to measure effect of treatment on fat weight in mice. The treatment is “diet” with two levels: “control” (blue dots) and “treated” (gold dots). Diet has a large effect on total body weight. The simulated data are in the plot above - these look very much like the real data.
The question is, what are problems with using an “ancova” linear model to estimate the direct effect of treatment on fat weight?</description>
    </item>
    
    <item>
      <title>Analyzing longitudinal data -- a simple pre-post design</title>
      <link>/2020/03/analyzing-longitudinal-data-a-simple-pre-post-design/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/analyzing-longitudinal-data-a-simple-pre-post-design/</guid>
      <description>A skeletal response to a twitter question:
“ANOVA (time point x group) or ANCOVA (group with time point as a covariate) for intervention designs? Discuss.”
follow-up “Only 2 time points in this case (pre- and post-intervention), and would wanna basically answer the question of whether out of the 3 intervention groups, some improve on measure X more than others after the intervention”
Here I compare five methods using fake pre-post data, including</description>
    </item>
    
    <item>
      <title>Normalization results in regression to the mean and inflated Type I error conditional on the reference values</title>
      <link>/2019/10/normalization-results-in-regression-to-the-mean/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/normalization-results-in-regression-to-the-mean/</guid>
      <description>Fig 1C of the Replication Study: Melanoma exosomes educate bone marrow progenitor cells toward a pro-metastatic phenotype through MET uses an odd (to me) three stage normalization procedure for the quantified western blots. The authors compared blot values between a treatment (shMet cells) and a control (shScr cells) using GAPDH to normalize the values. The three stages of the normalization are
first, the value for the Antibody levels were normalized by the value of a reference (GAPDH) for each Set.</description>
    </item>
    
    <item>
      <title>Blocking vs. covariate adjustment</title>
      <link>/2019/04/blocking-vs-covariate-adjustment/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/blocking-vs-covariate-adjustment/</guid>
      <description>“A more efficient design would be to first group the rats into homogeneous subsets based on baseline food consumption. This could be done by ranking the rats from heaviest to lightest eaters and then grouping them into pairs by taking the first two rats (the two that ate the most during baseline), then the next two in the list, and so on. The difference from a completely randomised design is that one rat within each pair is randomised to one of the treatment groups, and the other rat is then assigned to the remaining treatment group.</description>
    </item>
    
    <item>
      <title>Covariate adjustment in randomized experiments</title>
      <link>/2019/04/covariate-adjustment-in-randomized-experiments/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/covariate-adjustment-in-randomized-experiments/</guid>
      <description>The post motivated by a tweetorial from Darren Dahly
In an experiment, do we adjust for covariates that differ between treatment levels measured pre-experiment (“imbalance” in random assignment), where a difference is inferred from a t-test with p &amp;lt; 0.05? Or do we adjust for all covariates, regardless of differences pre-test? Or do we adjust only for covariates that have sustantial correlation with the outcome? Or do we not adjust at all?</description>
    </item>
    
    <item>
      <title>Bias in pre-post designs -- An example from the Turnbaugh et al (2006) mouse fecal transplant study</title>
      <link>/2018/03/bias-in-pre-post-designs-an-example-from-the-turnbaugh-et-al-2006-mouse-fecal-transplant-study/</link>
      <pubDate>Thu, 08 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/bias-in-pre-post-designs-an-example-from-the-turnbaugh-et-al-2006-mouse-fecal-transplant-study/</guid>
      <description>This post is motivated by a twitter link to a recent blog post critical of the old but influential study An obesity-associated gut microbiome with increased capacity for energy harvest with impressive citation metrics. In the post, Matthew Dalby smartly used the available data to reconstruct the final weights of the two groups. He showed these final weights were nearly the same, which is not good evidence for a treatment effect, given that the treatment was randomized among groups.</description>
    </item>
    
  </channel>
</rss>