<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on R Doodles</title>
    <link>/post/</link>
    <description>Recent content in Posts on R Doodles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Jun 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What does cell biology data look like?</title>
      <link>/2019/06/what-does-cell-biology-data-look-like/</link>
      <pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/what-does-cell-biology-data-look-like/</guid>
      <description>


&lt;p&gt;If I’m going to evaluate the widespread use of t-tests/ANOVAs on count data in bench biology then I’d like to know what these data look like, specifically the shape (“overdispersion”) parameter.&lt;/p&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set up&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(readxl)
library(ggpubr)
library(cowplot)
library(plyr) #mapvalues
library(data.table)

# glm packages
library(MASS)
library(pscl) #zeroinfl
library(DHARMa)
library(mvabund)

  data_path &amp;lt;- &amp;quot;../data&amp;quot; # notebook, console
  source(&amp;quot;../../../R/clean_labels.R&amp;quot;) # notebook, console&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-the-enteric-nervous-system-promotes-intestinal-health-by-constraining-microbiota-composition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data from The enteric nervous system promotes intestinal health by constraining microbiota composition&lt;/h1&gt;
&lt;div id=&#34;import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_enteric &amp;lt;- function(sheet_i, range_i, file_path, wide_2_long=TRUE){
  dt_wide &amp;lt;- data.table(read_excel(file_path, sheet=sheet_i, range=range_i))
  dt_long &amp;lt;- na.omit(melt(dt_wide, measure.vars=colnames(dt_wide), variable.name=&amp;quot;treatment&amp;quot;, value.name=&amp;quot;count&amp;quot;))
  return(dt_long)
}

folder &amp;lt;- &amp;quot;Data from The enteric nervous system promotes intestinal health by constraining microbiota composition&amp;quot;
fn &amp;lt;- &amp;quot;journal.pbio.2000689.s008.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, fn, sep=&amp;quot;/&amp;quot;)
fig1c &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;a2:b11&amp;quot;, file_path)
fig1e &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;d2:g31&amp;quot;, file_path)
fig1f &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 1&amp;quot;, range_i=&amp;quot;i2:l53&amp;quot;, file_path)
fig2a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 2&amp;quot;, range_i=&amp;quot;a2:d33&amp;quot;, file_path)
fig2d &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 2&amp;quot;, range_i=&amp;quot;F2:I24&amp;quot;, file_path)
fig3a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 3&amp;quot;, range_i=&amp;quot;a2:c24&amp;quot;, file_path)
fig3b &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 3&amp;quot;, range_i=&amp;quot;e2:g12&amp;quot;, file_path)
fig4a &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 4&amp;quot;, range_i=&amp;quot;a2:b125&amp;quot;, file_path)
fig5c &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 5&amp;quot;, range_i=&amp;quot;i2:l205&amp;quot;, file_path)
fig6d &amp;lt;- read_enteric(sheet_i=&amp;quot;Figure 6&amp;quot;, range_i=&amp;quot;I2:L16&amp;quot;, file_path)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimates-of-the-shape-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimates of the shape parameter&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_enteric &amp;lt;- function(fig_i, fig_num=NULL){
  fit &amp;lt;- glm.nb(count ~ treatment, data=fig_i)

  #fig_num &amp;lt;- names(fig_i)
  if(is.null(fig_num)){
    fig_num &amp;lt;- deparse(substitute(fig_i)) # this works when df is sent but not a list element
  }
  theta &amp;lt;- fit$theta
  fit_title &amp;lt;- paste0(fig_num, &amp;quot; (theta = &amp;quot;, round(theta,1), &amp;quot;)&amp;quot;)
  gg &amp;lt;- ggdotplot(fig_i,
           x=&amp;quot;treatment&amp;quot;, 
           y=&amp;quot;count&amp;quot;,
           color=&amp;quot;treatment&amp;quot;,
           pallete=&amp;quot;jco&amp;quot;,
           add=&amp;quot;mean&amp;quot;) +
    #annotate(&amp;quot;text&amp;quot;, x=1, y= max(fig_i[, count]), label=paste(&amp;quot;theta =&amp;quot;, round(theta,1))) +
    ggtitle(fit_title) +
    rremove(&amp;quot;legend&amp;quot;) +
    NULL
  return(gg)
}

plot_enteric2 &amp;lt;- function(fig_i, fig_num, i){
  fit &amp;lt;- glm.nb(count ~ treatment, data=fig_i[[i]])
  #fig_no &amp;lt;- deparse(substitute(fig_i)) # this works when df is sent but not a list element
  #fig_no &amp;lt;- names(fig_i)
  theta &amp;lt;- fit$theta
  fit_title &amp;lt;- paste0(fig_num[[i]], &amp;quot; (theta = &amp;quot;, round(theta,1), &amp;quot;)&amp;quot;)
  gg &amp;lt;- ggdotplot(fig_i[[i]],
           x=&amp;quot;treatment&amp;quot;, 
           y=&amp;quot;count&amp;quot;,
           color=&amp;quot;treatment&amp;quot;,
           pallete=&amp;quot;jco&amp;quot;,
           add=&amp;quot;mean&amp;quot;) +
    #annotate(&amp;quot;text&amp;quot;, x=1, y= max(fig_i[, count]), label=paste(&amp;quot;theta =&amp;quot;, round(theta,1))) +
    ggtitle(fit_title) +
    rremove(&amp;quot;legend&amp;quot;) +
    NULL
  return(gg)
}

fig_list_names &amp;lt;- c(&amp;quot;fig1c&amp;quot;, &amp;quot;fig1e&amp;quot;, &amp;quot;fig1f&amp;quot;, &amp;quot;fig2a&amp;quot;, &amp;quot;fig2d&amp;quot;, &amp;quot;fig3a&amp;quot;, &amp;quot;fig3b&amp;quot;, &amp;quot;fig4a&amp;quot;, &amp;quot;fig5c&amp;quot;, &amp;quot;fig6d&amp;quot;)
fig_list &amp;lt;- list(fig1c, fig1e, fig1f, fig2a, fig2d, fig3a, fig3b, fig4a, fig5c, fig6d)
names(fig_list) &amp;lt;- fig_list_names # super kludgy
# this doesn&amp;#39;t work
# gg_list &amp;lt;- lapply(fig_list, plot_enteric, names(fig_list))

# this works but requires i in the function which is unsatifying
#gg_list &amp;lt;- lapply(seq_along(fig_list), plot_enteric2, fig_i=fig_list, fig_num=names(fig_list))
gg_list &amp;lt;- list(NULL)
for(i in 1:length(fig_list)){
  gg_list[[i]] &amp;lt;- plot_enteric(fig_list[[i]], names(fig_list)[[i]])
}

plot_grid(plotlist=gg_list, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-09-what-does-cell-biology-data-look-like_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-organic-cation-transporter-3-oct3-is-a-distinct-catecholamines-clearance-route-in-adipocytes-mediating-the-beiging-of-white-adipose-tissue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data from Organic cation transporter 3 (Oct3) is a distinct catecholamines clearance route in adipocytes mediating the beiging of white adipose tissue&lt;/h1&gt;
&lt;div id=&#34;import-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;folder &amp;lt;- &amp;quot;Data from Organic cation transporter 3 (Oct3) is a distinct catecholamines clearance route in adipocytes mediating the beiging of white adipose tissue&amp;quot;
fn &amp;lt;- &amp;quot;journal.pbio.2006571.s012.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, fn, sep=&amp;quot;/&amp;quot;)
fig5b &amp;lt;- read_enteric(sheet_i=&amp;quot;Fig 5B&amp;quot;, range_i=&amp;quot;b2:c12&amp;quot;, file_path)
plot_enteric(fig5b)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimates-of-the-shape-parameter-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimates of the shape parameter&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-of-simulated-samples-that-differ-in-mu-and-theta&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plots of simulated samples that differ in &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reanalyzing data from Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice</title>
      <link>/2019/06/reanalyzing-data-from-human-gut-microbiota-from-autism-spectrum-disorder-promote-behavioral-symptoms-in-mice/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/reanalyzing-data-from-human-gut-microbiota-from-autism-spectrum-disorder-promote-behavioral-symptoms-in-mice/</guid>
      <description>


&lt;p&gt;A very skeletal analysis of&lt;/p&gt;
&lt;p&gt;Sharon, G., Cruz, N.J., Kang, D.W., Gandal, M.J., Wang, B., Kim, Y.M., Zink, E.M., Casey, C.P., Taylor, B.C., Lane, C.J. and Bramer, L.M., 2019. Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice. Cell, 177(6), pp.1600-1618.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pubpeer.com/publications/B521D325772244D8F656F1ED193ACA#&#34;&gt;which got some attention on pubpeer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Commenters are questioning the result of Fig1G. It is very hard to infer a p-value from plots like these, where the data are multi-level, regardless of if means and some kind of error bar is presented. A much better plot for inferring differences is an effects plot with the CI of the effect. That said, I’ll try to reproduce the resulting p-value.&lt;/p&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Caveats&lt;/h1&gt;
&lt;p&gt;Failure to reproduce or partial reproducibility might be an error in my coding, or my misunderstanding of the author’s methods, or my lack of knowledge of statistics generally or the R functions that I use more specifically.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(lmerTest)
library(emmeans)
library(lmtest)
library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;../data/Data from Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice/Fig1&amp;quot;
#data_path &amp;lt;- &amp;quot;data/Data from Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice/Fig1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fn &amp;lt;- &amp;quot;Fig1EFGH_subset8.csv&amp;quot;
file_path &amp;lt;- paste(data_path, fn, sep=&amp;quot;/&amp;quot;)
mouse &amp;lt;- fread(file_path)
mouse[, ASD_diagnosis:=factor(ASD_diagnosis, c(&amp;quot;NT&amp;quot;, &amp;quot;ASD&amp;quot;))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-1g&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Figure 1G&lt;/h1&gt;
&lt;div id=&#34;estimate-of-marginal-means-and-cohens-d&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimate of marginal means and Cohen’s d&lt;/h2&gt;
&lt;p&gt;One could reasonably estimate Cohen’s d several ways given the 2 x 2 design. Here are two&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- lmer(OFT_Distance ~ ASD_diagnosis + (1|Donor), data=mouse)
(m1.emm &amp;lt;- emmeans(m1, specs=c(&amp;quot;ASD_diagnosis&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  ASD_diagnosis emmean  SE   df lower.CL upper.CL
##  NT              4600 262 5.70     3951     5249
##  ASD             4235 206 6.08     3731     4738
## 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;- lmer(OFT_Distance ~ Gender + ASD_diagnosis + (1|Donor), data=mouse)
(m2.emm &amp;lt;- emmeans(m2, specs=c(&amp;quot;ASD_diagnosis&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  ASD_diagnosis emmean  SE   df lower.CL upper.CL
##  NT              4603 259 5.68     3960     5246
##  ASD             4233 204 6.07     3735     4731
## 
## Results are averaged over the levels of: Gender 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and effect, standardized to cohen’s D using residual standard deviation from model: (NT - ASD)/sigma&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(summary(m1))[&amp;quot;ASD_diagnosisASD&amp;quot;, &amp;quot;Estimate&amp;quot;]/sigma(m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.3928202&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(contrast(m2.emm, method=&amp;quot;revpairwise&amp;quot;))[1, &amp;quot;estimate&amp;quot;]/sigma(m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.3970379&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reported cohen’s D for the distance traveled is doTD-oASD = -0.58&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fig-1g-p-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fig 1G p-value&lt;/h2&gt;
&lt;div id=&#34;method-no.-1-as-stated-in-caption-to-fig-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Method no. 1 (as stated in caption to Fig 1)&lt;/h3&gt;
&lt;p&gt;The figure caption gives the method as “Hypothesis testing for differences of the means were done by a mixed effects analysis using donor diagnosis and mouse sex as fixed effects and donor ID as a random effect. p values were derived from a chi-square test”. This would be a Likelihood Ratio Test (LRT). The LRT requires that the model be fit by maximum likelhood for the statistic to have any meaning. The default fit in R is REML.&lt;/p&gt;
&lt;p&gt;The LRT of the model as specified in the caption of Fig1G&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# correct LRT
m1 &amp;lt;- lmer(OFT_Distance ~ Gender + ASD_diagnosis + (1|Donor), REML=FALSE, data=mouse)
m2 &amp;lt;- lmer(OFT_Distance ~ Gender + (1|Donor), REML=FALSE, data=mouse)
lrtest(m2, m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test
## 
## Model 1: OFT_Distance ~ Gender + (1 | Donor)
## Model 2: OFT_Distance ~ Gender + ASD_diagnosis + (1 | Donor)
##   #Df  LogLik Df  Chisq Pr(&amp;gt;Chisq)
## 1   4 -1706.7                     
## 2   5 -1706.0  1 1.4994     0.2208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is .2208&lt;/p&gt;
&lt;p&gt;The LRT of the model as specified but using the default lmer settings, which is fit by REML, which results in a meaningless chisq statistic&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- lmer(OFT_Distance ~ Gender + ASD_diagnosis + (1|Donor), data=mouse)
m2 &amp;lt;- lmer(OFT_Distance ~ Gender + (1|Donor), data=mouse)
lrtest(m2, m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test
## 
## Model 1: OFT_Distance ~ Gender + (1 | Donor)
## Model 2: OFT_Distance ~ Gender + ASD_diagnosis + (1 | Donor)
##   #Df  LogLik Df  Chisq Pr(&amp;gt;Chisq)    
## 1   4 -1695.0                         
## 2   5 -1687.6  1 14.692  0.0001266 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the p-value is 0.00013. Is this the reported p-value? I don’t know. A problem is that the methods section suggests a different model was used. This leads to:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;method-no.-2-as-stated-in-methods-section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;method no. 2 (as stated in methods section)&lt;/h2&gt;
&lt;p&gt;“Statistical analysis for behavioral outcomes in fecal transplanted offspring. Comparison of behavioral outcomes between TD Controls and ASD donors were tested using longitudinal linear mixed effects analyses, with test cycles and donors treated as repeated factors. Analyses were performed in SPSS (v 24); a priori alpha = 0.05. All outcomes were tested for normality and transformed as required. Diagonal covariance matrices were used so that intra-cycle and intra-donor correlations were accounted for in the modeling. The donor type (TD versus ASD) was the primary fixed effect measured, and mouse sex was an a priori covariate.”&lt;/p&gt;
&lt;p&gt;(I think) it would be unlikely to get the wrong LRT using SPSS. Also the specified model differs from that in the caption. Was the response transformed (note it’s not the reponse that needs to be “normal” but the residuals from the linear model (or equivalently the conditional response))&lt;/p&gt;
&lt;p&gt;correct LRT&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# correct LRT
m1 &amp;lt;- lmer(OFT_Distance ~ Gender + ASD_diagnosis + (1|Cycle) + (1|Donor), REML=FALSE, data=mouse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;- lmer(OFT_Distance ~ Gender + (1|Cycle) + (1|Donor), REML=FALSE, data=mouse)
lrtest(m2, m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test
## 
## Model 1: OFT_Distance ~ Gender + (1 | Cycle) + (1 | Donor)
## Model 2: OFT_Distance ~ Gender + ASD_diagnosis + (1 | Cycle) + (1 | Donor)
##   #Df  LogLik Df  Chisq Pr(&amp;gt;Chisq)
## 1   5 -1706.7                     
## 2   6 -1706.0  1 1.4974     0.2211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hmm. Need to explore the error.&lt;/p&gt;
&lt;p&gt;incorrect LRT&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# correct LRT
m1 &amp;lt;- lmer(OFT_Distance ~ Gender + ASD_diagnosis + (1|Cycle) + (1|Donor), data=mouse)
m2 &amp;lt;- lmer(OFT_Distance ~ Gender + (1|Cycle) + (1|Donor), data=mouse)
lrtest(m2, m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test
## 
## Model 1: OFT_Distance ~ Gender + (1 | Cycle) + (1 | Donor)
## Model 2: OFT_Distance ~ Gender + ASD_diagnosis + (1 | Cycle) + (1 | Donor)
##   #Df  LogLik Df  Chisq Pr(&amp;gt;Chisq)    
## 1   5 -1694.9                         
## 2   6 -1687.6  1 14.663  0.0001286 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basically the same results&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST -- Update</title>
      <link>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../../../2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/&#34;&gt;Update to the earlier post&lt;/a&gt;, which was written in response to my own thinking about how to teach stastics to experimental biologists working in fields that are dominated by hypothesis testing instead of estimation. That is, should these researchers learn GLMs or is a t-test on raw or log-transformed data on something like count data good enough – or even superior? My post was written without the benefit of either [Ives](Ives, Anthony R. “For testing the significance of regression coefficients, go ahead and log‐transform count data.” Methods in Ecology and Evolution 6, no. 7 (2015): 828-835) or &lt;a href=&#34;Warton,%20D.I.,%20Lyons,%20M.,%20Stoklosa,%20J.%20and%20Ives,%20A.R.,%202016.%20Three%20points%20to%20consider%20when%20choosing%20a%20LM%20or%20GLM%20test%20for%20count%20data.%20Methods%20in%20Ecology%20and%20Evolution,%207(8),%20pp.882-890&#34;&gt;Warton et al.&lt;/a&gt;. With hindsight, I do vaguely recall Ives, and my previous results support his conclusions, but I was unaware of Warton.&lt;/p&gt;
&lt;p&gt;Warton et al is a fabulous paper. A must read. A question that I have is, &lt;em&gt;under the null&lt;/em&gt; isn’t the response itself exchangeable, so that residuals are unnecessary? Regardless, the implementation in the mvabund package is way faster than my own R-scripted permutation. So here is my earlier simulation in light of Warton et al.&lt;/p&gt;
&lt;p&gt;TL;DR – If we live and die by NHST, then we want to choose a test with good Type I error control but has high power. The quasi-poisson both estimates an interpretable effect (unlike a t-test of log(y +1)) and has good Type I control with high power.&lt;/p&gt;
&lt;p&gt;A bit longer: The quasi-poisson LRT and the permutation NB have good Type I control and high power. The NB Wald and LRT have too liberal Type I control. The t-test of log response has good Type I control and high power at low &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; but is slightly inferior to the glm with increased &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The t-test, Welch, and Wilcoxan have conservative Type I control. Of these, the Wilcoxan has higher power than the t-test and Welch but not as high as the GLMs or log-transformed response.&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(MASS)
library(mvabund)
library(lmtest)
library(nlme)
library(data.table)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Single factor with two levels and a count (negative binomial) response.&lt;/li&gt;
&lt;li&gt;Relative effect sizes of 0%, 100%, and 200%&lt;/li&gt;
&lt;li&gt;Ref count of 4&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of 5 and 10&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;p&lt;/em&gt;-values computed from&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;t-test on raw response&lt;/li&gt;
&lt;li&gt;Welch t-test on raw response&lt;/li&gt;
&lt;li&gt;t-test on log transformed response&lt;/li&gt;
&lt;li&gt;Wilcoxan test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link using Wald test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link using LRT&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and permutation test (using PIT residuals)&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;glm with quasi-poisson family and log-link using LRT&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_sim &amp;lt;- function(sim_space=NULL, niter=1000, nperm=1000, algebra=FALSE){
  # the function was run with n=1000 and the data saved. on subsequent runs
  # the data are loaded from a file
  # the function creates three different objects to return, the object
  # return is specified by &amp;quot;return_object&amp;quot; = NULL, plot_data1, plot_data2
  
  set.seed(1)
  
  methods &amp;lt;- c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;, &amp;quot;nb.x2&amp;quot;, &amp;quot;nb.perm&amp;quot;, &amp;quot;qp&amp;quot;)
  p_table &amp;lt;- data.table(NULL)
  
  if(is.null(sim_space)){
    mu_0_list &amp;lt;- c(4) # control count
    theta_list &amp;lt;- c(0.5) # dispersion
    effect_list &amp;lt;- c(1) # effect size will be 1X, 1.5X, 2X, 3X
    n_list &amp;lt;- c(10) # sample size
    sim_space &amp;lt;- data.table(expand.grid(theta=theta_list, mu_0=mu_0_list, effect=effect_list, n=n_list))
  }
  
  res_table &amp;lt;- data.table(NULL)
  i &amp;lt;- 1 # this is just for debugging
  for(i in 1:nrow(sim_space)){
    # construct clean results table 
    p_table_part &amp;lt;- matrix(NA, nrow=niter, ncol=length(methods))
    colnames(p_table_part) &amp;lt;- methods
    
    # parameters of simulation
    theta_i &amp;lt;- sim_space[i, theta]
    mu_0_i &amp;lt;- sim_space[i, mu_0]
    effect_i &amp;lt;- sim_space[i, effect]
    n_i &amp;lt;- sim_space[i, n]
    treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Trt&amp;quot;), each=n_i)
    fd &amp;lt;- data.table(treatment=treatment)
    
    # mu (using algebra)
    if(algebra==TRUE){
      X &amp;lt;- model.matrix(~treatment)
      beta_0 &amp;lt;- log(mu_0_i)
      beta_1 &amp;lt;- log(effect_i*mu_0_i) - beta_0
      beta &amp;lt;- c(beta_0, beta_1)
      mu_i &amp;lt;- exp((X%*%beta)[,1])
    }else{ #  using R
      mu_vec &amp;lt;- c(mu_0_i, mu_0_i*effect_i)
      mu_i &amp;lt;- rep(mu_vec, each=n_i)
    }
    nb.error &amp;lt;- numeric(niter)
    
    for(iter in 1:niter){
      set.seed(niter*(i-1) + iter)
      fd[, y:=rnegbin(n=n_i*2, mu=mu_i, theta=theta_i)]
      fd[, log_yp1:=log10(y+1)]
      
      p.t &amp;lt;- t.test(y~treatment, data=fd, var.equal=TRUE)$p.value
      p.welch &amp;lt;- t.test(y~treatment, data=fd, var.equal=FALSE)$p.value
      p.log &amp;lt;- t.test(log_yp1~treatment, data=fd, var.equal=TRUE)$p.value
      p.wilcox &amp;lt;- wilcox.test(y~treatment, data=fd, exact=FALSE)$p.value
      
      # weighted lm, this will be ~same as welch for k=2 groups
      # fit &amp;lt;- gls(y~treatment, data=fd, weights = varIdent(form=~1|treatment), method=&amp;quot;ML&amp;quot;)
      # p.wls &amp;lt;- coef(summary(fit))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;p-value&amp;quot;]
      
      # negative binomial
      # default test using summary is Wald.
      # anova(fit) uses chisq of sequential fit, but using same estimate of theta
      # anova(fit2, fit1), uses chisq but with different estimate of theta
      # lrtest(fit) same as anova(fit2, fit1)
      
      # m1 &amp;lt;- glm.nb(y~treatment, data=fd)
      # m0 &amp;lt;- glm.nb(y~1, data=fd)
      # p.nb.x2 &amp;lt;- anova(m0, m1)[2, &amp;quot;Pr(Chi)&amp;quot;]
      # lr &amp;lt;- 2*(logLik(m1) - logLik(m0))
      # df.x2 = m0$df.residual-m1$df.residual
      # p.nb.x2 &amp;lt;- pchisq(lr, df=df.x2, lower.tail = F)
                
      m1 &amp;lt;- manyglm(y~treatment, data=fd) # default theta estimation &amp;quot;PHI&amp;quot;
      m0 &amp;lt;- manyglm(y~1, data=fd)
      lr &amp;lt;- 2*(logLik(m1) - logLik(m0))
      df.x2 = m0$df.residual-m1$df.residual
      p.nb &amp;lt;- coef(summary(m1))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;Pr(&amp;gt;wald)&amp;quot;] # Wald
      p.nb.x2 &amp;lt;- as.numeric(pchisq(lr, df=df.x2, lower.tail = F))
      p.nb.perm &amp;lt;- (anova(m0, m1, nBoot=nperm, show.time=&amp;#39;none&amp;#39;, p.uni=&amp;quot;unadjusted&amp;quot;)$uni.p)[2,1]

      # p.nb.x2 &amp;lt;- lrtest(fit)[2, &amp;quot;Pr(&amp;gt;Chisq)&amp;quot;] # doesn&amp;#39;t work with a data.table
      
      # quasipoisson
      fit &amp;lt;- glm(y~treatment, data=fd, family=quasipoisson)
      p.qp &amp;lt;- coeftest(fit)[2, &amp;quot;Pr(&amp;gt;|z|)&amp;quot;]
      
      p_table_part[iter,] &amp;lt;- c(p.t, p.welch, p.log, p.wilcox, p.nb, p.nb.x2, p.nb.perm, p.qp)
      
    } # niter
    p_table &amp;lt;- rbind(p_table, data.table(combo=i,
                                         mu_0=mu_0_i,
                                         effect=effect_i,
                                         n=n_i,
                                         theta=theta_i,
                                         nb.error=nb.error,
                                         p_table_part))
    
  } # combos
  
  return(p_table)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Algebra is slower (duh!)
# start_time &amp;lt;- Sys.time()
# do_sim(niter=niter, algebra=FALSE)
# end_time &amp;lt;- Sys.time()
# end_time - start_time
# 
# start_time &amp;lt;- Sys.time()
# do_sim(niter=niter, algebra=TRUE)
# end_time &amp;lt;- Sys.time()
# end_time - start_time

n_iter &amp;lt;- 2000
n_perm &amp;lt;- 2000
mu_0_list &amp;lt;- c(4) # control count
theta_list &amp;lt;- c(0.5) # dispersion
effect_list &amp;lt;- c(1, 2, 4) # effect size will be 1X, 1.5X, 2X, 3X
n_list &amp;lt;- c(5, 10) # sample size
sim_space &amp;lt;- data.table(expand.grid(theta=theta_list, mu_0=mu_0_list, effect=effect_list, n=n_list))

do_it &amp;lt;- FALSE # if FALSE the results are available as a file
if(do_it==TRUE){
  p_table &amp;lt;- do_sim(sim_space, niter=n_iter, nperm=n_perm)
  write.table(p_table, &amp;quot;../output/glm-v-lm.0004.txt&amp;quot;, row.names = FALSE, quote=FALSE)
}else{
  p_table &amp;lt;- fread(&amp;quot;../output/glm-v-lm.0001.txt&amp;quot;)
  p_table[, combo:=paste(effect, n, sep=&amp;quot;-&amp;quot;)]
  ycols &amp;lt;- setdiff(colnames(p_table), c(&amp;quot;combo&amp;quot;, &amp;quot;mu_0&amp;quot;, &amp;quot;effect&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;))
  res_table &amp;lt;- data.table(NULL)
  for(i in p_table[, unique(combo)]){
    p_table_part &amp;lt;- p_table[combo==i, ]
    mu_0_i &amp;lt;- p_table_part[1, mu_0]
    effect_i &amp;lt;- p_table_part[1, effect]
    n_i &amp;lt;- p_table_part[1, n]
    theta_i &amp;lt;- p_table_part[1, theta]
    n_iter_i &amp;lt;- nrow(p_table_part)
    p_sum &amp;lt;- apply(p_table_part[, .SD, .SDcols=ycols], 2, function(x) length(which(x &amp;lt;= 0.05))/n_iter_i)
    res_table &amp;lt;- rbind(res_table, data.table(mu_0 = mu_0_i,
                                             effect = effect_i,
                                             n = n_i,
                                             theta = theta_i,
                                             t(p_sum)))    
  }
  res_table[, n:=factor(n)]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;p&gt;Key: 1. “nb” uses the Wald test of negative binomial model. 2. “nb.x2” uses the LRT of negative binomial model. 3. “nb.perm” uses a permutation test on PIT residuals of negative binomial model 4. qp uses a LRT of quasi-poisson model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[effect==1,],
             caption = &amp;quot;Type 1 error as a function of n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:type-1-table&#34;&gt;Table 1: &lt;/span&gt;Type 1 error as a function of n&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;mu_0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;effect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;theta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Welch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Wilcoxan&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.perm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.032&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1280&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.054&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.036&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0505&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0675&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[effect!=1,],
             caption = &amp;quot;Power as a function of n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 2: &lt;/span&gt;Power as a function of n&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;mu_0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;effect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;theta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Welch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Wilcoxan&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nb.perm&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0465&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0845&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0825&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1950&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1860&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1025&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1730&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1850&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1285&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5345&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4405&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;plot&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- melt(res_table, 
            id.vars=c(&amp;quot;mu_0&amp;quot;, &amp;quot;effect&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;, &amp;quot;nb.error&amp;quot;),
            measure.vars=c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;, &amp;quot;nb.x2&amp;quot;, &amp;quot;nb.perm&amp;quot;, &amp;quot;qp&amp;quot;),
            variable.name=&amp;quot;model&amp;quot;,
            value.name=&amp;quot;frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[effect==1,], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(. ~ effect, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-30-glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update_files/figure-html/type-1-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[effect!=1,], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(. ~ effect, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-30-glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update_files/figure-html/plower-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Warton, D.I., Thibaut, L., Wang, Y.A., 2017. The PIT-trap—A “model-free” bootstrap procedure for inference about regression models with discrete, multivariate responses. PLOS ONE 12, e0181790. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0181790&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0181790&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Should we be skeptical of a &#34;large&#34; effect size if p &gt; 0.05?</title>
      <link>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/should-we-be-skeptical-of-a-large-effect-size-if-p-0-05/</guid>
      <description>


&lt;p&gt;Motivator: A twitter comment “Isn’t the implication that the large effect size is a direct byproduct of the lack of power? i.e. that if the the study had more power, the effect size would have been found to be smaller.”&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A thought: our belief in the magnitude of an observed effect should be based on our priors, which, hopefully, are formed from good mechanistic models and not sample size“.&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we observe a large effect but the sample size is small, then should we believe that the effect is strongly inflated?&lt;/li&gt;
&lt;li&gt;If we had measured a larger sample, would the effect be smaller?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But…maybe sample size should influence our prior, because the expected estimated effect &lt;strong&gt;magnitude&lt;/strong&gt; is bigger than than the true effect &lt;em&gt;if the true effect is near zero&lt;/em&gt; &lt;a href=&#34;../../../2019/04/the-statistical-significance-filter/&#34;&gt;explored a bit here&lt;/a&gt;. This is because, if an effect is near zero, estimates will vary on both sides of zero, and the absolute value of most of these estimates will be bigger than the absolute value of the true effect. But what effect size should we worry about this?&lt;/p&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: magrittr&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;p&gt;Simulate an experiment with two treatment levels (“control” and “treated”), with standardized (&lt;span class=&#34;math inline&#34;&gt;\(\frac{\delta}{\sigma}\)&lt;/span&gt;) effect sizes of 0.05, .1, .2, .3, .5, .8, 1, 2 and sample sizes of 100, 20, and 10. Cohen considered .8 a “large” standardized effect but I’ll leave what is large up to you. Regardless, its worth comparing the results here to observed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100 # per treatment level. power will be a function of effect size
b_array &amp;lt;- c(0.05, 0.1, 0.2, 0.3, 0.5, 0.8, 1, 2)
niter &amp;lt;- 10^4
res_table &amp;lt;- data.table(NULL)
power_table &amp;lt;- data.table(NULL)
for(b1 in b_array){
  y1 &amp;lt;- matrix(rnorm(n*niter), nrow=n)
  y2 &amp;lt;- matrix(rnorm(n*niter), nrow=n) + b1
  d100 &amp;lt;- apply(y2, 2, mean) - apply(y1,2,mean)
  d20 &amp;lt;- apply(y2[1:20,], 2, mean) - apply(y1[1:20,],2,mean)
  d10 &amp;lt;- apply(y2[1:10,], 2, mean) - apply(y1[1:10,],2,mean)
  res_table &amp;lt;- rbind(res_table, data.table(b=b1, d100=d100, d20=d20, d10=d10))
  power_table &amp;lt;- rbind(power_table, data.table(
    b=b1,
    &amp;quot;power (n=100)&amp;quot;=power.t.test(n=100, delta=b1, sd=1)$power,
    &amp;quot;power (n=20)&amp;quot;=power.t.test(n=20, delta=b1, sd=1)$power,
    &amp;quot;power (n=10)&amp;quot;=power.t.test(n=10, delta=b1, sd=1)$power
  ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;power-for-each-simulation-combination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power for each simulation combination&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(power_table, digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;absolute-median-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Absolute median effects&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[, .(&amp;quot;median(|b|) (n=100)&amp;quot;=median(abs(d100)),
              &amp;quot;median(|b|) (n=20)&amp;quot;=median(abs(d20)),
              &amp;quot;median(|b|) (n=10)&amp;quot;=median(abs(d10))
              ), by=b], digits=c(2, 3, 3, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median(|b|) (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.305&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.121&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.227&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.320&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.254&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.329&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.297&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.319&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.378&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.503&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.510&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.799&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.799&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.798&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.003&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.005&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;inflation-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inflation factors&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(res_table[, .(
              &amp;quot;IF (n=100)&amp;quot; = median(abs(d100))/b,
              &amp;quot;IF (n=20)&amp;quot; = median(abs(d20))/b,
              &amp;quot;IF (n=10)&amp;quot; = median(abs(d10))/b
              ), by=b], digits=c(2, 1, 1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=100)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=20)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IF (n=10)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;directly-answering-question-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Directly answering question #2&lt;/h1&gt;
&lt;p&gt;Notice that if power is obove about .2, the absolute median effect is not inflated. That is, a study would have to be wicked underpowered for there to be an expected inflated effect size. This is an indirect answer to question no. 2. A more direct answer is explored by computing the log10 ratio of absolute effects between sample size levels for each run of the simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_table[, d100.d20:=log10(abs(d20)/abs(d100))]
res_table[, d100.d10:=log10(abs(d10)/abs(d100))]
res_table[, d20.d10:=log10(abs(d10)/abs(d20))]
res_melt &amp;lt;- melt(res_table[, .SD, .SDcols=c(&amp;quot;b&amp;quot;, &amp;quot;d100.d20&amp;quot;, &amp;quot;d100.d10&amp;quot;, &amp;quot;d20.d10&amp;quot;)], id.vars=&amp;quot;b&amp;quot;, variable.name=&amp;quot;comparison&amp;quot;, value.name=&amp;quot;contrast&amp;quot;)
res_melt[, b:=factor(b)]
pd &amp;lt;- position_dodge(0.8)
ggplot(data=res_melt, aes(x=b, y=contrast, fill=comparison)) +
  geom_boxplot(position=pd, outlier.shape=NA) +
  coord_cartesian(ylim=c(-1.25, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-28-should-we-be-skeptical-of-a-large-effect-size-if-p-0-05_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the true effect is really small (0.05) then a smaller sample will often estimate a larger effect (just less than 75% of the time when decreasing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; from 100 to 20). When the true effect is about 0.5 or higher, decreasing sample size is no more likely to estimate a bigger effect than increasing sample size.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The original question, and the motivating tweet, raise the question of what a “large” effect is. There is large in the absolute since, which would require subject level expertise to identify, and large relative to noise.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that the original post was not about the &lt;a href=&#34;https://rdoodles.rbind.io/2019/04/the-statistical-significance-filter/&#34;&gt;statistical significance filter&lt;/a&gt; but about the ethics of a RCT in which the observed effect was “large” but there was not enough power to get a statistically significant p-value.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;at least if we are using an experiment to estimate an effect. If we are trying to estimate multiple effects, the bigger observed effects have tend to be inflated and the smaller observed effects tend to be dd&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Blocking vs. covariate adjustment</title>
      <link>/2019/04/blocking-vs-covariate-adjustment/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/blocking-vs-covariate-adjustment/</guid>
      <description>




HUGOMORE42

&lt;p&gt;“A more efficient design would be to first group the rats into homogeneous subsets based on baseline food consumption. This could be done by ranking the rats from heaviest to lightest eaters and then grouping them into pairs by taking the first two rats (the two that ate the most during baseline), then the next two in the list, and so on. The difference from a completely randomised design is that one rat within each pair is randomised to one of the treatment groups, and the other rat is then assigned to the remaining treatment group. Each rat in a pair is expected to eat a similar amount of food during the experiment because they have been matched on their baseline food consumption. By removing this source of variation, the comparison between rats in a pair will be mostly unaffected by the amount of food they eat, allowing treatment effects to be more easily detected.” – Lazic, Stanley E.. Experimental Design for Laboratory Biologists: Maximising Information and Improving Reproducibility . Cambridge University Press. Kindle Edition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)
library(doBy)
library(lmerTest)
library(nlme)

odd &amp;lt;- function(x) x%%2 != 0
even  &amp;lt;- function(x) x%%2 == 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate data in which the response is a function of baseline_food consumption:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10^1
niter &amp;lt;- 1000
# create the response y as a function of baseline_food
out_cols &amp;lt;- c(&amp;quot;adj&amp;quot;, &amp;quot;block.rand&amp;quot;, &amp;quot;block.fix&amp;quot;, &amp;quot;block.adj&amp;quot;)
b_mat &amp;lt;- p_mat &amp;lt;- matrix(NA, nrow=niter, ncol=length(out_cols))
colnames(b_mat) &amp;lt;-colnames(p_mat) &amp;lt;- out_cols
for(iter in 1:niter){
  baseline_food &amp;lt;- rnorm(n*2)
  beta_baseline_food &amp;lt;- 0.6
  y &amp;lt;- beta_baseline_food*baseline_food + sqrt(1-beta_baseline_food^2)*rnorm(n*2)
  
  # covariate adjustment
  # add treatment effect to half
  treatment &amp;lt;- as.factor(rep(c(&amp;quot;tr&amp;quot;, &amp;quot;cn&amp;quot;), each=n))
  beta_1 &amp;lt;- 1
  y[1:n] &amp;lt;- y[1:n] + beta_1
  fit1 &amp;lt;- lm(y ~ baseline_food + treatment)
  b_mat[iter, &amp;quot;adj&amp;quot;] &amp;lt;- coef(summary(fit1))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  p_mat[iter, &amp;quot;adj&amp;quot;] &amp;lt;- coef(summary(fit1))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  
  # block
  treatment &amp;lt;- NULL
  for(i in 1:n){
    treatment &amp;lt;- c(treatment, sample(c(&amp;quot;tr&amp;quot;, &amp;quot;cn&amp;quot;), 2))
  }
  fake_data &amp;lt;- data.table(y=y, baseline_food=baseline_food)
  setorder(fake_data, baseline_food)
  fake_data[, treatment:=factor(treatment)]
  fake_data[, block:=factor(rep(1:n, each=2))]
  fake_data[, y_exp:=ifelse(treatment==&amp;quot;tr&amp;quot;, y+1, y)]
  # fit2 &amp;lt;- lmer(y_exp ~ treatment + (1|block), data=fake_data)
  # b_mat[iter, &amp;quot;block&amp;quot;] &amp;lt;- coef(summary(fit2))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  fit2 &amp;lt;- lme(y_exp ~ treatment, random= ~1|block, data=fake_data)
  b_mat[iter, &amp;quot;block.rand&amp;quot;] &amp;lt;- coef(summary(fit2))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Value&amp;quot;]
  p_mat[iter, &amp;quot;block.rand&amp;quot;] &amp;lt;- coef(summary(fit2))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;p-value&amp;quot;]

  fit2b &amp;lt;- lm(y_exp ~ block + treatment, data=fake_data)
  b_mat[iter, &amp;quot;block.fix&amp;quot;] &amp;lt;- coef(summary(fit2b))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  p_mat[iter, &amp;quot;block.fix&amp;quot;] &amp;lt;- coef(summary(fit2b))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]

  fit3 &amp;lt;- lm(y_exp ~ baseline_food + treatment, data=fake_data)
  b_mat[iter, &amp;quot;block.adj&amp;quot;] &amp;lt;- coef(summary(fit3))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Estimate&amp;quot;]
  p_mat[iter, &amp;quot;block.adj&amp;quot;] &amp;lt;- coef(summary(fit3))[&amp;quot;treatmenttr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Estimates&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(b_mat, 2, quantile, probs=c(0.025, 0.5, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             adj block.rand block.fix block.adj
## 2.5%  0.2797401  0.1476697 0.1476697 0.1528887
## 50%   1.0076974  1.0161665 1.0161665 1.0165686
## 97.5% 1.7149816  1.8121259 1.8121259 1.7995946&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Power&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(p_mat, 2, function(x) sum(x &amp;lt; 0.05)/niter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        adj block.rand  block.fix  block.adj 
##      0.732      0.576      0.558      0.619&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusion: over this model space, simply adjusting for baseline food consumption is more powerful than creating blocks using baseline food consumption.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The statistical significance filter</title>
      <link>/2019/04/the-statistical-significance-filter/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/the-statistical-significance-filter/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-reported-effect-sizes-are-inflated&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Why reported effect sizes are inflated&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setup&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploration-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Exploration 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unconditional-means-power-and-sign-error&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Unconditional means, power, and sign error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-means&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Conditional means&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#filter-0.05&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; filter = 0.05&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#filter-0.2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; filter = 0.2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;why-reported-effect-sizes-are-inflated&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Why reported effect sizes are inflated&lt;/h1&gt;
&lt;p&gt;This post is motivated by many discussions in Gelman’s blog &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2011/09/10/the-statistical-significance-filter/&#34;&gt;but start here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we estimate an effect&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, the estimate will be a little inflated or a little diminished relative to the true effect but the expectation of the effect is the true effect. If all effects were reported, there would be no bias toward inflated effects. Reported effects are inflated if we use p-values to decide which to report and which to archive in the file drawer.&lt;/p&gt;
&lt;p&gt;The magnitude of an estimate of an effect is a function of its true effect size plus sampling error (this is with a perfectly designed and executed study. In any real study there will be biases of various sorts). The absolute magnitude of sampling error is bigger with smaller &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The relative magnitude is bigger for smaller true effect size. Consequently, estimates in low powered studies (some combination of low &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and small true effect size) can be wildly off, especially relative to the true effect size. In low powered studies, it is these “wildly-off” estimates that are big enough to have &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; 0.05\)&lt;/span&gt;. This phenomenon attenuates as power increases because estimates are less and less wildely-off.&lt;/p&gt;
&lt;p&gt;Here is a simulation of this&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(data.table)

source(&amp;quot;../R/fake_x.R&amp;quot;) # bookdown&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Exploration 1&lt;/h1&gt;
&lt;p&gt;Modeling a typical set of experiments in ecology or physiology with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; independent responses each with the same standardized effect size. How big are the reported effect sizes for the subset with &lt;span class=&#34;math inline&#34;&gt;\(p.val &amp;lt; 0.05\)&lt;/span&gt; (with or without correction for multiple testing). Make this a function of power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 20 # per treatment level. power will be a function of effect size
np1 &amp;lt;- n+1
N &amp;lt;- 2*n
niter &amp;lt;- 100 # number of iterations for each combination of fake data parameters
treatment_levels &amp;lt;- c(&amp;quot;Cn&amp;quot;, &amp;quot;Tr&amp;quot;)
Treatment &amp;lt;- rep(treatment_levels, each=n)
p &amp;lt;- 50
b &amp;lt;- pval &amp;lt;- numeric(p)
combo &amp;lt;- 0 # which treatment combo
res_table &amp;lt;- data.table(NULL)
for(beta_1 in c(0.05, 0.15, 0.5, 1)){
  combo &amp;lt;- combo + 1
  j1 &amp;lt;- 0
  j &amp;lt;- 0
  res &amp;lt;- matrix(NA, nrow=niter*p, ncol=3)
  colnames(res) &amp;lt;- c(&amp;quot;ID&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;pval&amp;quot;)
  for(iter in 1:niter){
    j1 &amp;lt;- j1 + j # completed row -- start row will be this plus 1
    Y &amp;lt;- matrix(rnorm(n*2*p), nrow=n*2, ncol=p)
    Y[np1:N,] &amp;lt;- Y[np1:N,] + beta_1
    fit &amp;lt;- lm(Y ~ Treatment)
    fit.coefs &amp;lt;- coef(summary(fit))
    for(j in 1:p){# inefficient...how do I extract this without a 
      res[j1 + j, &amp;quot;ID&amp;quot;] &amp;lt;- niter*(combo - 1) + iter
      res[j1 + j, &amp;quot;b&amp;quot;] &amp;lt;- fit.coefs[[j]][&amp;quot;TreatmentTr&amp;quot;, &amp;quot;Estimate&amp;quot;]
      res[j1 + j, &amp;quot;pval&amp;quot;] &amp;lt;- fit.coefs[[j]][&amp;quot;TreatmentTr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
    }
  } # iter
  res_table &amp;lt;- rbind(res_table, data.table(beta=beta_1, res))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unconditional-means-power-and-sign-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Unconditional means, power, and sign error&lt;/h1&gt;
&lt;p&gt;beta is the true effect. The unconditional mean is the mean of the estimated effect. The absolute value of the estimated effect is the measure of “size” or magnitude and the mean of the absolute values of the effect size will be bigger then the mean effect size if the true effect size is near zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table1 &amp;lt;- res_table[, .(mean_unconditional=mean(b),
              mean_abs_unconditional=mean(abs(b)),
              power = sum(pval &amp;lt; 0.05 &amp;amp; b &amp;gt; 0)/niter/p,
              sign.error=sum(pval &amp;lt; 0.05 &amp;amp; b &amp;lt; 0)/niter/p), by=.(beta)]
knitr::kable(table1, digits=c(2, 2, 2, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_unconditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_abs_unconditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;power&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sign.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.88&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-means&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Conditional means&lt;/h1&gt;
&lt;p&gt;The conditional mean is the mean effect size conditional on pval &amp;lt; filter. Again, beta is the true effect. And again, the absolute estimate (&lt;span class=&#34;math inline&#34;&gt;\(|b|\)&lt;/span&gt;) is the measure of effect “size”.&lt;/p&gt;
&lt;div id=&#34;filter-0.05&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; filter = 0.05&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table2 &amp;lt;- res_table[pval &amp;lt; 0.05, .(mean_conditional=mean(b),
                         mean_abs.conditional=mean(abs(b)),
                         multiplier = mean(abs(b))/beta), by=.(beta)]
knitr::kable(table2, digits=c(2, 2, 2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_abs.conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;multiplier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.08&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;filter-0.2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; filter = 0.2&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table3 &amp;lt;- res_table[pval &amp;lt; 0.2, .(mean_conditional=mean(b),
                         mean_abs.conditional=mean(abs(b)),
                         multiplier = mean(abs(b))/beta), by=.(beta)]
knitr::kable(table3, digits=c(2, 2, 2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_abs.conditional&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;multiplier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.55&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;for example, in an experiment, if we compare the mean response between a control group and a treated group, the difference in means is the effect. More generally, an effect is the coefficient of a linear model&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Covariate adjustment in randomized experiments</title>
      <link>/2019/04/covariate-adjustment-in-randomized-experiments/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/covariate-adjustment-in-randomized-experiments/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://twitter.com/statsepi/status/1115902270888128514&#34;&gt;The post motivated by a tweetorial from Darren Dahly&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In an experiment, do we adjust for covariates that differ between treatment levels measured pre-experiment (“imbalance” in random assignment), where a difference is inferred from a t-test with p &amp;lt; 0.05? Or do we adjust for all covariates, regardless of differences pre-test? Or do we adjust only for covariates that have sustantial correlation with the outcome? Or do we not adjust at all?&lt;/p&gt;
&lt;p&gt;The original tweet focussed on Randomized Clinical Trials, which typically have large sample size. Here I simulate experimental biology, which typically has much smaller n.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(data.table)

source(&amp;quot;../R/fake_x.R&amp;quot;) # bookdown&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fake-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fake data&lt;/h1&gt;
&lt;p&gt;Generate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; correlated variables and assign the first to the response (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt;) and the rest to the covariates (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;). Construct a treatment variable and effect and add this to the response.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100 # per treatment level - this is modified below
p &amp;lt;- 3 # number of covariates (columns of the data)
pp1 &amp;lt;- p+1
beta_0 &amp;lt;- 0 # intercept

niter &amp;lt;- 2000 # modified below
measure_cols &amp;lt;- c(&amp;quot;no_adjust&amp;quot;, &amp;quot;imbalance&amp;quot;, &amp;quot;all_covariates&amp;quot;, &amp;quot;weak_covariates&amp;quot;, &amp;quot;strong_covariates&amp;quot;)

xcols &amp;lt;- paste0(&amp;quot;X&amp;quot;, 1:p)
build_ycols &amp;lt;- c(&amp;quot;Y_o&amp;quot;, xcols)
cor_ycols &amp;lt;- c(&amp;quot;Y&amp;quot;, xcols)

b_mat &amp;lt;- data.table(NULL)
se_mat &amp;lt;- data.table(NULL)
p_mat &amp;lt;- data.table(NULL)
ci_mat &amp;lt;- data.table(NULL)

for(beta_1 in c(0, 0.2, 0.8)){ # treatment effect on standardized scale
  beta &amp;lt;- c(beta_0, beta_1)
  for(n in c(6, 10, 50)){
    # larger iterations with smaller n
    niter &amp;lt;- round((3*10^4)/sqrt(n), 0)
    niter &amp;lt;- 2000
    
    # repopulate with NA each n
    b &amp;lt;- se &amp;lt;- pval &amp;lt;- ci &amp;lt;- matrix(NA, nrow=niter, ncol=length(measure_cols))
    colnames(b) &amp;lt;- colnames(se) &amp;lt;- colnames(pval) &amp;lt;- colnames(ci) &amp;lt;- measure_cols
    
    Treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Tr&amp;quot;), each=n)
    X &amp;lt;- model.matrix(formula(&amp;quot;~ Treatment&amp;quot;))
    
    for(iter in 1:niter){
      # generate p random, correlated variables. The first is assigned to Y
      fake_data &amp;lt;- fake.X(n*2, pp1, fake.eigenvectors(pp1), fake.eigenvalues(pp1))
      colnames(fake_data) &amp;lt;- build_ycols
      
      # resacale so that var(Y) = 1, where Y is the first column
      fake_data &amp;lt;- fake_data/sd(fake_data[,1])
      
      fake_data &amp;lt;- data.table(fake_data)
      
      # view the scatterplots
      #gg &amp;lt;- ggpairs(X,progress = ggmatrix_progress(clear = FALSE))
      show_it &amp;lt;- FALSE
      if(show_it ==TRUE){
        gg &amp;lt;- ggpairs(fake_data)
        print(gg, progress = F)
      }
      
      # add the treatment effect
      fake_data[, Y:=Y_o + X%*%beta]
      fake_data[, Treatment:=Treatment]
      
      # model 1 - just the treatment
      fit1 &amp;lt;- lm(Y ~ Treatment, data=fake_data)
      res &amp;lt;- coef(summary(fit1))[&amp;quot;TreatmentTr&amp;quot;, ]
      b[iter, 1] &amp;lt;- res[&amp;quot;Estimate&amp;quot;]
      se[iter, 1] &amp;lt;- res[&amp;quot;Std. Error&amp;quot;]
      pval[iter, 1] &amp;lt;-res[&amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
      ci_i &amp;lt;- confint(fit1)[&amp;quot;TreatmentTr&amp;quot;,]
      ci[iter, 1] &amp;lt;- ifelse(beta_1 &amp;gt;= ci_i[1] &amp;amp; beta_1 &amp;lt;= ci_i[2], 1, 0)
      res1 &amp;lt;- copy(res)
      
      # model 2 - adjust for imablance
      inc_xcols &amp;lt;- NULL
      for(i in 1:p){
        formula &amp;lt;- paste0(xcols[i], &amp;quot; ~ Treatment&amp;quot;)
        fit2a &amp;lt;- lm(formula, data=fake_data)
        if(coef(summary(fit2a))[&amp;quot;TreatmentTr&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; 0.05){
          inc_xcols &amp;lt;- c(inc_xcols, xcols[i])
        }
      }
      if(length(inc_xcols) &amp;gt; 0){ # if any signifianct effects refit, otherwise use old fit
        formula &amp;lt;- paste0(&amp;quot;Y ~ Treatment + &amp;quot;, paste(inc_xcols, collapse=&amp;quot; + &amp;quot;))
        fit2b &amp;lt;- lm(formula, data=fake_data)
        res &amp;lt;- coef(summary(fit2b))[&amp;quot;TreatmentTr&amp;quot;, ]
        ci_i &amp;lt;- confint(fit2b)[&amp;quot;TreatmentTr&amp;quot;,]
      }else{
        res &amp;lt;- res1
      }
      b[iter, 2] &amp;lt;- res[&amp;quot;Estimate&amp;quot;]
      se[iter, 2] &amp;lt;- res[&amp;quot;Std. Error&amp;quot;]
      pval[iter, 2] &amp;lt;-res[&amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
      ci[iter, 2] &amp;lt;- ifelse(beta_1 &amp;gt;= ci_i[1] &amp;amp; beta_1 &amp;lt;= ci_i[2], 1, 0)
      
      
      # model 3- adjust for covariates
      (ycor &amp;lt;- abs(cor(fake_data[, .SD, .SDcols=cor_ycols])[2:pp1, 1]))
      mean(ycor)
      
      j &amp;lt;- 2
      for(target_cor in c(0, .2, .4)){
        j &amp;lt;- j+1
        if(target_cor == 0.2){
          inc &amp;lt;- which(ycor &amp;lt; target_cor) # include only weak covariates
        }else{
          inc &amp;lt;- which(ycor &amp;gt; target_cor) # include all OR strong covariates
        }
        if(length(inc) &amp;gt; 0){  # if matches refit, otherwise use old fit
          inc_xcols &amp;lt;- xcols[inc]
          formula &amp;lt;- paste0(&amp;quot;Y ~ Treatment + &amp;quot;, paste(inc_xcols, collapse=&amp;quot; + &amp;quot;))
          fit3 &amp;lt;- lm(formula, data=fake_data)
          res &amp;lt;- coef(summary(fit3))[&amp;quot;TreatmentTr&amp;quot;, ]
          ci_i &amp;lt;- confint(fit3)[&amp;quot;TreatmentTr&amp;quot;,]
        }else{
          res &amp;lt;- res1
        }
        b[iter, j] &amp;lt;- res[&amp;quot;Estimate&amp;quot;]
        se[iter, j] &amp;lt;- res[&amp;quot;Std. Error&amp;quot;]
        pval[iter, j] &amp;lt;-res[&amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
        ci[iter, j] &amp;lt;- ifelse(beta_1 &amp;gt;= ci_i[1] &amp;amp; beta_1 &amp;lt;= ci_i[2], 1, 0)
      }
    }  
    b_mat &amp;lt;- rbind(b_mat, data.table(n=n, beta_1=beta_1, b))
    se_mat &amp;lt;- rbind(se_mat, data.table(n=n, beta_1=beta_1, se))
    p_mat &amp;lt;- rbind(p_mat, data.table(n=n, beta_1=beta_1, pval))
    ci_mat &amp;lt;- rbind(ci_mat, data.table(n=n, beta_1=beta_1, ci))
  }
}

p_long &amp;lt;- melt(p_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;p&amp;quot;)
ci_long &amp;lt;- melt(ci_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;covers&amp;quot;)
b_long &amp;lt;- melt(b_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;b&amp;quot;)
se_long &amp;lt;- melt(se_mat, measure.vars=measure_cols, variable.name=&amp;quot;method&amp;quot;, value.name=&amp;quot;se&amp;quot;)


#ci_long[, .(coverage=sum(covers)/niter), by=.(method, n, beta_1)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-estimates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distribution of estimates&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pd &amp;lt;- position_dodge(0.8)
gg &amp;lt;- ggplot(data=b_long, aes(x=factor(n), y=b, fill=method)) +
  geom_boxplot(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-se-of-estimate&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distribution of SE of estimate&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pd &amp;lt;- position_dodge(0.8)
gg &amp;lt;- ggplot(data=se_long, aes(x=factor(n), y=se, fill=method)) +
  geom_boxplot(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# type I
p_sum &amp;lt;- p_long[, .(error=sum(p &amp;lt; 0.05)/niter), by=.(method, n, beta_1)]
pd &amp;lt;- position_dodge(0.8)
gg &amp;lt;- ggplot(data=p_sum[beta_1==0], aes(x=factor(n), y=error, color=method, group=method)) +
  geom_point(position=pd) +
  geom_line(position=pd) + 
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  ylab(&amp;quot;Type I error&amp;quot;) +
  # facet_grid(.~beta_1) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# power
# need p_sum from above
gg &amp;lt;- ggplot(data=p_sum[beta_1!=0], aes(x=factor(n), y=error, color=method)) +
  geom_point(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  ylab(&amp;quot;Power&amp;quot;) +
  facet_grid(.~beta_1) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sign-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sign error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sign error
p_long2 &amp;lt;- cbind(p_long, b=b_long[, b])
sign_error &amp;lt;- p_long2[beta_1 &amp;gt; 0, .(error=sum(p &amp;lt; 0.1 &amp;amp; b &amp;lt; 0)/niter), by=.(method, n, beta_1)]
gg &amp;lt;- ggplot(data=sign_error, aes(x=factor(n), y=error, color=method)) +
  geom_point(position=pd) +
  xlab(&amp;quot;sample size (per treatment level)&amp;quot;) +
  ylab(&amp;quot;Sign error&amp;quot;) +
  facet_grid(.~beta_1) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-12-covariate-adjustment-in-randomized-experiments_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What to write, and not write, in a results section — an ever-growing list</title>
      <link>/2019/01/what-to-write-and-not-write-in-a-results-section-an-ever-growing-list/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/what-to-write-and-not-write-in-a-results-section-an-ever-growing-list/</guid>
      <description>


&lt;p&gt;“GPP (n=4 per site) increased from the No Wildlife site to the Hippo site but was lowest at the Hippo + WB site (Fig. 6); however, these differences were not significant due to low sample sizes and high variability.” – Subalusky, A.L., Dutton, C.L., Njoroge, L., Rosi, E.J., and Post, D.M. (2018). Organic matter and nutrient inputs from large wildlife influence ecosystem function in the Mara River, Africa. Ecology 99, 2558–2574.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paired line plots</title>
      <link>/2019/01/paired-line-plots/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/paired-line-plots/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-libraries&#34;&gt;load libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#make-some-fake-data&#34;&gt;make some fake data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#make-a-plot-with-ggplot&#34;&gt;make a plot with ggplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;ggplot scripts to draw figures like those in the Dynamic Ecology post &lt;a href=&#34;https://dynamicecology.wordpress.com/2019/01/21/paired-line-plots-a-k-a-reaction-norms-to-visualize-likert-data/#comments&#34;&gt;Paired line plots (a.k.a. “reaction norms”) to visualize Likert data&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-some-fake-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;make some fake data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)
n &amp;lt;- 40
self &amp;lt;- rbinom(n, 5, 0.25) + 1
others &amp;lt;- self + rbinom(n, 3, 0.5)
fd &amp;lt;- data.table(id=factor(rep(1:n, 2)),
                 who=factor(rep(c(&amp;quot;self&amp;quot;, &amp;quot;others&amp;quot;), each=n)),
                 stigma &amp;lt;- c(self, others))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-a-plot-with-ggplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;make a plot with ggplot&lt;/h1&gt;
&lt;p&gt;The students are identified by the column “id”. Each students’ two responses (“self” and “others”) are joined by a line using geom_line(), which knows who to join with the “group=id” statement in the aes function in line 1. The alpha setting draws transparent lines (alpha=1 is opaque) so the more lines that are superimposed the darker the line. The lines are slightly jittered in the vertical direction so that overlapping lines appear wider.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;jitter_mag &amp;lt;- 0.03
gg &amp;lt;- ggplot(data=fd, aes(x=who, y=stigma, group=id)) +
  geom_line(size=1,
            alpha=0.3, 
            position=position_jitter(w=0, h=jitter_mag)) +
  ylab(&amp;quot;Amount of stigma (1=low, 6=high)&amp;quot;) +
  xlab(&amp;quot;Responding about views of others or self&amp;quot;) +
  theme_pubr() +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-22-paired-line-plots_files/figure-html/ggplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GLM vs. t-tests vs. non-parametric tests if all we care about is NHST</title>
      <link>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../../../2019/05/glm-vs-t-tests-vs-non-parametric-tests-if-all-we-care-about-is-nhst-update/&#34;&gt;This post has been updated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A skeleton simulation of different strategies for NHST for count data if all we care about is a p-value, as in bench biology where p-values are used to simply give one confidence that something didn’t go terribly wrong (similar to doing experiments in triplicate – it’s not the effect size that matters only “we have experimental evidence of a replicable effect”).&lt;/p&gt;
&lt;p&gt;tl;dr - At least for Type I error at small &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, log(response) and Wilcoxan have the best performance over the simulation space. T-test is a bit conservative. Welch is even more conservative. glm-nb is too liberal.&lt;/p&gt;
&lt;div id=&#34;load-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;load libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggpubr)
library(MASS)
library(data.table)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Single factor with two levels and a count (negative binomial) response.&lt;/li&gt;
&lt;li&gt;Relative effect sizes of 0%, 50%, 100%, and 200%&lt;/li&gt;
&lt;li&gt;Ref count of 4, 10, 100&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of 5, 10, 20, 40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;p&lt;/em&gt;-values computed from&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;t-test on raw response&lt;/li&gt;
&lt;li&gt;Welch t-test on raw response&lt;/li&gt;
&lt;li&gt;t-test on log transformed response&lt;/li&gt;
&lt;li&gt;Wilcoxan test&lt;/li&gt;
&lt;li&gt;glm with negative binomial family and log-link&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_sim &amp;lt;- function(niter=1, return_object=NULL){
  # the function was run with n=1000 and the data saved. on subsequent runs
  # the data are loaded from a file
  # the function creates three different objects to return, the object
  # return is specified by &amp;quot;return_object&amp;quot; = NULL, plot_data1, plot_data2
  methods &amp;lt;- c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;)
  p_table_part &amp;lt;- matrix(NA, nrow=niter, ncol=length(methods))
  colnames(p_table_part) &amp;lt;- methods
  p_table &amp;lt;- data.table(NULL)
  
  res_table &amp;lt;- data.table(NULL)
  beta_0_list &amp;lt;- c(4, 10, 100) # control count
  theta_list &amp;lt;- c(0.5, 1, 100) # dispersion
  effect_list &amp;lt;- c(1:3, 5) # relative effect size will be 0%, 50%, 100%, 200%
  n_list &amp;lt;- c(5, 10, 20, 40) # sample size
  n_rows &amp;lt;- length(beta_0_list)*length(theta_list)*length(effect_list)*length(n_list)*niter
  sim_space &amp;lt;- expand.grid(theta_list, beta_0_list, effect_list, n_list)
  plot_data1 &amp;lt;- data.table(NULL)
  plot_data2 &amp;lt;- data.table(NULL)
  debug_table &amp;lt;- data.table(matrix(NA, nrow=niter, ncol=2))
  setnames(debug_table, old=colnames(debug_table), new=c(&amp;quot;seed&amp;quot;,&amp;quot;model&amp;quot;))
  debug_table[, seed:=as.integer(seed)]
  debug_table[, model:=as.character(model)]
  i &amp;lt;- 0
  for(theta_i in theta_list){
    for(beta_0 in beta_0_list){
      # first get plots of distributions given parameters
      y &amp;lt;- rnegbin(n=10^4, mu=beta_0, theta=theta_i)
      x_i &amp;lt;- seq(min(y), max(y), by=1)
      prob_x_i &amp;lt;- dnbinom(x_i, size=theta_i, mu=beta_0)
      plot_data1 &amp;lt;- rbind(plot_data1, data.table(
        theta=theta_i,
        mu=beta_0,
        x=x_i,
        prob_x=prob_x_i
      ))
      # the simulation
      for(effect in effect_list){
        for(n in n_list){
          beta_1 &amp;lt;- (effect-1)*beta_0/2 # 0% 50% 100%

          do_manual &amp;lt;- FALSE
          if(do_manual==TRUE){
            theta_i &amp;lt;- res_table[row, theta]
            beta_0 &amp;lt;- res_table[row, beta_0]
            beta_1 &amp;lt;- res_table[row, beta_1]
            n &amp;lt;- res_table[row, n]
          }
          
          beta &amp;lt;- c(beta_0, beta_1)
          treatment &amp;lt;- rep(c(&amp;quot;Cn&amp;quot;, &amp;quot;Trt&amp;quot;), each=n)
          X &amp;lt;- model.matrix(~treatment)
          mu &amp;lt;- (X%*%beta)[,1]
          fd &amp;lt;- data.table(treatment=treatment, y=NA)
          for(iter in 1:niter){
            i &amp;lt;- i+1
            set.seed(i)
            fd[, y:=rnegbin(n=n*2, mu=mu, theta=theta_i)]
            fd[, log_yp1:=log10(y+1)]
            p.t &amp;lt;- t.test(y~treatment, data=fd, var.equal=TRUE)$p.value
            p.welch &amp;lt;- t.test(y~treatment, data=fd, var.equal=FALSE)$p.value
            p.log &amp;lt;- t.test(log_yp1~treatment, data=fd, var.equal=TRUE)$p.value
            p.wilcox &amp;lt;- wilcox.test(y~treatment, data=fd, exact=FALSE)$p.value
            fit &amp;lt;- glm.nb(y~treatment, data=fd)
            debug_table[iter, seed:=i]
            debug_table[iter, model:=&amp;quot;glm.nb&amp;quot;]
            #if(fit$th.warn == &amp;quot;iteration limit reached&amp;quot;){
            if(!is.null(fit$th.warn)){
              fit &amp;lt;- glm(y~treatment, data=fd, family=poisson)
              debug_table[iter, model:=&amp;quot;poisson&amp;quot;]
            }
            p.nb &amp;lt;- coef(summary(fit))[&amp;quot;treatmentTrt&amp;quot;, &amp;quot;Pr(&amp;gt;|z|)&amp;quot;]
            p_table_part[iter,] &amp;lt;- c(p.t, p.welch, p.log, p.wilcox, p.nb)
          }
          p_table &amp;lt;- rbind(p_table, data.table(p_table_part, debug_table))
          p_sum &amp;lt;- apply(p_table_part, 2, function(x) length(which(x &amp;lt;= 0.05))/niter)
          res_table &amp;lt;- rbind(res_table, data.table(beta_0=beta_0,
                                                   beta_1=beta_1,
                                                   n=n,
                                                   theta=theta_i,
                                                   t(p_sum)))
        } # n
      } # effect
      plot_data2 &amp;lt;- rbind(plot_data2, data.table(
        theta=theta_i,
        mu=beta_0,
        n_i=n,
        beta1=beta_1,
        x=treatment,
        y=fd[, y]
      ))
    }
  }
  if(is.null(return_object)){return(res_table)}else{
    if(return_object==&amp;quot;plot_data1&amp;quot;){return(plot_data1)}
    if(return_object==&amp;quot;plot_data2&amp;quot;){return(plot_data2)}
    
  }
  
}

do_it &amp;lt;- FALSE # if FALSE the results are available as a file
if(do_it==TRUE){
  res_table &amp;lt;- do_sim(niter=1000)
  write.table(res_table, &amp;quot;../output/glm-t-wilcoxon.txt&amp;quot;, row.names = FALSE, quote=FALSE)
}else{
  plot_data &amp;lt;- do_sim(niter=1, return_object=&amp;quot;plot_data2&amp;quot;)
  res_table &amp;lt;- fread(&amp;quot;../output/glm-t-wilcoxon.txt&amp;quot;)
  res_table[, n:=factor(n)]
}
#res_table&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Distribution of the response for the 3 x 3 simulation space&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extreme inelegance
mu_levels &amp;lt;- unique(plot_data[, mu])
theta_levels &amp;lt;- unique(plot_data[, theta])
show_function &amp;lt;- FALSE
show_violin &amp;lt;- TRUE

if(show_function==TRUE){
  gg1 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg2 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg3 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[1],], geom=&amp;quot;line&amp;quot;)
  gg4 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg5 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg6 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[2],], geom=&amp;quot;line&amp;quot;)
  gg7 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
  gg8 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
  gg9 &amp;lt;- qplot(x=x, y=prob_x, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[3],], geom=&amp;quot;line&amp;quot;)
}

if(show_violin==TRUE){
  gg1 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg2 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg3 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[1],], add=&amp;quot;jitter&amp;quot;)
  gg4 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg5 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg6 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[2],], add=&amp;quot;jitter&amp;quot;)
  gg7 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[1] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
  gg8 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[2] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
  gg9 &amp;lt;- ggviolin(x=&amp;quot;x&amp;quot;, y=&amp;quot;y&amp;quot;, data=plot_data[mu==mu_levels[3] &amp;amp; theta==theta_levels[3],], add=&amp;quot;jitter&amp;quot;)
}

gg_example &amp;lt;- plot_grid(gg1, gg2, gg3, gg4, gg5, gg6, gg7, gg8, gg9, 
          nrow=3,
          labels=c(paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[1]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[2]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[1], &amp;quot;; theta=&amp;quot;, theta_levels[3]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[2], &amp;quot;; theta=&amp;quot;, theta_levels[3]),
                   paste0(&amp;quot;mu=&amp;quot;, mu_levels[3], &amp;quot;; theta=&amp;quot;, theta_levels[3])),
          label_size = 10, label_x=0.1)
gg_example&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Type I error&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- melt(res_table, 
            id.vars=c(&amp;quot;beta_0&amp;quot;, &amp;quot;beta_1&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;theta&amp;quot;),
            measure.vars=c(&amp;quot;t&amp;quot;, &amp;quot;Welch&amp;quot;, &amp;quot;log&amp;quot;, &amp;quot;Wilcoxan&amp;quot;, &amp;quot;nb&amp;quot;),
            variable.name=&amp;quot;model&amp;quot;,
            value.name=&amp;quot;frequency&amp;quot;)
# res[, beta_0:=factor(beta_0)]
# res[, beta_1:=factor(beta_1)]
# res[, theta:=factor(theta)]
# res[, n:=factor(n)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=res[beta_1==0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_0 ~ theta, labeller=label_both) +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/type%20I-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ouch. glm-nb with hih error rates especially when n is small and the scale parameter is small&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Power&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0_levels &amp;lt;- unique(res$beta_0)
# small count
gg1 &amp;lt;- ggplot(data=res[beta_0==b0_levels[1] &amp;amp; beta_1 &amp;gt; 0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_1 ~ theta, labeller=label_both) +
  NULL

# large count
gg2 &amp;lt;- ggplot(data=res[beta_0==b0_levels[3] &amp;amp; beta_1 &amp;gt; 0], aes(x=n, y=frequency, group=model, color=model)) +
  geom_line() +
  facet_grid(beta_1 ~ theta, labeller=label_both) +
  NULL

gg1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/power-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-07-glm-vs-non-parametric-tests-if-all-we-care-about-is-nhst_files/figure-html/power-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;glm-nb has higher power, especially at small n, but at a type I cost.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Expected covariances in a causal network</title>
      <link>/2019/01/expected-covariances-in-a-causal-network/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/expected-covariances-in-a-causal-network/</guid>
      <description>


&lt;p&gt;This is a skeleton post&lt;/p&gt;
&lt;div id=&#34;standardized-variables-wrights-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standardized variables (Wright’s rules)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10^5

# z is the common cause of g1 and g2
z &amp;lt;- rnorm(n)

# effects of z on g1 and g2
b1 &amp;lt;- 0.7
b2 &amp;lt;- 0.7
r12 &amp;lt;- b1*b2
g1 &amp;lt;- b1*z + sqrt(1-b1^2)*rnorm(n)
g2 &amp;lt;- b2*z + sqrt(1-b2^2)*rnorm(n)
var(g1) # E(VAR(g1)) = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.997149&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(g2) # E(VAR(g2)) = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9972956&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(g1, g2) # E(COR(g1,g2)) = b1*b2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4881183&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1*b2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.49&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# effects of g1 and g2 on y1 and y2
a11 &amp;lt;- 0.5 # effect of g1 on y1
a12 &amp;lt;- 0.5 # effect of g1 on y2
a21 &amp;lt;- 0.5 # effect of g2 on y1
a22 &amp;lt;- 0.5 # effect of g2 on y2
r2.1 &amp;lt;- a11^2 + a21^2 + 2*a11*a21*b1*b2 # systematic variance of y1
r2.2 &amp;lt;- a12^2 + a22^2 + 2*a12*a22*b1*b2 # systematic variance of y1
y1 &amp;lt;- a11*g1 + a21*g2 + sqrt(1-r2.1)*rnorm(n)
y2 &amp;lt;- a12*g1 + a22*g2 + sqrt(1-r2.2)*rnorm(n)
var(y1) # E() = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9963031&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(y2) # E() = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.994697&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(y1, y2) # E() = a11*a12 + a21*a22 + a11*r12*a22 + a21*r12*a12&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7454303&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a11*a12 + a21*a22 + a11*r12*a22 + a21*r12*a12 # sum of all paths&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.745&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;non-standardized-variables-wrights-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-standardized variables (Wright’s rules)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10^6

# z is the common cause of g1 and g2
sigma_z &amp;lt;- 2
z &amp;lt;- rnorm(n, sd=sigma_z)

# effects of z on g1 and g2
b1 &amp;lt;- 1.2
b2 &amp;lt;- 1.2
s12 &amp;lt;- b1*b2*sigma_z^2 # cov(g1, g2)

sigma.sq_g1.sys &amp;lt;- (b1*sigma_z)^2 # systematic variance of g1
sigma.sq_g2.sys &amp;lt;- (b2*sigma_z)^2 # systematic variance of g1
r2.g1 &amp;lt;- 0.9 # R^2 for g1
r2.g2 &amp;lt;- 0.9 # R^2 for g2
sigma.sq_g1 &amp;lt;- sigma.sq_g1.sys/r2.g1
sigma.sq_g2 &amp;lt;- sigma.sq_g2.sys/r2.g2
sigma_g1 &amp;lt;- sqrt(sigma.sq_g1)
sigma_g2 &amp;lt;- sqrt(sigma.sq_g2)

g1 &amp;lt;- b1*z + sqrt(sigma.sq_g1 - sigma.sq_g1.sys)*rnorm(n)
g2 &amp;lt;- b2*z + sqrt(sigma.sq_g2 - sigma.sq_g2.sys)*rnorm(n)
var(g1) # E(VAR(g1)) = sigma.sq_g1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.400034&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(g2) # E(VAR(g2)) = sigma.sq_g2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.403562&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma.sq_g1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma.sq_g2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(g1,g2) # E() = b1*b2*sigma_z^2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.759874&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s12 # b1*b2*sigma_z^2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.76&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# effects of g1 and g2 on y1 and y2
a11 &amp;lt;- 1.5 # effect of g1 on y1
a12 &amp;lt;- 1.5 # effect of g1 on y2
a21 &amp;lt;- 3 # effect of g2 on y1
a22 &amp;lt;- 1.5 # effect of g2 on y2
sigma.sq_y1.sys &amp;lt;- a11^2*sigma.sq_g1 + a21^2*sigma.sq_g2 + 2*a11*a21*s12 # systematic variance of y1
sigma.sq_y2.sys &amp;lt;- a12^2*sigma.sq_g1 + a22^2*sigma.sq_g2 + 2*a12*a22*s12 # systematic variance of y2
r2.1 &amp;lt;- 0.9 # R^2 for y1
r2.2 &amp;lt;- 0.9 # R^2 for y2
sigma.sq_y1 &amp;lt;- sigma.sq_y1.sys/r2.1
sigma.sq_y2 &amp;lt;- sigma.sq_y2.sys/r2.2
sigma_y1 &amp;lt;- sqrt(sigma.sq_y1)
sigma_y2 &amp;lt;- sqrt(sigma.sq_y2)
y1 &amp;lt;- a11*g1 + a21*g2 + sqrt(sigma.sq_y1 - sigma.sq_y1.sys)*rnorm(n)
y2 &amp;lt;- a12*g1 + a22*g2 + sqrt(sigma.sq_y2 - sigma.sq_y2.sys)*rnorm(n)
var(y1) # E() = sigma.sq_y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 137.5161&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(y2) # E() = sigma.sq_y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60.87395&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma.sq_y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 137.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma.sq_y2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a11.s &amp;lt;- a11*sigma_g1/sigma_y1
a12.s &amp;lt;- a12*sigma_g1/sigma_y2
a21.s &amp;lt;- a21*sigma_g2/sigma_y1
a22.s &amp;lt;- a22*sigma_g2/sigma_y2
r12 &amp;lt;- s12/sigma_g1/sigma_g2
r12 # expected correlation between g1 and g2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(g1, g2) # actual correlation between g1 and g2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8997275&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# covariance y1 and y2
cov(y1, y2) #&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 82.10576&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(a11.s*a12.s + a21.s*a22.s + a11.s*r12*a22.s + a21.s*r12*a12.s)*sigma_y1*sigma_y2 # expected cov(y1, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 82.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# more compact and not using the standardized parameters
# = sum of paths where path coef = stand coef * parent sd
a11*a12*sigma.sq_g1 + a21*a22*sigma.sq_g2 + a11*s12*a22 + a21*s12*a12&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 82.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# correlation y1 and y2
cor(y1, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8973897&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a11.s*a12.s + a21.s*a22.s + a11.s*r12*a22.s + a21.s*r12*a12.s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8973799&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;expected-value-of-b-with-missing-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expected value of b with missing covariate&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(summary(lm(y1 ~ g1 + g2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate  Std. Error    t value   Pr(&amp;gt;|t|)
## (Intercept) 0.006165963 0.003710027   1.661972 0.09651863
## g1          1.503519031 0.003360074 447.466106 0.00000000
## g2          2.994464649 0.003359148 891.435776 0.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(summary(lm(y1 ~ g1)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate  Std. Error      t value  Pr(&amp;gt;|t|)
## (Intercept) 0.004241297 0.004970132    0.8533571 0.3934615
## g1          4.198463665 0.001964609 2137.0475423 0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# expected = direct + cor*indirect using cor
a11 + cor(g1,g2)*a21&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.199183&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# expected = direct + cor*indirect, using cov as model param
a11 + (s12/sigma_g1/sigma_g2)*a21&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# need to correct if different variances in g1 and g2
# expected is a11 + rho*sigma_g2/sigma_g1*a21
rho &amp;lt;- s12/sigma_g1/sigma_g2
a11 + rho*sigma_g2/sigma_g1*a21&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this is equal to
a11 + s12/sigma_g1^2*a21&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which is equal to a11 + b21*a21&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Compute a random data matrix (fake data) without rmvnorm</title>
      <link>/2018/12/compute-a-random-data-matrix-fake-data-without-rmvnorm/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/compute-a-random-data-matrix-fake-data-without-rmvnorm/</guid>
      <description>


&lt;p&gt;This is a skeleton post until I have time to flesh it out. The post is motivated by a question on twitter about creating fake data that has a covariance matrix that simulates a known (given) covariance matrix that has one or more negative (or zero) eigenvalues.&lt;/p&gt;
&lt;p&gt;First, some libraries&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(mvtnorm)
library(MASS)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, some functions…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random.sign &amp;lt;- function(u){
  # this is fastest of three
    out &amp;lt;- sign(runif(u)-0.5)
    #randomly draws from {-1,1} with probability of each = 0.5
    return(out)
}

fake.eigenvectors &amp;lt;- function(p){
    a &amp;lt;- matrix(rnorm(p*p), p, p) # only orthogonal if p is infinity so need to orthogonalize it
    a &amp;lt;- t(a)%*%a # this is a pseudo-covariance matrix
    E &amp;lt;- eigen(a)$vectors # decompose to truly orthogonal columns
    return(E)
}

fake.eigenvalues &amp;lt;- function(p, m=p, start=2/3, rate=2){
  # m is the number of positive eigenvalues
  # start and rate control the decline in the eigenvalue
  s &amp;lt;- start/seq(1:m)^rate
  s &amp;lt;- c(s, rep(0, p-m)) # add zero eigenvalues
  L &amp;lt;- diag(s/sum(s)*m) # rescale so that sum(s)=m and put into matrix,
  # which would occur if all the traits are variance standardized
  return(L)
}

fake.cov.matrix &amp;lt;- function(p){
    # p is the size of the matrix (number of cols and rows)
    E &amp;lt;- fake.eigenvectors(p)
    L &amp;lt;- diag(fake.eigenvalues(p))
    S &amp;lt;- E%*%L%*%t(E)
    return(S)
}

# two functions to compute the random data
fake.X &amp;lt;- function(n,p,E,L){
  # n is number of observations
  # p is number of variables
  X &amp;lt;- matrix(rnorm(n*p),nrow=n,ncol=p) %*% t(E%*%sqrt(L))
    return(X)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, some fake data. Since, I’m not starting with a known covariance matrix, I have to create one. This isn’t necessary if you already have data. I start with fake data that does have a full-rank covariance matrix, and then create fake data that has a single zero eigenvalue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2)
n &amp;lt;- 10^4 # number of cases (rows of the data)
p &amp;lt;- 5 # number of variables (columns of the data)

# start with a matrix. In real life this would be our data
X &amp;lt;- fake.X(n, p, fake.eigenvectors(p), fake.eigenvalues(p))
# and here is our &amp;quot;real&amp;quot; covariance matrix
S &amp;lt;- cov(X)

# okay now we want to create fake data sets with this structure
decomp &amp;lt;- eigen(S)
E &amp;lt;- decomp$vectors
L &amp;lt;- diag(decomp$values)
fake_data &amp;lt;- fake.X(n, p, E, L)

# okay what if our real covariance matrix is not positive definite, that is has zero (negative) eigenvalues.
# Here is our fake data and cov matrix
k &amp;lt;- 1 # number of eigenvalues to delete
X &amp;lt;- fake.X(n, p, fake.eigenvectors(p), fake.eigenvalues(p, m=(p-k)))
# and here is our &amp;quot;real&amp;quot; covariance matrix
S &amp;lt;- cov(X)
decomp &amp;lt;- eigen(S)
E &amp;lt;- decomp$vectors
L &amp;lt;- diag(decomp$values) # note last eigenvalue is negative

# can we simulate with rmvnorm? mvrnorm?
fake_data_rmvnorm &amp;lt;- rmvnorm(n, mean=rep(0, p), sigma=S)
fake_data_mvrnorm &amp;lt;- mvrnorm(n, mu=rep(0, p), Sigma=S)

# now let&amp;#39;s sample fake data from this non-pos matrix
# set m to p-1
m &amp;lt;- p-k
E_reduced &amp;lt;- E[1:p, 1:m] # the first m columns of E
L_reduced &amp;lt;- L[1:m, 1:m] # the first m diag elements of L
fake_data &amp;lt;- fake.X(n, m, E_reduced, L_reduced)
S # compare...pretty good!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]        [,2]        [,3]       [,4]       [,5]
## [1,]  0.4617029 -0.21686839  0.27537359 -0.4226386 -0.4834831
## [2,] -0.2168684  0.33629531 -0.03734511  0.3562057  0.2632369
## [3,]  0.2753736 -0.03734511  1.08725135 -0.5114116 -0.6554844
## [4,] -0.4226386  0.35620567 -0.51141163  0.7362452  0.8899933
## [5,] -0.4834831  0.26323687 -0.65548437  0.8899933  1.3156042&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(fake_data_rmvnorm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]        [,2]        [,3]       [,4]       [,5]
## [1,]  0.4576706 -0.21395310  0.27322299 -0.4155847 -0.4731524
## [2,] -0.2139531  0.33445024 -0.02847549  0.3487285  0.2528381
## [3,]  0.2732230 -0.02847549  1.07250120 -0.4958794 -0.6397814
## [4,] -0.4155847  0.34872855 -0.49587939  0.7217563  0.8751866
## [5,] -0.4731524  0.25283813 -0.63978139  0.8751866  1.3047852&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(fake_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  0.4596107 -0.2148138  0.2670967 -0.4164482 -0.4760025
## [2,] -0.2148138  0.3366904 -0.0379339  0.3556336  0.2612796
## [3,]  0.2670967 -0.0379339  1.0760712 -0.5059885 -0.6473009
## [4,] -0.4164482  0.3556336 -0.5059885  0.7306168  0.8807419
## [5,] -0.4760025  0.2612796 -0.6473009  0.8807419  1.3014562&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The top matrix is the observed covariance matrix. The middle matrix is using rmvnorm. The bottom matrix is from my own code to generate fake data that is modeled to simulate the real data (although here, even the real data is fake).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reporting effects as relative differences...with a confidence interval</title>
      <link>/2018/11/reporting-effects-as-relative-differences-with-a-confidence-interval/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/reporting-effects-as-relative-differences-with-a-confidence-interval/</guid>
      <description>


&lt;p&gt;Researchers frequently report results as relative effects, for example,&lt;/p&gt;
&lt;p&gt;“Male flies from selected lines had 50% larger upwind flight ability than male flies from control lines (Control mean: 117.5 cm/s; Selected mean 176.5 cm/s).”&lt;/p&gt;
&lt;p&gt;where a relative effect is&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
100 \frac{\bar{y}_B - \bar{y}_A}{\bar{y}_A}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;If we are to follow best practices, we should present this effect with a measure of uncertainty, such as a confidence interval. The absolute effect is 59.0 cm/s and the 95% CI of this effect is (48.7, 69.3 cm/s). But if we present the result as a relative effect, or percent difference from some reference value, how do we compute a “relative CI”?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/notebooks/relative_standard_errors.nb.html&#34;&gt;Read the whole post here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interaction plots with ggplot2</title>
      <link>/2018/10/interaction-plots-with-ggplot2/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/interaction-plots-with-ggplot2/</guid>
      <description>


&lt;p&gt;ggpubr is a fantastic resource for teaching applied biostats because it makes ggplot a bit easier for students. I’m not super familiar with all that ggpubr can do, but I’m not sure it includes a good “interaction plot” function. Maybe I’m wrong. But if I’m not, here is a simple function to create a gg_interaction plot.&lt;/p&gt;
&lt;p&gt;The gg_interaction function returns a ggplot of the &lt;em&gt;modeled&lt;/em&gt; means and standard errors and not the raw means and standard errors computed from each group independently. The modeled means and errors are computed using the emmeans function from the emmeans package. If a random term is passed, gg_interaction uses the function lmer, from the package lme4, to fit a linear mixed model with the random term as a random intercept.&lt;/p&gt;
&lt;p&gt;(requires ggplot2, data.table, and emmeans)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg_interaction &amp;lt;- function(x, y, random=NULL, data){
  # x is a vector of the column labels of categorical variables
  # y is the response column
  # random is a column name of a blocking factor
  # data is a data.frame or data.table
  dt &amp;lt;- data.table(data)
  fixed_part &amp;lt;- paste(y, &amp;quot;~&amp;quot;, paste(x[1], x[2], sep=&amp;quot;*&amp;quot;))
  if(is.null(random)){ # linear model
    lm_formula &amp;lt;- formula(fixed_part)
    fit &amp;lt;- lm(lm_formula, data=dt)
  }else{ ## linear mixed model
    random_part &amp;lt;- paste(&amp;quot;(1|&amp;quot;, random, &amp;quot;)&amp;quot;, sep=&amp;quot;&amp;quot;)
    lmm_formula &amp;lt;- formula(paste(fixed_part, random_part, sep=&amp;quot; + &amp;quot;))
    fit &amp;lt;- lmer(lmm_formula, data=dt)
  }
  fit.emm &amp;lt;- data.table(summary(emmeans(fit, specs=x)))
  new_names &amp;lt;- c(&amp;quot;f1&amp;quot;, &amp;quot;f2&amp;quot;)
  setnames(fit.emm, old=x, new=new_names)
  pd &amp;lt;- position_dodge(.3)
  gg &amp;lt;- ggplot(data=fit.emm, aes(x=f1, y=emmean, shape=f2, group=f2)) +
    #geom_jitter(position=pd, color=&amp;#39;gray&amp;#39;, size=2) +
    geom_point(color=&amp;#39;black&amp;#39;, size=4, position=pd) +
    geom_errorbar(aes(ymin=(emmean-SE), ymax=(emmean+SE)), 
                  color=&amp;#39;black&amp;#39;, width=.2, position=pd) +
    geom_line(position=pd) +
    xlab(x[1]) +
    ylab(y) +
    theme_bw() +
    guides(shape=guide_legend(title=x[2])) +
    theme(axis.title=element_text(size = rel(1.5)),
          axis.text=element_text(size = rel(1.5)),
          legend.title=element_text(size = rel(1.3)),
          legend.text=element_text(size = rel(1.3))) +
    NULL
  return(gg)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use data from a study of the synergistic effect of UVB and temperature on infection intensity (citations below) to show how to use the function. The data are from a 2 x 2 factorial experiment and with a single blocking (random) factor “tank”.&lt;/p&gt;
&lt;p&gt;Dryad source: Cramp RL, Reid S, Seebacher F, Franklin CE (2014) Data from: Synergistic interaction between UVB radiation and temperature increases susceptibility to parasitic infection in a fish. Dryad Digital Repository. &lt;a href=&#34;https://doi.org/10.5061/dryad.74b31&#34; class=&#34;uri&#34;&gt;https://doi.org/10.5061/dryad.74b31&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Article Source: Cramp RL, Reid S, Seebacher F, Franklin CE (2014) Synergistic interaction between UVB radiation and temperature increases susceptibility to parasitic infection in a fish. Biology Letters 10(9): 20140449. &lt;a href=&#34;https://doi.org/10.1098/rsbl.2014.0449&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsbl.2014.0449&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(readxl)
library(data.table)
library(lme4)
library(emmeans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;../data&amp;quot;
folder &amp;lt;- &amp;quot;Data from Synergistic interaction between UVB radiation and temperature increases susceptibility to parasitic infection in a fish&amp;quot;
filename &amp;lt;- &amp;quot;Cramp et al raw data.xlsx&amp;quot;
file_path &amp;lt;- paste(data_path, folder, filename, sep=&amp;quot;/&amp;quot;)
fish &amp;lt;- data.table(read_excel(file_path, sheet=&amp;quot;Infection Intensity&amp;quot;))
setnames(fish, old=colnames(fish), new=c(&amp;quot;UV&amp;quot;, &amp;quot;Temp&amp;quot;, &amp;quot;Tank&amp;quot;, &amp;quot;Whitespots&amp;quot;))
fish[, UV:=factor(UV, c(&amp;quot;Low&amp;quot;, &amp;quot;High&amp;quot;))]
fish[, Temp:=factor(Temp)]
gg &amp;lt;- gg_interaction(x=c(&amp;quot;UV&amp;quot;, &amp;quot;Temp&amp;quot;), y=&amp;quot;Whitespots&amp;quot;, random=&amp;quot;Tank&amp;quot;, data=fish)
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-15-interaction-plots-with-ggplot2_files/figure-html/gg_interaction-example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Textbook error 101 -- A low p-value for the full model does not mean that the model is a good predictor of the response</title>
      <link>/2018/09/a-low-p-value-for-the-full-model-does-not-mean-that-the-model-is-a-good-predictor-of-the-response/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/a-low-p-value-for-the-full-model-does-not-mean-that-the-model-is-a-good-predictor-of-the-response/</guid>
      <description>


&lt;p&gt;On page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point average.” Ah no.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(mvtnorm)
rho &amp;lt;- 0.5
n &amp;lt;- 10^5
Sigma &amp;lt;- diag(2)
Sigma[1,2] &amp;lt;- Sigma[2,1] &amp;lt;- rho
X &amp;lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma)
colnames(X) &amp;lt;- c(&amp;quot;X1&amp;quot;, &amp;quot;X2&amp;quot;)
beta &amp;lt;- c(0.01, -0.02)
y &amp;lt;- X%*%beta + rnorm(n)
fit &amp;lt;- lm(y ~ X)
summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2513 -0.6704 -0.0026  0.6701  4.1725 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.003597   0.003158  -1.139    0.255    
## XX1          0.005449   0.003629   1.502    0.133    
## XX2         -0.016562   0.003641  -4.548 5.41e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9987 on 99997 degrees of freedom
## Multiple R-squared:  0.0002146,  Adjusted R-squared:  0.0001946 
## F-statistic: 10.73 on 2 and 99997 DF,  p-value: 2.184e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A p-value is not a measure of the predictive capacity of a model because the p-value is a function of 1) signal, 2) noise (unmodeled error), and 3) sample size while predictive capacity is a function of the signal:noise ratio. If the signal:noise ratio is tiny, the predictive capacity is small but the p-value can be tiny if the sample size is large.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A simple ggplot of some measure against depth</title>
      <link>/2018/09/a-simple-ggplot-of-some-measure-against-depth/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/a-simple-ggplot-of-some-measure-against-depth/</guid>
      <description>


&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;set up&lt;/h1&gt;
&lt;p&gt;The goal is to plot the measure of something, say O2 levels, against depth (soil or lake), with the measures taken on multiple days&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;first-create-fake-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First – create fake data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;depths &amp;lt;- c(0, seq(10,100, by=10))
dates &amp;lt;- c(&amp;quot;Jan-18&amp;quot;, &amp;quot;Mar-18&amp;quot;, &amp;quot;May-18&amp;quot;, &amp;quot;Jul-18&amp;quot;)
x &amp;lt;- expand.grid(date=dates, depth=depths)
n &amp;lt;- nrow(x)
head(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     date depth
## 1 Jan-18     0
## 2 Mar-18     0
## 3 May-18     0
## 4 Jul-18     0
## 5 Jan-18    10
## 6 Mar-18    10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(formula(~date + depth), data=x)
head(X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) dateMar-18 dateMay-18 dateJul-18 depth
## 1           1          0          0          0     0
## 2           1          1          0          0     0
## 3           1          0          1          0     0
## 4           1          0          0          1     0
## 5           1          0          0          0    10
## 6           1          1          0          0    10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta &amp;lt;- c(0,1,2,3,0.5)
y &amp;lt;- X%*%beta + rnorm(n, sd=5)
fake_data &amp;lt;- data.frame(O2=y, x)
head(fake_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          O2   date depth
## 1 -5.618861 Jan-18     0
## 2 13.885828 Mar-18     0
## 3  1.309404 May-18     0
## 4  4.442890 Jul-18     0
## 5  6.504647 Jan-18    10
## 6  5.378927 Mar-18    10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;second-make-ggplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Second – make ggplot&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(data=fake_data, aes(x=depth, y=O2, group=date, color=date)) +
  geom_point() +
  geom_line() +
  coord_flip() +
  # coord_flip does not make the y-axis the x-axis but the horizontal axis. 
  # so still need to refer to &amp;quot;y&amp;quot; to modify O2
  # specify position=&amp;quot;right&amp;quot; to flip the y axis to the top after coord_flip
  scale_y_continuous(position = &amp;quot;right&amp;quot;) +
  # reverse the depth axis, which makes it go down instead of up
  scale_x_reverse() +
  xlab(&amp;quot;Depth&amp;quot;) +
  ylab(expression(O[2])) +
  theme_minimal() +
  NULL
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-10-a-simple-ggplot-of-some-measure-against-depth_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Should the model-averaged prediction be computed on the link or response scale in a GLM?</title>
      <link>/2018/05/should-the-model-averaged-prediction-be-computed-on-the-link-or-response-scale-in-a-glm/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/should-the-model-averaged-prediction-be-computed-on-the-link-or-response-scale-in-a-glm/</guid>
      <description>


&lt;p&gt;[updated to include additional output from MuMIn, BMA, and BAS]&lt;/p&gt;
&lt;p&gt;This post is a follow up &lt;a href=&#34;/2018/05/model-averaged-coefficients-of-a-glm&#34;&gt;to my inital post&lt;/a&gt;, which was written as as a way for me to pen my mental thoughts on the recent review of &lt;a href=&#34;https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1309&#34; target=&#34;_blank&#34;&gt;“Model averaging in ecology: a review of Bayesian, information‐theoretic and tactical approaches for predictive inference”&lt;/a&gt;. It was also written without contacting and discussing the issue with the authors. This post benefits from a series of e-mails with the lead author Carsten Dormann and the last author Florian Hartig.&lt;/p&gt;
&lt;p&gt;The Dormann et al. paper focuses on model-averaged predictions, but has a short discussion on problems with model-averaged coefficients in the supplement. It is in the supplement, that the authors state that for generalized linear models (GLMs) “coefficient averaging is not equivalent to prediction averaging”. Brian Cade&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; also makes this argument.&lt;/p&gt;
&lt;p&gt;In my previous post, I argued that this statement is wrong – predictions from a parameter-averaged model is mathematically identical to averaging predictions. It was hard for me to understand how this was not immediately obvious until my e-mail exchange with Carsten and Florian. In short, I assume that all averaging is done on the link scale and then the predictions are back-transformed to the response scale. Carsten and Florian (and Brian Cade?) argue that this is the “wrong” way to compute averaged predictions.&lt;/p&gt;
&lt;p&gt;The summary of our differences is&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Carsten and Florian argue that predictions should be computed on the link scale, back-transformed to the response scale, and then averaged on the response scale. I understand their reason to be that first, this is how everyone outside of ecology does it, and second, predictions have to be averaged on the response scale for non-linear models because there is no link scale, and, since, GLMs are non-linear models, they should be averaged on the response scale.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I argue that because a GLM model is a function of a linear predictor, any subsequent averaging should be on the linear scale, so that additive relationships remain additive and not multiplicative. Averaging on the link (linear) scale maintains consistency with the meaning of the fit parameters.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;[update] Perhaps &lt;em&gt;the&lt;/em&gt; reason for averaging on the response scale, at least in a Bayesian framework, is eq. 1 of Hoeting et al. 1999, which is&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{pr}(\Delta | D) = \sum_{k=1}^K \textrm{pr}(\Delta | M_k, D) \textrm{pr}(M_k | D)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; is a prediction. For a GLM, these &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; are on the response scale because the models are &lt;span class=&#34;math inline&#34;&gt;\(\textrm{E}(Y) = \mu = g^{-1}(X\beta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Some final thoughts using equation S2 from Dormann et al. 2018&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\frac{1}{m} \sum_{i=1}^m{g^{-1}(Xb_i)} \ne g^{-1}(X \frac{\sum_{i=1}^m{b_i}}{m})
\end{equation}\]&lt;/span&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Dormann et al. advocate averaging using the LHS of eq. S2, I advocate using the RHS.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If using the RHS of S2 to average predictors, &lt;a href=&#34;/2018/05/model-averaged-coefficients-of-a-glm&#34;&gt;then averaging the predictors or computing the predictor from averaged coefficients are mathematically equivalent&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GLMs are unlike non-linear models in that non-linear models do not have link functions although some can be linearized. There is no linear model that is fit. Consquently, for non-linear models, averging the predictors of non-linear models on the “response scale” is consistent with the fit model. So in my opinion, this isn’t good justification for averaging GLMs on the response scale.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because Carsten and Florian (and presumably the other co-authors of Dormann et al.) argue that predictions should be model averaged on the response scale, this raises questions about the default output for MuMIn and BAS which default to predictions on the link scale that were averaged on the link scale (both do produce predictions averaged on the response scale with type=‘response’). This raises one question and one concern. What “good” is a prediction averaged on the link scale – the default output of MuMIn and BAS? If we want a ma-prediction on the link scale, we can get this two ways: log transform the ma-predictions that were averaged on the response scale or simply average on the link scale. The second way does not produce predictions that are direct transformations with those on the response scale, so which is the correct way?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The concern raised above. The default prediction (if “type=” is not specified) for the MuMIn or BAS package is the prediction averaged on the link scale. Probably most researchers would want to interpret predictions on the response scale and the team that publishes this may achieve this by, perhaps naively, simply back-transforming the default predictions. This is the RHS of eq. S2. Someone comes along to reproduce the results, takes the same data, and specifies type=’response’. They get slightly different results, because they’ve used the LHS of eq. S1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I think this argument from Russell Lenth, the author of the amazingly useful emmeans (formerly lsmeans) package, supports my argument for averaging on the link scale. Here, Lenth comments on why &lt;a href=&#34;https://cran.r-project.org/web/packages/emmeans/vignettes/transformations.html&#34;&gt;the package computes marginal means on the link and not response scale for GLMs&lt;/a&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The model is our best guide&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This choice of timing is based on the idea that the model is right. In particular, the fact that the response is transformed suggests that the transformed scale is the best scale to be working with. In addition, the model specifies that the effects of source and percent are linear on the transformed scale; inasmuch as marginal averaging to obtain EMMs is a linear operation, that averaging is best done on the transformed scale. For those two good reasons, back-transforming to the response scale is delayed until the very end by default.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The difference in predicted values computed on the link vs. response scale is trivially small for the example below, and perhaps in most real examples, and if this is the case, our argument doesn’t really matter. There are much bigger sources of error in modeling than this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, in response to an exchange with Florian, I wanted to make sure that my understanding of MuMIn is correct (it seems to be), so below are five ways to compute a simple count (poisson) example using fake data in addition to the computation from MuMIn.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Like MuMIn, The BAS package defaults to averaging on the link scale without back-transformation to the response scale – see the code at the bottom. Unlike MuMIn, BAS outputs the predictions on the response scale averaged on the response scale – these are not equal to the back-transformation of the default predictions on the link scale. I couldn’t find this documented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also explore the BMA and AICcmodelavg packages.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;how-does-mumin-model-average-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does MuMIn model average predictions?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(MuMIn)
library(BMA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: leaps&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: robustbase&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;robustbase&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:survival&amp;#39;:
## 
##     heart&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: inline&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: rrcov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scalable Robust Estimators with High Breakdown Point (version 1.4-3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BAS)
library(AICcmodavg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;AICcmodavg&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:MuMIn&amp;#39;:
## 
##     AICc, DIC, importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple model of counts&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  set.seed(1)
  n &amp;lt;- 100
  exp_beta_0 &amp;lt;- 175 # mean Y on response scale
  exp_beta_1 &amp;lt;- 0.99 # effect on response scale
  exp_beta_2 &amp;lt;- 0.99 # effect on response sacle
  
  # create correlated x1 and x2 due to common factor z
  z &amp;lt;- rnorm(n) # common factor to correlate X1 and X2
  r &amp;lt;- 0.6 # correlation between x1 and x2
  alpha &amp;lt;- sqrt(r) # &amp;quot;effect&amp;quot; of Z on X1 and X2
  x1 &amp;lt;- alpha*z + sqrt(1-r)*rnorm(n)
  x2 &amp;lt;- alpha*z + sqrt(1-r)*rnorm(n)

  # expected count in link space
  E_log_count &amp;lt;- log(exp_beta_0) + log(exp_beta_1)*x1 + log(exp_beta_2)*x2 # expected log count
  # observed counts
  count &amp;lt;- rpois(n=n, lambda=exp(E_log_count))

  # create data.table and fit 
  dt &amp;lt;- data.table(count=count, x1=x1, x2=x2)
  fit &amp;lt;- glm(count ~ x1 + x2, family=poisson(link = &amp;quot;log&amp;quot;), data=dt,na.action=na.fail )
  X &amp;lt;- model.matrix(fit)
  
  # all model regression using MuMIn
  fit.mm &amp;lt;- dredge(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Fixed term is &amp;quot;(Intercept)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  model_set &amp;lt;- get.models(fit.mm, subset=TRUE) # all models
  fit.avg &amp;lt;- model.avg(model_set) # coeffcients are on link scale
  fit.avg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## model.avg(object = model_set)
## 
## Component models: 
## &amp;#39;2&amp;#39;      &amp;#39;12&amp;#39;     &amp;#39;1&amp;#39;      &amp;#39;(Null)&amp;#39;
## 
## Coefficients: 
##        (Intercept)          x2           x1
## full      5.168167 -0.02107276 -0.002326521
## subset    5.168167 -0.02288317 -0.007265122&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # (0) MuMIn predict
  yhat.default &amp;lt;- predict(fit.avg)
  yhat.MuMIn1 &amp;lt;- predict(fit.avg, type=&amp;#39;response&amp;#39;)
  yhat.MuMIn2 &amp;lt;- exp(predict(fit.avg, type=&amp;#39;link&amp;#39;))
  yhat.MuMIn3 &amp;lt;- predict(fit.avg, backtransform=TRUE)
  yhat.MuMIn4 &amp;lt;- exp(predict(fit.avg, backtransform=FALSE))
  head(data.table(default=exp(yhat.default),
                  MuMIn1=yhat.MuMIn1, 
                  MuMIn2=yhat.MuMIn2,
                  MuMIn3=yhat.MuMIn3,
                  MuMIn4=yhat.MuMIn4
                  ))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     default   MuMIn1   MuMIn2   MuMIn3   MuMIn4
## 1: 176.7927 176.7934 176.7927 176.7927 176.7927
## 2: 171.1034 171.1072 171.1034 171.1034 171.1034
## 3: 174.7764 174.7805 174.7764 174.7764 174.7764
## 4: 171.3024 171.3036 171.3024 171.3024 171.3024
## 5: 180.1184 180.1233 180.1184 180.1184 180.1184
## 6: 171.9407 171.9422 171.9407 171.9407 171.9407&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  #is this averaged on link or response scale? And is it the coefficients or the prediction that is averaged?
  
  # (1) average coefficients on link scale. compute prediction on link scale. transform predictions to response scale
  b &amp;lt;- fit.avg$coefficients[&amp;#39;full&amp;#39;,][colnames(X)]
  yhat1 &amp;lt;- exp((X%*%b)[,1]) #
  b_ma_link &amp;lt;- b

  # (2) compute predictions for each model on link scale. Average on link scale. Backtransform to response scale
  yhat2a &amp;lt;- exp(predict(fit.avg, backtransform=FALSE))
  w &amp;lt;- fit.mm$weight
  yhat2b.each_model.link_scale &amp;lt;- sapply(model_set, predict)
  yhat2b.link_scale &amp;lt;- (yhat2b.each_model.link_scale%*%w)[,1]
  yhat2b &amp;lt;- exp(yhat2b.link_scale)
  
  # (3) compute predictions for each model on link scale. Backtransform to response scale. Average on response scale. This is method of Dormann et al.
  yhat3.each_model.response_scale &amp;lt;- exp(yhat2b.each_model.link_scale)
  yhat3 &amp;lt;- (yhat3.each_model.response_scale%*%w)[,1]
  
  # (4) backtransform coefficients to response scale. Average coefficients on response scale. Compute prediction on response scale.
  B &amp;lt;- exp(fit.mm[,colnames(X)])
  B[is.na(B)] &amp;lt;- 0.0
  b_ma &amp;lt;- t(B)%*%w
  yhat4 &amp;lt;- (X%*%b_ma)[,1] #

  # (5) average coefficients on link scale. backtransform to response scale. compute prediction on response scale
  b &amp;lt;- exp(fit.avg$coefficients[&amp;#39;full&amp;#39;,][colnames(X)])
  yhat5 &amp;lt;- (X%*%b)[,1] #&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;The first few rows of the results matrix of the predictions using five different methods for their computation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       MuMIn MuMIn.rs    yhat1    yhat2    yhat3    yhat4    yhat5
## 1: 176.7927 176.7934 176.7927 176.7927 176.7934 175.1100 174.4955
## 2: 171.1034 171.1072 171.1034 171.1034 171.1072 176.7358 176.9463
## 3: 174.7764 174.7805 174.7764 174.7764 174.7805 175.5243 174.7209
## 4: 171.3024 171.3036 171.3024 171.3024 171.3036 176.9411 177.9302
## 5: 180.1184 180.1233 180.1184 180.1184 180.1233 174.4711 174.2690
## 6: 171.9407 171.9422 171.9407 171.9407 171.9422 176.5958 176.9982&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Column Keys&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;MuMIn = MuMIn’s default prediction backtransformed to response scale&lt;/p&gt;
&lt;p&gt;MuMIn.rs = MuMIn with type = ‘response’&lt;/p&gt;
&lt;p&gt;yhat1 = Coefficients averaged on link scale. Predictions computed on link scale from averaged coefficients and then backtransformed to response scale. My preferred method. RHS of eq. S2.&lt;/p&gt;
&lt;p&gt;yhat2 = Predictions computed on link scale for each model and then averaged on link scale and then backtransformed to response scale. Alternative to my preferred method.&lt;/p&gt;
&lt;p&gt;yhat3 = Predictions computed on link scale for each model and then backtransformed to response scale and then averaged on response scale. This is the method of Dormann et al. 2018 and Cade 2015&lt;/p&gt;
&lt;p&gt;yhat4 = Coefficients backtransformed to response scale and then averaged on response scale. Predictions computed on response scale from averaged coefficients.&lt;/p&gt;
&lt;p&gt;yhat5 = Coefficients averaged on link scale and then backtransformed to response scale, which are used to compute averaged predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-bic.glm-model-average-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does bic.glm model average predictions?&lt;/h2&gt;
&lt;p&gt;This simulation uses the fake data generated above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.bma &amp;lt;- bic.glm(count ~ x1 + x2, glm.family=poisson(link = &amp;quot;log&amp;quot;), data=dt)
# (0) BMA predict
yhat.default &amp;lt;- predict(fit.bma, newdata=dt) # these are on the response scale

# (1) use model averaged B to get prediction on link scale. Backtransform to response scale (eq. s2 RHS of appendix)
b = fit.bma$postmean
Xb &amp;lt;- X%*%b
yhat1 &amp;lt;- exp(Xb) # averaged predictions backtransformed to response

res &amp;lt;- data.table(default=yhat.default, yhat1=yhat1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-the-bas-package-model-average-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does the BAS package model-average predictions?&lt;/h2&gt;
&lt;p&gt;This simulation uses the fake data generated above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # fit using BMA
  packageVersion(&amp;quot;BAS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;#39;1.5.0&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # fit &amp;lt;- glm(count ~ x1 + x2, family=poisson(link = &amp;quot;log&amp;quot;), data=dt,na.action=na.fail )
  fit.bma &amp;lt;- bas.glm(count ~ x1 + x2, family=poisson(link = &amp;quot;log&amp;quot;), data=dt)
 
  res.bma.rs &amp;lt;- predict(fit.bma, type=&amp;#39;response&amp;#39;)
  res.bma0.rs &amp;lt;- res.bma.rs$fit[,1]
  res.bma1.rs.ls &amp;lt;- res.bma.rs$Ybma[,1]
  res.bma1.rs.exp.ls &amp;lt;- exp(res.bma1.rs.ls)
  
  res.bma.ls &amp;lt;- predict(fit.bma)
  res.bma0.ls &amp;lt;- res.bma.ls$fit[,1]
  res.bma1.ls.ls &amp;lt;- res.bma.ls$Ybma[,1]
  res.bma1.ls.exp.ls &amp;lt;- exp(res.bma1.ls.ls)
  
  
  bas.res1 &amp;lt;- data.table(
    default.fit=res.bma0.ls,
    default.Ybma=res.bma1.ls.ls,
    default.exp.Ybma=res.bma1.ls.exp.ls,
    response.fit=res.bma0.rs,
    response.Ybma=res.bma1.rs.ls,
    response.exp.Ybma=res.bma1.rs.exp.ls
  )
  
  res.bma &amp;lt;- predict(fit.bma)
  yhat.bma.response &amp;lt;- res.bma$fit[,1]
  yhat.bma.link &amp;lt;- res.bma$Ybma[,1]
  YHAT &amp;lt;- t(res.bma$Ypred)
  w &amp;lt;- res.bma$postprobs
  # averaged on response scale
  yhat1.bma.response &amp;lt;- (exp(YHAT)%*%w)[,1]
  # averaged on link scale and then backtransformed
  yhat2.bma.link &amp;lt;- (YHAT%*%w)[,1]
  yhat2.bma.response &amp;lt;- exp(yhat2.bma.link)

  bas.res2 &amp;lt;- data.table(yhat0=yhat.bma.response,
             yhat1=yhat1.bma.response,
             yhat2=yhat2.bma.response)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;result-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Result 1&lt;/h3&gt;
&lt;p&gt;predict.basglm seems to have the same output regardless of the type=‘response’ specification. The prediction on the link scale is in “Ybma” and the prediction on the response scale is in “fit”. Note that &lt;span class=&#34;math inline&#34;&gt;\(exp(Ybma) \ne fit\)&lt;/span&gt;. s&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(bas.res1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    default.fit default.Ybma default.exp.Ybma response.fit response.Ybma
## 1:    176.5610     5.173649         176.5580     176.5610      5.173649
## 2:    173.0640     5.153591         173.0518     173.0640      5.153591
## 3:    175.7336     5.168932         175.7271     175.7336      5.168932
## 4:    172.5097     5.150390         172.4988     172.5097      5.150390
## 5:    177.8421     5.180803         177.8256     177.8421      5.180803
## 6:    173.3129     5.155058         173.3059     173.3129      5.155058
##    response.exp.Ybma
## 1:          176.5580
## 2:          173.0518
## 3:          175.7271
## 4:          172.4988
## 5:          177.8256
## 6:          173.3059&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;result-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Result 2&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(fit\)&lt;/span&gt; is indeed the predictions averaged on the response scale and not &lt;span class=&#34;math inline&#34;&gt;\(exp(Ybma)\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(bas.res2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       yhat0    yhat1    yhat2
## 1: 176.5610 176.5610 176.5580
## 2: 173.0640 173.0640 173.0518
## 3: 175.7336 175.7336 175.7271
## 4: 172.5097 172.5097 172.4988
## 5: 177.8421 177.8421 177.8256
## 6: 173.3129 173.3129 173.3059&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Key:&lt;/p&gt;
&lt;p&gt;yhat0 - “fit”, which is the prediction on the response scale&lt;/p&gt;
&lt;p&gt;yhat1 - my manual computation of the average on the response scale&lt;/p&gt;
&lt;p&gt;yhat2 - my manual computation of the average on the link scale and then back-transformed to the response scale.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-the-aiccmodavg-package-model-average-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does the AICcmodavg package model-average predictions?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(min.trap)
##assign &amp;quot;UPLAND&amp;quot; as the reference level as in Mazerolle (2006)
min.trap$Type &amp;lt;- relevel(min.trap$Type, ref = &amp;quot;UPLAND&amp;quot;)
##set up candidate models
Cand.mod &amp;lt;- list()
Cand.mod[[1]] &amp;lt;- glm(Num_anura ~ Type + log.Perimeter + Num_ranatra,
                     family = poisson, offset = log(Effort),
                     data = min.trap)
Cand.mod[[2]] &amp;lt;- glm(Num_anura ~ Type + log.Perimeter, family = poisson,
                     offset = log(Effort), data = min.trap)
Cand.mod[[3]] &amp;lt;- glm(Num_anura ~ Type + Num_ranatra, family = poisson,
                     offset = log(Effort), data = min.trap)
Cand.mod[[4]] &amp;lt;- glm(Num_anura ~ Type, family = poisson,
                     offset = log(Effort), data = min.trap)
Cand.mod[[5]] &amp;lt;- glm(Num_anura ~ log.Perimeter + Num_ranatra,
                     family = poisson, offset = log(Effort),
                     data = min.trap)

Cand.mod[[6]] &amp;lt;- glm(Num_anura ~ log.Perimeter, family = poisson,
                     offset = log(Effort), data = min.trap)
Cand.mod[[7]] &amp;lt;- glm(Num_anura ~ Num_ranatra, family = poisson,
                     offset = log(Effort), data = min.trap)
Cand.mod[[8]] &amp;lt;- glm(Num_anura ~ 1, family = poisson,
                     offset = log(Effort), data = min.trap)
##check c-hat for global model
c_hat(Cand.mod[[1]], method = &amp;quot;pearson&amp;quot;) #uses Pearson&amp;#39;s chi-square/df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;c-hat&amp;#39; 1.04 (method: pearson estimator)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##note the very low overdispersion: in this case, the analysis could be
##conducted without correcting for c-hat as its value is reasonably close
##to 1
##assign names to each model
Modnames &amp;lt;- c(&amp;quot;type + logperim + invertpred&amp;quot;, &amp;quot;type + logperim&amp;quot;,
              &amp;quot;type + invertpred&amp;quot;, &amp;quot;type&amp;quot;, &amp;quot;logperim + invertpred&amp;quot;,
              &amp;quot;logperim&amp;quot;, &amp;quot;invertpred&amp;quot;, &amp;quot;intercept only&amp;quot;)
##model selection table based on AICc
aictab(cand.set = Cand.mod, modnames = Modnames)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model selection based on AICc:
## 
##                              K  AICc Delta_AICc AICcWt Cum.Wt     LL
## type + invertpred            3 54.03       0.00   0.60   0.60 -23.42
## type + logperim + invertpred 4 56.57       2.54   0.17   0.77 -23.23
## logperim + invertpred        3 57.91       3.88   0.09   0.86 -25.35
## invertpred                   2 58.63       4.60   0.06   0.92 -27.03
## type + logperim              3 59.38       5.35   0.04   0.96 -26.09
## type                         2 59.74       5.71   0.03   1.00 -27.58
## intercept only               1 65.47      11.44   0.00   1.00 -31.65
## logperim                     2 67.27      13.24   0.00   1.00 -31.35&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat.pred &amp;lt;- data.frame(Type = factor(c(&amp;quot;BOG&amp;quot;, &amp;quot;UPLAND&amp;quot;)),
                       log.Perimeter = mean(min.trap$log.Perimeter),
                       Num_ranatra = mean(min.trap$Num_ranatra),
                       Effort = mean(min.trap$Effort))
yhat.response &amp;lt;- modavgPred(cand.set = Cand.mod, modnames = Modnames,
           newdata = min.trap, type = &amp;quot;response&amp;quot;)
yhat.link &amp;lt;- modavgPred(cand.set = Cand.mod, modnames = Modnames,
           newdata = min.trap, type = &amp;quot;link&amp;quot;)
yhat.default &amp;lt;- modavgPred(cand.set = Cand.mod, modnames = Modnames,
           newdata = min.trap)

res &amp;lt;- data.table(response=yhat.response$mod.avg.pred, 
                  link=exp(yhat.link$mod.avg.pred),
                  default=yhat.default$mod.avg.pred)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Cade, B.S. (2015). Model averaging and muddled multimodel inferences. Ecology 96, 2370–2382.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On model averaging the coefficients of linear models</title>
      <link>/2018/05/on-model-averaging-regression-coefficients/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/on-model-averaging-regression-coefficients/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;/2018/05/an-even-more-compact-defense-of-coefficient-model-averaging/&#34;&gt;a shorter argument based on a specific example is here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;“What model averaging does not mean is averaging parameter estimates, because parameters in different models have different meanings and should not be averaged, unless you are sure you are in a special case in which it is safe to do so.” – Richard McElreath, p. 196 of the textbook I wish I had learned from &lt;a href=&#34;http://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34;&gt;Statistical Rethinking&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an infrequent but persistent criticism of model-averaged coefficients in the applied statistics literature on model averaging. I have heard from reviewers of a manuscript that it is infrequent only because it is &lt;em&gt;probably&lt;/em&gt; common knowledge among statisticians – I’m not aware of actual data. In most sources, the criticism is like McElreath’s, one sentence without any further explanation. Three recent papers in the ecology literature have expanded explanations (&lt;a href=&#34;https://scholar.google.com/scholar?q=Considerations+for+assessing+model+averaging+of+regression+coefficients&amp;amp;hl=en&amp;amp;as_sdt=0&amp;amp;as_vis=1&amp;amp;oi=scholart&amp;amp;sa=X&amp;amp;ved=0ahUKEwjEkdHjh_TaAhWhwVkKHV21DEwQgQMIJTAA&#34; target=&#34;_blank&#34;&gt;Banner and Higgs 2017&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C20&amp;amp;q=Model+averaging+and+muddled+multimodel+inferences&amp;amp;btnG=&#34; target=&#34;_blank&#34;&gt;Cade 2015&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C20&amp;amp;q=Model+averaging+in+ecology%3A+a+review+of+Bayesian%2C+information‐theoretic+and+tactical+approaches+for+predictive+inference&amp;amp;btnG=&#34; target=&#34;_blank&#34;&gt;Dormann et al. 2018&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;McElreath doesn’t give any hints what a “special case” looks like and I am not aware of any defense of model-averaged coefficients more generally, other than the extremely brief response to Draper’s equally brief comment in &lt;a href=&#34;https://scholar.google.com/scholar?cluster=4093301480813393179&amp;amp;hl=en&amp;amp;as_sdt=0,20&#34; target=&#34;_blank&#34;&gt;Hoeting et al. 1999&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Regardless, I think the “different meanings” criticism is wrong, and even obviously wrong, but the recognition of this has been masked by confused thinking about the parameters of a linear model. The confusion arises because of incorrectly equating the definition of the coefficients of a linear model and the definition of the parameters of a linear model.&lt;/p&gt;
&lt;p&gt;A coefficient of a linear model is the difference in the modeled value given a one unit difference in the predictor,&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
b_j.k = (\hat{Y}|X_j=x+1, X_k=x_k) - (\hat{Y}|X_j=x, X_k=x_k)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;For generalized linear models, the modeled values are on the link scale. Because the modeled value of a linear model is a conditional mean, the coefficient is a difference in conditional means and therefore, is conditional on the additional covariates in the model (&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Here is where the confusion arises – &lt;strong&gt;A linear model coefficient estimates two different parameters&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression parameter, which is a function of probablistic conditioning
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\theta_{j.k} = E[Y|X_j=x+1, X_k=x_k] - E[Y|X_j=x, X_k=x_k]
\end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Effect parameter, which is a function of causal conditioning
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\beta_j = E((Y_i | {X_j=x+1}) - (Y_i | {X_j=x}))
\end {equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\beta_j = E(Y | do(X_j=x+1)) - E(Y | do(X_j=x))
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Confusion arises when the definition of the linear model coefficient is thought to be the defintion of the parameter that is estimated. This is only true for the regression parameter.&lt;/p&gt;
&lt;p&gt;The regression parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_{j.k}\)&lt;/span&gt; is conditional on the covariates &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt;. The “true” model is the model specified by the researcher – there is no “generating” model. The model, and the model parameters, change as the researcher adds or deletes independent variables. Shalizi refers to this as “probabalistic conditioning” (&lt;a href=&#34;http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&#34; target=&#34;_blank&#34;&gt;p. 505 of 01/30/2017 edition&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I give two, equivalent definitions of the effect parameter. The first is the counterfactual definition, developed by Rubin. It is the average of individual causal effects, where an individual causal effect is a &lt;em&gt;potential outcome&lt;/em&gt; – what would happen if we could measure the response in individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; under two conditions (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x + 1\)&lt;/span&gt;) with only &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; having changed. The second is an interventional definition based on the &lt;span class=&#34;math inline&#34;&gt;\(do\)&lt;/span&gt; operator, developed by Pearl. The &lt;span class=&#34;math inline&#34;&gt;\(do\)&lt;/span&gt; operator represents what would happen in a hypothetical intervention that modifies &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; but leaves all other variables unchanged.&lt;/p&gt;
&lt;p&gt;The effect parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; is the “generating” effect and, consequently, it is not conditional on other variables that also generate (or causally effect) the response. Shalizi refers to this as “causal conditioning.” The true model includes all of the variables that causally effect &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Causal graphs identify different sources of bias in the estimation of effect parameters and can be used to reduce the true model down to the set of variables necessary to estimate an effect without bias.&lt;/p&gt;
&lt;p&gt;If our goal is mere description of the relationships among variables, the coefficients are estimating the regression parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_{j.k}\)&lt;/span&gt;. Parameters from different models truly have different meanings because each specifically describes the relationship between &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; conditional on a specific set of covariates. Averaging coefficients from different models would be averaging apples and oranges. Indeed, any sort of model selection would be comparing apples and oranges. Omitted variable bias and confounding are irrelevant or meaningless in the context of linear models as mere description, an important point that is missing from most statistics textbooks &lt;a href=&#34;https://scholar.google.com/scholar?cluster=12625276465843289889&amp;amp;hl=en&amp;amp;as_sdt=0,20&#34; target=&#34;_blank&#34;&gt;although see Gelman and Hill&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If our goal is to understand a system at a level that allows intervention (manipulation of variables) to generate a specific outcome, the &lt;span class=&#34;math inline&#34;&gt;\(b^[m]_j\)&lt;/span&gt; from each of the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; nested submodels estimates the same effect parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; and so have the same meaning. In any observational study, all of the models are wrong and each of the estimates &lt;span class=&#34;math inline&#34;&gt;\(b^[m]_j\)&lt;/span&gt; is biased by some unknown amount because of ommitted confounders. Model averaging the coefficients is a reasonable method for combining information from these models, all of which we know are wrong.&lt;/p&gt;
&lt;p&gt;If our goal is prediction, the issue is moot, because in prediction we are generally not interested in the coefficients, unless we want to quantify something like variable importance. Parameters simply serve the purpose of mapping data to a prediction, they don’t have any meaning beyond this. If model-averaging the coefficients improves predictive performance, then model-average. And, for linear models, including generalized linear models, model-averaged predictions computed by averaging the predictions or averaging the coefficients &lt;a href=&#34;/post/2018-05-04-model-averaged-coefficients-of-a-glm/&#34;&gt;are equivalent as long as the averaging is on the link scale&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This rebuttal to the claim that coefficients from linear models cannot be meaningfully averaged is not a defense of model averaging more generally. I am not advocating model averaging over alternative methods for causal modeling, I am simply arguing that coefficients from nested submodels can have the same meaning across models and, therefore, can be meaningfully averaged.&lt;/p&gt;
&lt;p&gt;I am looking for a strong argument to my rebuttal. That means…&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Arguing that the shrinkage property of model averaging is &lt;em&gt;ad hoc&lt;/em&gt; and there are better methods (such as the family of penalized regression methods that include the lasso and ridge regression) that explicitly model the shrinkage parameter is not a argument against my rebuttal, only an argument for alternatives to model averaging.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arguing that model selection and model averaging is mindless and careful selection of covariates is superior is not an argument against my rebuttal, only an argument against model selection and averaging more generally.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arguing that model selection and model averaging is mindless and careful construction of a strucutural model based on prior knowledge is superior is not an argument against this rebuttal, only an argument against model selection and averaging more generally.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arguing that model averaging was developed for model-averaged predictions or that most of the theory applies to model-averaged predictions is not an argument against this rebuttal, only an argument that we need better theoretical and empirical work on model-averaged coefficients.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arguing that coefficients from non-linear models cannot be meaningfully averaged is not an argument against my rebuttal, only an argument on the limitation of model averaging coefficients (and again, contrary to &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C20&amp;amp;q=Model+averaging+and+muddled+multimodel+inferences&amp;amp;btnG=&#34; target=&#34;_blank&#34;&gt;Cade 2015&lt;/a&gt; and &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C20&amp;amp;q=Model+averaging+in+ecology%3A+a+review+of+Bayesian%2C+information‐theoretic+and+tactical+approaches+for+predictive+inference&amp;amp;btnG=&#34; target=&#34;_blank&#34;&gt;Dormann et al. 2018&lt;/a&gt;, coefficients of nested submodels from generalized linear models, which are &lt;em&gt;linear&lt;/em&gt; models, can be meaningfully averaged).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>An even more compact defense of coefficient model averaging</title>
      <link>/2018/05/an-even-more-compact-defense-of-coefficient-model-averaging/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/an-even-more-compact-defense-of-coefficient-model-averaging/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;/2018/05/on-model-averaging-regression-coefficients/&#34;&gt;a longer, more detailed argument is here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The parameter that is averaged “needs to have the same meaning in all “models” for the equations to be straightforwardly interpretable; the coefficient of x1 in a regression of y on x1 is a different beast than the coefficient of x1 in a regression of y on x1 and x2.” – David Draper in a comment on &lt;a href=&#34;https://scholar.google.com/scholar?cluster=4093301480813393179&amp;amp;hl=en&amp;amp;as_sdt=0,20&#34; target=&#34;_blank&#34;&gt;Hoeting et al. 1999&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;David Draper suggested this example from the textbook by Freedman, Pisani and Purves. The treatment is not randomized so this is an observational design.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
BP_i = \beta_0 + \beta_1 Treatment_i  +  e_i
\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
BP_i = \gamma_0 + \gamma_1 Treatment_i + \gamma_2 Age_i + \varepsilon_i
\end{equation}\]&lt;/span&gt;
&lt;p&gt;the population of interest is adult women in the year 1960. BP is blood pressure. The treatment is a binary factor (1 = takes the contraceptive pill, 0 = doesn’t). Age is a confound: as age goes up, blood pressure goes up and pill use goes down.&lt;/p&gt;
&lt;p&gt;The “different parameters” argument of Draper and McElreath is that &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt; estimate different parameters and are not meaningfully averaged. Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; estimates an unconditional parameter and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt; estimates a parameter conditional on &lt;span class=&#34;math inline&#34;&gt;\(Age\)&lt;/span&gt;. Shalizi refers to this as probabilistic conditioning (&lt;a href=&#34;http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&#34; target=&#34;_blank&#34;&gt;p. 505 of 01/30/2017 edition&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;My argument (following Pearl and Shalizi) is that the pill researchers are not interested in description but in causal modeling. In this case, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt; both estimate the effect parameter (the true causal effect of the pill on BP), each with some unknown bias due to omitted confounders. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt; can therefore be meaningfully averaged. A causal effect is not conditional on other factors that also causally effect the response. Shalizi refers to this as “causal conditioning”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/scholar?cluster=12625276465843289889&amp;amp;hl=en&amp;amp;as_sdt=0,20&#34;&gt;Gelman and Hill 2006&lt;/a&gt; did not explicitly define both kinds of parameters but did recognize that regression does estimate two kinds of parameters when they state that the formula for ommitted variable bias “is commonly presented in regression texts as a way of describing the bias that can be incurred if a model is specified incorrectly. However, this term has little meaning outside of a context in which one is attempting to make causal inferences.”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model-averaged coefficients of a GLM</title>
      <link>/2018/05/model-averaged-coefficients-of-a-glm/</link>
      <pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/model-averaged-coefficients-of-a-glm/</guid>
      <description>


&lt;p&gt;This is a very quick post as a comment to the statement&lt;/p&gt;
&lt;p&gt;“For linear models, predicting from a parameter-averaged model is mathematically identical to averaging predictions, but this is not the case for non-linear models…For non-linear models, such as GLMs with log or logit link functions g(x)&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, such coefficient averaging is not equivalent to prediction averaging.”&lt;/p&gt;
&lt;p&gt;from the supplement of Dormann et al. &lt;a href=&#34;https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1309&#34; target=&#34;_blank&#34;&gt;Model averaging in ecology: a review of Bayesian, information‐theoretic and tactical approaches for predictive inference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is within the context of problems with model-averaged coefficients. I think the authors are arguing that for GLMs, we cannot use model-averaged predictions to justify interpreting model-averaged coefficients since modeled-averaged coefficients are not the same thing that produced the model-averaged predictions. I think this is wrong, model-averaged coefficients &lt;em&gt;are&lt;/em&gt; the same thing that produced the model-averaged predictions &lt;em&gt;if&lt;/em&gt; both are averaged on the link scale.&lt;/p&gt;
&lt;p&gt;Here is their equation S2&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\frac{1}{m} \sum_{i=1}^m{g^{-1}(Xb_i)} \ne g^{-1}(X \frac{\sum_{i=1}^m{b_i}}{m})
\end{equation}\]&lt;/span&gt;
&lt;p&gt;This equation is true but, I think, irrelevant. The LHS averages predictions, but the average is on the response scale. The RHS averages the coefficients, but the average is on the link scale. I would agree with the statement &lt;em&gt;if&lt;/em&gt; this is the way researchers are averaging (although I would think naive researchers would be averaging coefficients on the response scale and predictions on the link scale)&lt;/p&gt;
&lt;p&gt;I would think the LHS is the incorrect method for computing predictions on the response scale. With all averaging on the link scale, equation S2 becomes&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
g^{-1}(\frac{1}{m} \sum_{i=1}^m{(Xb_i)}) = g^{-1}(X \frac{\sum_{i=1}^m{b_i}}{m})
\end{equation}\]&lt;/span&gt;
&lt;p&gt;The RHS is the same as their S2. The LHS computes the prediction for each model on the link scale, then averages over these on the link scale, and then back-transforms to the response scale. Both LHS and RHS are correct ways to get the model-averaged predictions on the response scale. And, both the LHS and RHS are equivalent and show that “predicting from a parameter-averaged model [RHS] is mathematically identical to averaging predictions [LHS].”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In short.&lt;/strong&gt; Model-averaged coefficients should be averaged on the link scale. It would indeed be not “correct” to interpret coefficients that are model-averaged on the response scale, although as Dormann et al state, “In practice, however, both log and logit are sufficiently linear, making coefficient averaging an acceptable approximation.” If the issue is that researchers are model averaging coefficients on the response scale, this isn’t an issue with model averaging coefficients in general, but only of model-averaging coefficients on a response scale.&lt;/p&gt;
&lt;p&gt;Here is a short R-doodle showing that “predicting from a parameter-averaged model is mathematically identical to averaging predictions” for a GLM, as long as one is doing all the averaging on the link scale&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MuMIn)
library(ggplot2)
library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expit &amp;lt;- function(x) {exp(x)/(1+exp(x))} # the inverse logit function. This generates the probability of the event p
logit &amp;lt;- function(p) {log(p/(1-p))} # the log of the odds or &amp;quot;logodds&amp;quot; given the probability of an event p. This is NOT the odds ratio, which is the ratio of two odds.
p2odd &amp;lt;- function(p) {p/(1-p)} # the odds of the probability of an event
odd2p &amp;lt;- function(x) {x/(1+x)} # the probability associated with an odds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100
z &amp;lt;- rnorm(n)
# create two correlated variables, x1 and x2, with E[cor] = zeta^2
zeta &amp;lt;- 0.7
sigma &amp;lt;- 0.3
x1 &amp;lt;- zeta*z + sqrt(1-zeta^2)*rnorm(n)
x2 &amp;lt;- zeta*z + sqrt(1-zeta^2)*rnorm(n)

# create a performance measure as function of x1 and x2
perf &amp;lt;- x1 + x2 + rnorm(n)*sigma # coefficients both = 1

# transform performance to probability of survival

# create fake data
p.survival &amp;lt;- expit(perf)
y &amp;lt;- rbinom(n, 1, p.survival)
dt &amp;lt;- data.table(y=y,x1=x1,x2=x2)

# fit
fit &amp;lt;- glm(y ~ x1 + x2, data=dt, family=binomial(link=&amp;#39;logit&amp;#39;), na.action=na.fail)

# all subsets regression and model average using MuMIn
fit.all &amp;lt;- dredge(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Fixed term is &amp;quot;(Intercept)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.avg &amp;lt;- model.avg(fit.all, fit=TRUE) # coeffcients are on link scale

model_set &amp;lt;- get.models(fit.all, subset=TRUE) # all models
X &amp;lt;- model.matrix(fit)

# (0) MuMIn predict
yhat0.response_scale &amp;lt;- predict(fit.avg, backtransform=TRUE)

# RHS eq. S2
# verify &amp;quot;by hand&amp;quot; by predicting on link scale then back transforming to response scale
yhat0.link_scale &amp;lt;- predict(fit.avg, backtransform=FALSE)
yhat0.response_scale2 &amp;lt;- expit(yhat0.link_scale) 
head(data.table(yhat0.response_scale, yhat0.response_scale2)) # these should be equal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    yhat0.response_scale yhat0.response_scale2
## 1:            0.1958144             0.1958144
## 2:            0.6747000             0.6747000
## 3:            0.2327797             0.2327797
## 4:            0.2047924             0.2047924
## 5:            0.5043722             0.5043722
## 6:            0.8114121             0.8114121&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yhat0 &amp;lt;- yhat0.response_scale # predictions using MuMIn package


# (1) use model averaged B to get prediction on link scale. back-transform to response scale
# this is RHS eq. S2 RHS of appendix
# I use MuMIn model.avg function to model average coefficients on link scale
# then I compute predictions on link scale
# then I back-transform predictions to response scale 
# This should equal yhat0 from above
b &amp;lt;- model.avg(model_set)$coefficients[&amp;#39;full&amp;#39;,][colnames(X)]
yhat1.link_scale &amp;lt;- X%*%b
yhat1 &amp;lt;- expit(yhat1.link_scale)
MSE1 &amp;lt;- sqrt(mean((yhat1 - dt[, y])^2))

# (2) a variant of yhat1 and yhat0 - I am &amp;quot;by hand&amp;quot; computing the average prediction on the link scale
# then back-transforming to response scale
# this can be thought of as the corrected LHS of S2
w &amp;lt;- fit.all$weight
yhat2.each_model.link_scale &amp;lt;- sapply(model_set, predict)
yhat2.link_scale &amp;lt;- yhat2.each_model.link_scale%*%w
yhat2 &amp;lt;- expit(yhat2.link_scale)
MSE2 &amp;lt;- sqrt(mean((yhat2 - dt[, y])^2))

# (3) Thisis the &amp;quot;incorrect&amp;quot; method of model averaging&amp;quot;
# LHS of S2
# model average predictions on response scale (i.e. back-transform each prediction to response scale and then model average)
# I need the first two calculations from #(2) above to get yhat2.each_model.link_scale
yhat3.each_model.response_scale &amp;lt;- expit(yhat2.each_model.link_scale)
yhat3 &amp;lt;- yhat3.each_model.response_scale%*%w
MSE3 &amp;lt;- sqrt(mean((yhat3-dt[, y])^2))

# Predicted values computed 4 different ways
head(data.table(yhat0=yhat0, yhat1=yhat1[,1], yhat2=yhat2[,1], yhat3=yhat3[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        yhat0     yhat1     yhat2     yhat3
## 1: 0.1958144 0.1958144 0.1958144 0.1966351
## 2: 0.6747000 0.6747000 0.6747000 0.6729130
## 3: 0.2327797 0.2327797 0.2327797 0.2334907
## 4: 0.2047924 0.2047924 0.2047924 0.2088204
## 5: 0.5043722 0.5043722 0.5043722 0.5043093
## 6: 0.8114121 0.8114121 0.8114121 0.8105789&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#yhat0 = MuMIn model averaged predictions (correct method)
#yhat1 = RHS of equation s2 in appendix (correct method)
#yhat2 = Corrected LHS of s2 in appendix (correct method)
#yhat3 = LHS of s2 in appendix (incorrect?)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;understood but awkward and confusing. It is unconventional to call a GLM a non-linear model, especially given the name “General Linear Model”&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On alpha</title>
      <link>/2018/04/on-alpha/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/on-alpha/</guid>
      <description>


&lt;p&gt;This post is motivated by Terry McGlynn’s thought provoking &lt;a href=&#34;https://smallpondscience.com/2018/04/23/how-do-we-move-beyond-an-arbitrary-statistical-threshold/&#34;&gt;How do we move beyond an arbitrary statistical threshold?&lt;/a&gt; I have been struggling with the ideas explored in Terry’s post ever since starting my PhD 30 years ago, and its only been in the last couple of years that my own thoughts have begun to gel. This long marination period is largely because of my very classical biostatistical training. My PhD is from the Department of Anatomical Sciences at Stony Brook but the content was geometric morphometrics and James Rohlf was my mentor for morphometrics specifically, and multivariate statistics more generally. The last year of my PhD, I was Robert Sokal’s RA (I was the programmer!) and got two co-authored papers with him. I invested a tremendous amount of time generating little statistical doodles (first in Excel, then in Pascal, and then in R) to better understand ANOVA, and permutation tests, and the bootstrap, and similar frequentist tools.&lt;/p&gt;
&lt;p&gt;My answer is partly answered by my two posts to the &lt;a href=&#34;https://rapidecology.com&#34;&gt;Rapid Ecology&lt;/a&gt; blog. The first – &lt;a href=&#34;https://rapidecology.com/2018/04/09/when-do-we-introduce-best-statistical-practices-to-undergraduate-biology-majors/&#34;&gt;When do we introduce best statistical practices to undergraduate biology majors?&lt;/a&gt; was posted April 9. The second – “Abandon ANOVA-type experiments” is a more radical answer, and is scheduled to appear in a couple of weeks.&lt;/p&gt;
&lt;p&gt;Here, I expand on the second post but make it more general. Terry finishes his post with the statement “To be clear, I’m not arguing (here) that we should be ditching the hypothesis falsification approach to answering questions”. Maybe he’s arguing this elsewhere. Regardless, I &lt;em&gt;am&lt;/em&gt; arguing that here. I am &lt;em&gt;not&lt;/em&gt; arguing against the use of p-values (here!) – simply against the concept of comparing a p-value to a type I error rate (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The practice of comparing a p-value to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and classifying a result as “significant” or “non-significant” has led to the cargo-cult science practice of “discovery by p-value.” Many scientists literally believe they have discovered something about the world because they found p &amp;lt; 0.05. Fat poop microbes cause obesity? Exists (p &amp;lt; 0.05). Many scientists literally believe that something doesn’t exist because p &amp;gt; 0.05. An interaction between CO2 and Temperature on larval growth? Doesn’t exist (p = 0.079). Or, if we want the interaction to exist, then we report “the interaction trends toward significance (p = 0.079)”. How come results never trend away from significance?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparing a p-value to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is a reasonable decision theoretic strategy relevant to manufacturing (let’s test a sample from this lot and throw out the whole batch if p &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;). By contrast, in most papers in ecology or physiology that I read, a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is not used to make a decision. Classifying a p-value as signficant or non-significant adds zero-value to the analysis. Instead, it creates the illusion of discovery. Sometimes &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values are used to make decisions, for example, statistical significance is routinely used to find the subset of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; that are thought to be causally related to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This might be a classic multiple regression with tens of environmental variables or a more modern genomic analysis with thousands of genes or hundreds of thousands of SNPs. There are a great many papers devoted to methods for “correcting for multiple testing” as if we can discover by statistical significance. Scientific discovery and knowlege requires replication and rigorous probing, not statistical significance. I frankly don’t see the point of model simplification or adjusting p-values for multiple testing. Instead, we should use the results to rank the effect sizes and then do the hard work of experimentally isolating and rigorously probing these effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My answer is not to lower &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; or advocate for a more flexible &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. And, banning asterisks from tables and plots or the word “signficant” from the text isn’t really enough. I think we should simply teach our students to stop hypothesis testing. We should teach our students that estimating effect sizes is critical for model development and testing (the focus of the not-yet-published post at Rapid Ecology), and of course, for decision making. We should teach our students that uncertaintly is a part of science and the different ways to measure uncertainty. We should teach our students that rigorous probing of a hypothesis is vital for discovery. We should teach our students that replication is vital for discovery. And we should lobby editors to stop publishing cargo-cult science practices.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining data, distribution summary, model effects, and uncertainty in a single plot</title>
      <link>/2018/03/combining-data-distribution-summary-model-effects-and-uncertainty-in-a-single-plot/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/combining-data-distribution-summary-model-effects-and-uncertainty-in-a-single-plot/</guid>
      <description>


&lt;p&gt;A Harrell plot combines a forest plot of estimated treatment effects and uncertainty, a dot plot of raw data, and a box plot of the distribution of the raw data into a single plot. A Harrell plot encourages best practices such as exploration of the distribution of the data and focus on effect size and uncertainty, while discouraging bad practices such as ignoring distributions and focusing on &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values. Consequently, a Harrell plot should replace the bar plots and Cleveland dot plots that are currently ubiquitous in the literature.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.middleprofessor.com/files/quasipubs/harrell_plot_intro.html&#34;&gt;Read the whole post here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is the range of reasonable P-values given a two standard error difference in means?</title>
      <link>/2018/03/what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means/</link>
      <pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means/</guid>
      <description>


&lt;p&gt;Here is the motivating quote for this post, from Andrew Gelman’s blog post &lt;a href=&#34;http://andrewgelman.com/2017/11/28/five-ways-fix-statistics/&#34;&gt;“Five ways to fix statistics”&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I agree with just about everything in Leek’s article except for this statement: “It’s also impractical to say that statistical metrics such as P values should not be used to make decisions. Sometimes a decision (editorial or funding, say) must be made, and clear guidelines are useful.” Yes, decisions need to be made, but to suggest that p-values be used to make editorial or funding decisions—that’s just horrible. That’s what’s landed us in the current mess. As my colleagues and I have discussed, we strongly feel that editorial and funding decisions should be based on theory, statistical evidence, and cost-benefit analyses—not on a noisy measure such as a p-value. &lt;em&gt;Remember that if you’re in a setting where the true effect is two standard errors away from zero, that the p-value could easily be anywhere from 0.00006 and 1. That is, in such a setting, the 95% predictive interval for the z-score is (0, 4), which corresponds to a 95% predictive interval for the p-value of (1.0, 0.00006)&lt;/em&gt;. That’s how noisy the p-value is. So, no, don’t use it to make editorial and funding decisions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m not sure how Gelman computed these numbers, but the statement seems worthy of exploring with an R-doodle. Here is the way I’d frame the question for exploration: given a true, two SED (standard error of the difference in means) effect, what is the interval containing 95% of future &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values? Here is the R-doodle, which also explores the interval given 1, 3, and 4 SED effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# doodle to see 95% CI of p-value (range of p-values consistent with data) given
# a 2SE effect size (i.e. just at 0.05 for large n)
# motivating quote:
# &amp;quot;Remember that if you’re in a setting where the true effect is two standard errors away from zero, that the p-value could easily be anywhere from 0.00006 and 1. That is, in such a setting, the 95% predictive interval for the z-score is (0, 4), which corresponds to a 95% predictive interval for the p-value of (1.0, 0.00006). That’s how noisy the p-value is.&amp;quot;
# source: http://andrewgelman.com/2017/11/28/five-ways-fix-statistics/
# Jeffrey Walker
# November 29, 2017

library(ggplot2)
library(data.table)
set.seed(1)
n &amp;lt;- 30
niter &amp;lt;- 5*10^3
sigma &amp;lt;- 1
x &amp;lt;- rep(c(0,1),each=n)
p &amp;lt;- numeric(niter)
d &amp;lt;- numeric(niter)
res &amp;lt;- data.table(NULL)
# initialize in SED units
for(sed_effect in 1:4){
  # the effect in SEM units
  se_effect &amp;lt;- sqrt(2*sed_effect^2) # 
  # the effect in SD units
  sd_effect &amp;lt;- se_effect/sqrt(n)
  power &amp;lt;- power.t.test(n, sd_effect, sigma)$power
  y1 &amp;lt;- matrix(rnorm(n*niter,mean=0.0, sd=sigma), nrow=n)
  y2 &amp;lt;- matrix(rnorm(n*niter, mean=sd_effect, sd=sigma),nrow=n)
  for(i in 1:niter){
    p[i] &amp;lt;- t.test(y1[,i],y2[,i])$p.value
  }
  ci &amp;lt;- quantile(p, c(0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975))
  res &amp;lt;- rbind(res, data.table(n=n,
                          d.sed=sed_effect,
                          d.sem=se_effect, d.sd=round(sd_effect, 2),
                          power=round(power,2),
                          data.table(t(ci))))
}
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     n d.sed    d.sem d.sd power         2.5%           5%          25%
## 1: 30     1 1.414214 0.26  0.16 3.347471e-03 9.251045e-03 9.726431e-02
## 2: 30     2 2.828427 0.52  0.50 1.307935e-04 4.364955e-04 9.216022e-03
## 3: 30     3 4.242641 0.77  0.84 4.042272e-06 1.242138e-05 4.220133e-04
## 4: 30     4 5.656854 1.03  0.98 4.570080e-08 2.297983e-07 1.471134e-05
##             50%         75%        95%      97.5%
## 1: 0.2936458989 0.611365558 0.91934854 0.95630882
## 2: 0.0499891796 0.189903833 0.69030016 0.84298129
## 3: 0.0036464345 0.023343898 0.17671907 0.29660133
## 4: 0.0001906092 0.001769547 0.02642579 0.05377549&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;old_names &amp;lt;- c(&amp;#39;2.5%&amp;#39;, &amp;#39;5%&amp;#39;, &amp;#39;25%&amp;#39;, &amp;#39;50%&amp;#39;, &amp;#39;75%&amp;#39;, &amp;#39;95%&amp;#39;, &amp;#39;97.5%&amp;#39;)
new_names &amp;lt;- c(&amp;#39;lo3&amp;#39;, &amp;#39;lo2&amp;#39;, &amp;#39;lo1&amp;#39;, &amp;#39;med&amp;#39;, &amp;#39;up1&amp;#39;, &amp;#39;up2&amp;#39;, &amp;#39;up3&amp;#39;)
setnames(res, old=old_names, new=new_names)
gg &amp;lt;- ggplot(data=res, aes(x=d.sed, y=med)) +
  geom_linerange(aes(ymin=lo3, ymax=up3)) +
  geom_linerange(aes(ymin=lo1, ymax=up1), size=4, color=&amp;#39;darkgray&amp;#39;) +
  geom_point() +
  geom_hline(yintercept=0.05, linetype=&amp;#39;dashed&amp;#39;) +
#  geom_hline(yintercept=0.05, aes(linetype=&amp;#39;dashed&amp;#39;, color=&amp;#39;darkgray&amp;#39;))
#  geom_hline(yintercept=0.05, mapping=aes(linetype=&amp;#39;dashed&amp;#39;, color=&amp;#39;red&amp;#39;))
  labs(x=&amp;#39;Difference in means (SED)&amp;#39;, y=&amp;#39;p-value&amp;#39;) +
  theme_minimal()
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-18-what-is-the-range-of-reasonable-p-values-given-a-two-standard-error-difference-in-means_files/figure-html/simulation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A 2 SED effect has an expected &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value near 0.05 given a reasonable sample size. My 95% interval for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values for a 2 SED effect is (0.0001, .83), which is narrower than Gelman’s. I’m not sure we’re computing the same thing. I’ve explored the question, what is the &lt;em&gt;confidence interval&lt;/em&gt; of the SED and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value if the true effect, in SED units, is 2?&lt;/p&gt;
&lt;p&gt;Regardless, the larger point remains intact. The larger point, of course, is that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values are noisy. If an effect is just statistically significant, future &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values from the experiment would reasonably range from very small to very large (and this assumes that the only difference in future experiments is sampling variation).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bias in pre-post designs -- An example from the Turnbaugh et al (2006) mouse fecal transplant study</title>
      <link>/2018/03/bias-in-pre-post-designs-an-example-from-the-turnbaugh-et-al-2006-mouse-fecal-transplant-study/</link>
      <pubDate>Thu, 08 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/bias-in-pre-post-designs-an-example-from-the-turnbaugh-et-al-2006-mouse-fecal-transplant-study/</guid>
      <description>


&lt;p&gt;This post is motivated by a twitter link to a &lt;a href=&#34;https://honey-guide.com/2018/02/13/graphic-faecal-transplants-and-obesity/&#34; target=&#34;_blank&#34;&gt;recent blog post&lt;/a&gt; critical of the old but influential study &lt;a href=&#34;https://scholar.google.co.uk/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=An+obesity-associated+gut+microbiome+with+increased+capacity+for+energy+harvest.&amp;amp;btnG=&#34;&gt;An obesity-associated gut microbiome with increased capacity for energy harvest&lt;/a&gt; with &lt;a href=&#34;https://www.nature.com/articles/nature05414/metrics&#34; target=&#34;_blank&#34;&gt;impressive citation metrics&lt;/a&gt;. In the post, Matthew Dalby smartly used the available data to reconstruct the final weights of the two groups. He showed these final weights were nearly the same, which is not good evidence for a treatment effect, given that the treatment was randomized among groups.&lt;/p&gt;
&lt;p&gt;This is true, but doesn’t quite capture the essence of the major problem with the analysis: a simple &lt;em&gt;t&lt;/em&gt;-test of differences in percent weight change fails to condition on initial weight. And, in pre-post designs, groups with smaller initial measures are expected to have more change due to &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_toward_the_mean&#34; target=&#34;_blank&#34;&gt;regression to the mean&lt;/a&gt;. This is exactly what was observed. In the fecal transplant study, the initial mean weight of the rats infected with &lt;em&gt;ob/ob&lt;/em&gt; feces was smaller (by 1.2SD) than that of the rats infected with &lt;em&gt;+/+&lt;/em&gt; feces and, consequently, the expected difference in the change in weight is not zero but positive (this is the expected difference &lt;em&gt;conditional on an inital difference&lt;/em&gt;). More generally, a difference in percent change &lt;em&gt;as an estimate of the parametric difference in percent change&lt;/em&gt; is not biased, but it is also not an estimate of the treatment effect, except under very limited conditions explained below. If these conditions are not met, a difference in percent change &lt;em&gt;as an estimate of the treatment effect&lt;/em&gt; is biased, unless estimated conditional on (or “adusted for”) the initial weight.&lt;/p&gt;
&lt;p&gt;Regression to the mean also has consequences on the hypothesis testing approach taken by the authors. Somewhat perplexingly, a simple &lt;em&gt;t&lt;/em&gt;-test of the post-treatment weights or of pre-post difference in weight, or of percent change in weight does not have elevated Type I error. This is demonstrated using simulation below. The explaination is, in short, the Type I error rate is also a function of the initial difference in weight. If the initial difference is near zero, the Type I error is much less than the nominal alpha (say, 0.05). But as the intial difference moves away from zero, the Type I error is much greater than the nominal alpha. Over the space of the initial difference, these rates average to the nominal alpha.&lt;/p&gt;
&lt;div id=&#34;continue-reading-the-whole-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;a href=&#34;https://www.middleprofessor.com/files/quasipubs/change_scores.html&#34;&gt;continue reading the whole post&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What is an R doodle?</title>
      <link>/2018/03/what-is-an-r-doodle/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/what-is-an-r-doodle/</guid>
      <description>


&lt;p&gt;An R doodle is a short script to check intuition or understanding. Almost always, this involves generating fake data. I might create an R doodle when I’m reviewing a manuscript or reading a published paper and I want to check if their statistical analysis is doing what the authors think it is doing. Or maybe I create it to help me figure out what the authors are doing. Or I might be teaching some method and I create an R doodle to help me understand how the method behaves given different input (fake) data sets. Or I might create an R doodle to help others understand - this could be for students, or some blog comment, or my department colleagues.&lt;/p&gt;
&lt;div id=&#34;why-this-blog&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why this blog?&lt;/h2&gt;
&lt;p&gt;My R doodles are reactionary - I read something and start coding in R studio. I have an organized folder of these on my Google drive. Some I have expanded into published papers (&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/evo.12406/full&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://jeb.biologists.org/content/218/22/3647.eLetters&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;https://peerj.com/articles/2575/&#34;&gt;here&lt;/a&gt;) but I have a file drawer full of effectively completed manuscripts that are unlikely to be submitted. Writing R doodles, and especially the manuscripts, consumes a large fraction of my professional time, but others don’t really gain from this hard work unless I publish the results. This blog was started to archive my R doodles going forward. Some of these will be expanded into longer scripts and posts. Some of these will be fleshed out into manuscripts for archiving at &lt;a href=&#34;https://peerj.com/preprints/&#34;&gt;PeerJ preprints&lt;/a&gt; or bioR&lt;span class=&#34;math inline&#34;&gt;\(\chi\)&lt;/span&gt;iv(&lt;a href=&#34;https://www.biorxiv.org&#34; class=&#34;uri&#34;&gt;https://www.biorxiv.org&lt;/a&gt;). Others will be expanded into &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;Shiny apps&lt;/a&gt;. Some I might even submit for publication in a journal. But ultimately, the goal is to have an archive for teaching and learning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;My statistics training was classical biostatistics – both &lt;a href=&#34;https://en.wikipedia.org/wiki/Robert_R._Sokal&#34;&gt;Robert Sokal&lt;/a&gt; and &lt;a href=&#34;http://life.bio.sunysb.edu/ee/rohlf/&#34;&gt;James Rohlf&lt;/a&gt; were my principal mentors during my PhD years. But, I have zero training in Math, Mathematical statistics, or Computer Science and this will be evident in the way I think and write. Statisticians frequently comment on the muddled thinking found in statistics textbooks written by Biologists (or non-statisticians generally). That said, it is pretty easy to find muddled thinking in textbooks written by statisticians and even easier to find muddled thinking in biology papers with statisticians on the author list. Statistical thinking just isn’t that easy.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>